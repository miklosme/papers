{
  "arxivId": "2502.08916",
  "title": "PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic Decision-Making Applied to Histopathology",
  "abstract": "Diagnosing diseases through histopathology whole slide images (WSIs) is fundamental in modern pathology but is challenged by the gigapixel scale and complexity of WSIs. Trained histopathologists overcome this challenge by navigating the WSI, looking for relevant patches, taking notes, and compiling them to produce a final holistic diagnostic. Traditional AI approaches, such as multiple instance learning and transformer-based models, fail short of such a holistic, iterative, multi-scale diagnostic procedure, limiting their adoption in the real-world. We introduce PathFinder, a multi-modal, multi-agent framework that emulates the decision-making process of expert pathologists. PathFinder integrates four AI agents—the Triage Agent, Navigation Agent, Description Agent, and Diagnosis Agent—that collaboratively navigate WSIs, gather evidence, and provide comprehensive diagnoses with natural language explanations. The Triage Agent classifies the WSI as benign or risky; if risky, the Navigation and Description Agents iteratively focus on significant regions, generating importance maps and descriptive insights of sampled patches. Finally, the Diagnosis Agent synthesizes the findings to determine the patient's diagnostic classification. Our Experiments show that PathFinder outperforms state-of-the-art methods in skin melanoma diagnosis by 8% while offering inherent explainability through natural language descriptions of diagnostically relevant patches. Qualitative analysis by pathologists shows that the Description Agent's outputs are of high quality and comparable to GPT-40. PathFinder is also the first AI-based system to surpass the average performance of pathologists in this challenging melanoma classification task by 9%, setting a new record for efficient, accurate, and interpretable AI-assisted diagnostics in pathology. Data, demo, code and models are available at https://pathfinder-dx.github.io/.",
  "summary": "PathFinder is a multi-agent AI system designed to mimic how pathologists diagnose diseases from biopsy images. It uses different AI agents specializing in tasks like assessing risk, navigating the image, describing features, and making a diagnosis.  PathFinder combines image and text data, and its diagnostic explanations are provided in natural language.\n\nKey aspects relevant to LLM-based multi-agent systems include: different agents collaborating on sub-tasks, integrating vision and language models, using text descriptions for agent guidance and feedback, challenges of applying LLMs to specialized tasks with limited data, the use of instruction tuning, and the ability to generate natural language explanations for increased interpretability.  PathFinder demonstrates that specialized agents working together can outperform generalist models in complex domains, highlighting the potential of multi-agent LLM systems.",
  "takeaways": "This paper presents exciting opportunities for JavaScript developers working with LLM-based multi-agent systems. Here are practical examples applying its insights to web development scenarios:\n\n**1. Triage Agent for Intelligent Task Routing:**\n\n* **Scenario:** A collaborative web application for document editing where multiple LLMs specialize in different tasks (grammar, style, conciseness).\n* **Implementation:**  A JavaScript triage agent, using a framework like TensorFlow.js, could analyze incoming documents and classify them based on the type of editing required (e.g., \"needs grammar check,\" \"needs style refinement\"). This classification routes the document to the appropriate LLM agent, optimizing workflow.\n\n```javascript\n// Example (Simplified) using TensorFlow.js\nasync function triageDocument(documentText) {\n  const model = await tf.loadLayersModel('path/to/triage_model.json');\n  const input = preprocessText(documentText); // Convert text to numerical input\n  const prediction = model.predict(input);\n  const taskCategory = getTaskCategory(prediction); // Map prediction to task category\n  return taskCategory; \n}\n```\n\n**2. Navigation Agent for Focused Information Retrieval:**\n\n* **Scenario:** A research platform where users query a knowledge base managed by multiple LLM agents specializing in different domains.\n* **Implementation:** A JavaScript navigation agent, integrated with a vector database like Pinecone or Weaviate, guides the search process.  The initial query is used to generate an importance map over the knowledge base (represented as vectors). The navigation agent, using a library like LangChain.js, iteratively selects the most relevant knowledge chunks, sends them to the appropriate LLM agents for processing, and refines the search based on the agents' responses.\n\n```javascript\n// Example (Conceptual) using LangChain.js\nasync function navigateKnowledgeBase(query, navigationAgent, llmAgents) {\n  let importanceMap = await generateImportanceMap(query, vectorDB);\n  let results = [];\n  for (let i = 0; i < maxIterations; i++) {\n    let relevantChunk = selectMostRelevantChunk(importanceMap);\n    let agent = selectAgent(relevantChunk, llmAgents);\n    let response = await agent.processChunk(relevantChunk);\n    results.push(response);\n    importanceMap = updateImportanceMap(importanceMap, response);\n  }\n  return results;\n}\n```\n\n**3. Description Agent for Summarization and Explanation:**\n\n* **Scenario:** An e-commerce platform where multiple LLM agents manage product information, customer reviews, and market trends.\n* **Implementation:**  A JavaScript description agent (using an LLM via an API like OpenAI or Cohere) summarizes the information gathered by other agents. For a product page, it might consolidate product features described by an LLM, customer sentiment extracted by another, and market analysis by a third, presenting a concise summary to the user.\n\n```javascript\n// Example (Conceptual) using OpenAI API\nasync function generateProductSummary(productID) {\n  const features = await llm_product.getProductFeatures(productID);\n  const reviews = await llm_reviews.getReviewsSummary(productID);\n  const market = await llm_market.getMarketTrends(productID);\n\n  const prompt = `Summarize the following product information:\\nFeatures: ${features}\\nReviews: ${reviews}\\nMarket: ${market}`;\n  const summary = await openai.complete(prompt);\n  return summary;\n}\n\n```\n\n**4. Diagnosis Agent for Decision Making:**\n\n* **Scenario:** A financial planning application where LLMs analyze income, expenses, market conditions, and risk tolerance.\n* **Implementation:** A JavaScript diagnosis agent (using a pre-trained LLM or a fine-tuned model via LoRA implemented in a library like transformers.js) integrates the analyses from different LLMs and makes a recommendation.  It might combine income projections, expense analysis, investment strategies suggested by other LLMs, and provide a final financial plan.\n\n```javascript\n// Example (Conceptual) using transformers.js\nasync function generateFinancialPlan(userData) {\n  // ... get analyses from specialized LLM agents\n  const combinedAnalysis = combineAgentOutputs(incomeAnalysis, expenseAnalysis, investmentStrategies, riskAnalysis);\n  const prompt = `Based on the following analysis, provide a comprehensive financial plan:\\n${combinedAnalysis}`;\n  const plan = await diagnosisLLM.generate(prompt);\n  return plan;\n}\n```\n\nThese are just a few examples.  By combining the insights from the paper with JavaScript frameworks and libraries, developers can create powerful and intelligent multi-agent web applications. Remember to address the limitations discussed in the paper, especially regarding computational resources and potential hallucinations. Continuous refinement and robust testing are essential for building reliable and effective LLM-based multi-agent systems.",
  "pseudocode": "No pseudocode block found. However, the paper describes algorithmic processes, particularly for the Navigation Agent and the process of generating trajectories for training the Diagnosis Agent. While not presented as formal pseudocode, these can be translated into conceptual JavaScript functions.\n\n**1. Navigation Agent's Importance Map Generation and Patch Selection**\n\n```javascript\nasync function getNextPatch(maskedWSI, previousDescriptions) {\n  // 1. Aggregate previous descriptions into a text embedding.\n  let aggregatedTextEmbedding = null;\n  if (previousDescriptions.length > 0) {\n      aggregatedTextEmbedding = await Promise.all(previousDescriptions.map(async (description) => {\n          return await t5Encoder.encode(description); // Assuming a T5 encoder model\n        }));\n\n      aggregatedTextEmbedding = aggregatedTextEmbedding.reduce((sum, embedding) => sum.add(embedding), tf.zeros(aggregatedTextEmbedding[0].shape)).div(previousDescriptions.length)\n  }\n\n  // 2. Generate importance map using U-Net.\n  const importanceMap = await unetModel.predict([maskedWSI, aggregatedTextEmbedding]); // Assuming tf.tensor inputs\n\n  // 3. Probabilistically sample the next patch based on the importance map.\n  const flattenedImportanceMap = importanceMap.flatten();\n  const probabilities = flattenedImportanceMap.div(tf.sum(flattenedImportanceMap));\n\n  const nextIndex = tf.multinomial(probabilities, 1).dataSync()[0]\n\n  const patchRow = Math.floor(nextIndex / importanceMap.shape[0]);\n  const patchCol = nextIndex % importanceMap.shape[0];\n\n  // 4. Extract the patch from the high-resolution WSI (not shown here, requires image processing).\n\n   const hrPatch = extractPatchFromHighResolutionWSI(patchRow, patchCol)\n\n  return { hrPatch, patchRow, patchCol };\n\n\n  \n}\n\n// Example usage (simplified):\nconst maskedWSI = tf.tensor(imageData); // Masked WSI image data as a tensor.\nconst previousDescriptions = [\"Spindle cells with oval nuclei...\", \"Inflammation...\"];\nconst {hrPatch, patchRow, patchCol} = await getNextPatch(maskedWSI, previousDescriptions);\n\n\nasync function extractPatchFromHighResolutionWSI(patchRow, patchCol){\n  // This would involve code specific to image processing\n  // and depends on how the WSIs are stored and accessed.\n  // A simplified placeholder using a dummy function.\n  return \"High resolution path at row: \"+ patchRow+ \" and col: \" + patchCol\n\n}\n```\n\n*   **Purpose:** This function mimics the core logic of the Navigation agent. It takes a masked WSI and previous textual descriptions as input and outputs coordinates of the next patch to examine. It uses a pre-trained U-Net model to generate an importance map, conditioned on aggregated text embeddings of the descriptions. Probabilistic sampling is then employed to select the next patch based on the importance map.\n\n**2. Generating Diagnostic Trajectories for Diagnosis Agent Training**\n\n\n```javascript\nasync function generateDiagnosticTrajectory(wsi) {\n  let trajectory = [];\n  let maskedWSI = wsi.clone(); // Start with the original WSI\n  let descriptions = [];\n  \n\n  for (let i = 0; i < 10; i++) { // Generate a trajectory of 10 patches\n\n    const {hrPatch, patchRow, patchCol} = await getNextPatch(maskedWSI, descriptions)\n\n    const description = await descriptionAgent.generateDescription(hrPatch); // Assuming a description agent\n    descriptions.push(description);\n\n    // Mask the selected patch on WSI (not shown here).\n    maskedWSI = maskPatch(maskedWSI, patchRow, patchCol); \n\n    trajectory.push(description);\n\n        // Paraphrase descriptions using LLaMA (not shown, requires LLaMA API call)\n        // for j < descriptions.length:\n        //   descriptions[j] = llama.paraphrase(descriptions[j])\n\n  }\n\n\n  return trajectory;\n}\n\nfunction maskPatch(maskedWSI, patchRow, patchCol) {\n  // Placeholder for a function that would mask the selected patch in WSI\n  return \"Masked WSI with row: \" + patchRow+ \" and column: \" + patchCol + \" masked\"\n}\n\n\n\n\n// Example usage:\nconst wsi = tf.tensor(wsiImageData)\nconst trajectory = await generateDiagnosticTrajectory(wsi)\n\n\n```\n\n*   **Purpose:**  This function simulates a pathologist's examination of a WSI by generating a sequence of patch descriptions (a \"trajectory\").  It iteratively calls the `getNextPatch` function, obtains descriptions from a description agent, masks previously selected regions, and paraphrases the descriptions for increased variability.\n\n**Key Points and Simplifications:**\n\n*   These JavaScript snippets focus on conveying the core algorithmic ideas. Real-world implementations would require integration with specific machine learning libraries (like TensorFlow.js), image processing tools, and potentially access to APIs for LLMs and VLMs.\n*   Details like image pre-processing, patch extraction from high-resolution WSIs, masking patches on the WSI, and paraphrasing using an LLM are represented by placeholder functions for clarity.\n*   TensorFlow.js is assumed for handling tensor operations.\n*   Error handling and more robust logic are omitted for brevity.\n\n\nThese JavaScript representations and explanations help bridge the gap between the research concepts described in the paper and practical implementation considerations for JavaScript developers working with LLM-based multi-agent systems. They illustrate how the described agents could interact and how data can be processed for training and prediction within such a framework.",
  "simpleQuestion": "Can AI agents improve medical image diagnosis?",
  "timestamp": "2025-02-14T06:08:40.422Z"
}