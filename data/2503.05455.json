{
  "arxivId": "2503.05455",
  "title": "Controllable Complementarity: Subjective Preferences in Human-AI Collaboration",
  "abstract": "Research on human-AI collaboration often prioritizes objective performance. However, understanding human subjective preferences is essential to improving human-AI complementarity and human experiences. We investigate human preferences for controllability in a shared workspace task with AI partners using Behavior Shaping (BS), a reinforcement learning algorithm that allows humans explicit control over AI behavior. In one experiment, we validate the robustness of BS in producing effective AI policies relative to self-play policies, when controls are hidden. In another experiment, we enable human control, showing that participants perceive AI partners as more effective and enjoyable when they can directly dictate AI behavior. Our findings highlight the need to design AI that prioritizes both task performance and subjective human preferences. By aligning AI behavior with human preferences, we demonstrate how human-AI complementarity can extend beyond objective outcomes to include subjective preferences.",
  "summary": "This research explores how to make AI agents better teammates by accounting for human preferences, specifically the desire for control over AI behavior.  Researchers developed Behavior Shaping (BS), a reinforcement learning method that allows humans to directly influence AI actions.  Experiments in a collaborative cooking game showed that humans preferred controllable AI partners and enjoyed working with them more, especially when the AI consistently adhered to the given control settings.  These findings highlight the importance of designing controllable and predictable multi-agent systems that align with human expectations, potentially increasing human-AI team performance and user satisfaction.  This is especially relevant for LLM-based multi-agent systems, where aligning with user intent and providing predictable responses are crucial for effective collaboration.  The ability to control individual agent behavior through BS-like mechanisms could enhance user trust and allow for more fine-grained control over complex multi-agent interactions.",
  "takeaways": "This paper explores how subjective human preferences, especially for controllability, impact human-AI collaboration. Here are some practical examples of how a JavaScript developer working on LLM-based multi-agent AI projects can apply these insights within web development scenarios:\n\n**1. Building Controllable LLM Agents in a Collaborative Web App:**\n\nImagine a web application for collaborative writing, similar to Google Docs, but with multiple LLM agents assisting users.  The paper’s concept of Behavior Shaping (BS) can be applied here.\n\n* **Scenario:** Users can set behavioral preferences for their LLM assistant (e.g., “formal tone,” “creative suggestions,” “focus on conciseness”).  These preferences act as the `w` weights in the BS algorithm, influencing the LLM's behavior.\n* **Implementation:**\n    * **Frontend (React, Vue, etc.):** Create a user interface with sliders or dropdowns allowing users to adjust weights corresponding to different writing styles. These weight values are then passed to the backend.\n    * **Backend (Node.js):**  Use a library like LangChain or LlamaIndex to manage interactions with the LLM.  The user-defined weights can be incorporated into the LLM prompt as context or instructions. For instance, append a string like `“Write in a formal tone with a focus on conciseness (weight: 0.8)”` to the prompt.\n    * **LLM Interaction:** The LLM generates text based on the prompt, reflecting the user-defined behavioral preferences.\n\n**2.  Multi-Agent Meeting Summarization with Controllable Focus:**\n\nConsider a web app that uses multiple LLM agents to summarize meeting transcripts. Each agent can be specialized in summarizing different aspects (e.g., action items, key decisions, sentiment analysis).\n\n* **Scenario:** Users control which aspects of the meeting each agent focuses on, creating a customizable summary.\n* **Implementation:**\n    * **Frontend:**  Provide checkboxes or a drag-and-drop interface for users to assign weights to different summary aspects for each LLM agent.\n    * **Backend:**  Use a framework like Socket.IO to manage real-time communication between the frontend, backend, and the LLMs.  Each agent receives a tailored prompt, incorporating the user-defined weights (e.g.,  `“Summarize action items with high priority (weight: 1.0), sentiment analysis with low priority (weight: 0.2)”`).\n\n**3.  Interactive Storytelling with Predictable LLM Characters:**\n\nDevelop a web-based interactive story where LLM agents represent different characters. Users interact with these characters, influencing the story's progression.  Predictability, as highlighted in the paper, is key for a coherent narrative.\n\n* **Scenario:**  Users can adjust character predictability using sliders, impacting how consistently characters adhere to their established personalities and backstories. This relates to the paper's H2.3, suggesting users will manipulate controls for predictability.\n* **Implementation:**\n    * **Frontend:** A visual novel-style interface where dialogue choices are presented to the user. Sliders control each character's predictability.\n    * **Backend:**  Store character profiles and backstories as JSON data. Use a library like TensorFlow.js to implement a simple \"personality model\" for each character. This model could be a neural network that takes user input and the predictability setting as input and outputs a probability distribution over possible dialogue responses.  Higher predictability settings would constrain the probability distribution closer to responses aligned with the character's established profile.\n\n**Key Considerations for JavaScript Developers:**\n\n* **Prompt Engineering:**  Carefully design LLM prompts to effectively incorporate the behavioral weights. Experiment with different approaches (e.g., using weights as direct instructions, adjusting temperature parameters, fine-tuning LLMs).\n* **Real-time Communication:** For multi-agent systems, frameworks like Socket.IO are essential for managing communication between agents and the frontend.\n* **UI/UX Design:** Create intuitive interfaces for users to control and understand agent behavior.  Visualizations and clear feedback mechanisms are important.\n* **Evaluation:** Collect user feedback to assess the effectiveness of the controllable agents and identify areas for improvement.  Metrics like user satisfaction, perceived effectiveness, and task completion time are relevant.\n\n\nBy considering these insights and examples, JavaScript developers can create LLM-powered multi-agent web applications that are not only effective but also aligned with human preferences, leading to more engaging and satisfying user experiences.",
  "pseudocode": "The provided research paper contains one pseudocode block, describing the Behavior Shaping (BS) training procedure:\n\n```javascript\nfunction behaviorShapingTraining(environment, agent, numEpisodes) {\n  // 1. Initialize behavioral reward functions\n  const behavioralRewardFunctions = [ \n    // Example: Onion in Pot\n    (state, action, weight) => {\n      if (action === 'put_onion_in_pot') {\n        return weight;\n      }\n      return 0;\n    },\n    // Example: Deliver Dish\n    (state, action, weight) => {\n      if (action === 'deliver_dish') {\n        return weight;\n      }\n      return 0;\n    },\n    // Example: Plate Dish\n    (state, action, weight) => {\n      if (action === 'plate_dish') {\n        return weight;\n      }\n      return 0;\n    }\n  ];\n\n\n  // 2. Initialize weight distributions (e.g., normal distribution)\n  const getWeights = () =>  behavioralRewardFunctions.map(() => randomNormal(0, 1)); // Assumes you have a randomNormal function\n\n\n  // 3. Training loop\n  for (let episode = 0; episode < numEpisodes; episode++) {\n    let state = environment.reset();\n    const weights = getWeights(); // Sample weights for this episode\n\n    for (let t = 0; t < environment.maxSteps; t++) {\n      // Get observations (This depends on the environment)\n      const observations = environment.getObservations(state);\n\n      // Agent selects action based on observations AND weights \n      const action = agent.getAction(observations, weights);\n\n\n      // Environment steps\n      const { nextState, reward, done } = environment.step(action);\n\n      // Calculate shaped reward\n      const shapedReward = reward + behavioralRewardFunctions.reduce((sum, func, index) => sum + func(state, action, weights[index]), 0);\n\n      // Update the agent (using your RL algorithm of choice, e.g., PPO)\n      agent.update(state, action, shapedReward, nextState, done);\n\n      state = nextState;\n\n      if (done) {\n        break;\n      }\n    }\n  }\n\n  return agent; // Return trained agent\n}\n\n\n// Helper function for sampling from a normal distribution (replace with your implementation)\nfunction randomNormal(mean, stddev) {\n  // ... your implementation for sampling from normal distribution\n}\n\n```\n\n**Explanation and Purpose:**\n\nThis algorithm aims to train a more robust and controllable reinforcement learning agent by incorporating *behavior shaping*.  Traditional RL agents learn to maximize a fixed reward. BS adds controllable behavioral components to the reward function.\n\n1. **Behavioral Reward Functions Initialization:**  The algorithm starts by defining a set of behavioral reward functions (`behavioralRewardFunctions`). Each function takes the current state, the agent's action, and a weight as input.  It returns a reward based on whether the action aligns with the desired behavior.  In the provided code, we created example reward functions reflecting those in the paper. You should adjust these based on the desired behaviours in your application.\n\n2. **Weight Distributions Initialization:** Each behavioral reward function is associated with a weight sampled from a distribution (`getWeights()`). In the paper, they use a normal distribution. This random sampling ensures the agent experiences diverse situations during training, making it more robust to unseen scenarios.\n\n3. **Training Loop:** The main training loop iterates through a specified number of episodes.  Inside each episode:\n    * Weights are sampled for each behavioral function.\n    * The agent interacts with the environment, taking actions based on its observations *and the current weights*.  This is crucial: the policy learns to condition its actions on both the environment state *and* the desired behavioral profile.\n    * The reward is calculated, including both the environment's base reward and the weighted sum of the behavioral rewards. This *shaped reward* guides the agent toward the overall desired behavior.\n    * The agent's policy is updated based on the shaped reward using a chosen RL algorithm (e.g., PPO, as mentioned in the paper).\n\n**Key Improvement of BS:**\n\nThe core idea is that by manipulating the `weights` at test time, you can control the agent's behavior. For example, setting a high positive weight for \"deliver_dish\" encourages the agent to prioritize deliveries. This control mechanism is not present in standard RL training.  This is especially relevant for LLM-based agents, where interpretability and controllability are key challenges. This method provides a structured approach to influence agent behavior during training, allowing for finer control at deployment.",
  "simpleQuestion": "How can AI learn human preferences for better collaboration?",
  "timestamp": "2025-03-10T06:08:31.665Z"
}