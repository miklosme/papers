{
  "arxivId": "2501.18944",
  "title": "O-MAPL: Offline Multi-agent Preference Learning",
  "abstract": "Inferring reward functions from demonstrations is a key challenge in reinforcement learning (RL), particularly in multi-agent RL (MARL), where large joint state-action spaces and complex inter-agent interactions complicate the task. While prior single-agent studies have explored recovering reward functions and policies from human preferences, similar work in MARL is limited. Existing methods often involve separate stages of supervised reward learning and MARL algorithms, leading to unstable training. In this work, we introduce a novel end-to-end preference-based learning framework for cooperative MARL, leveraging the underlying connection between reward functions and soft Q-functions. Our approach uses a carefully-designed multi-agent value decomposition strategy to improve training efficiency. Extensive experiments on SMAC and MAMuJoCo benchmarks show that our algorithm outperforms existing methods across various tasks.",
  "summary": "This paper introduces O-MAPL, a new algorithm for training multi-agent AI systems from human (or LLM) preferences instead of explicitly defined rewards.  It's particularly useful when specifying rewards is difficult, but demonstrating desired behavior is easier.  \n\nKey points for LLM-based multi-agent systems:\n\n* **Preference-based Learning:** O-MAPL learns directly from pairwise comparisons of agent behaviors (e.g., \"trajectory A is better than trajectory B\"), which can be provided by LLMs like GPT-4.\n* **End-to-End Training:**  It simplifies the training process by directly learning agent policies from preferences, avoiding the separate steps of reward modeling and policy optimization.\n* **Centralized Training with Decentralized Execution:**  Agents are trained together with a shared understanding, but can act independently once deployed.\n* **Value Factorization with Mixing Networks:**  Addresses the scalability challenges of multi-agent systems by decomposing complex value functions into smaller, agent-specific components, combined using mixing networks.\n* **LLM-Generated Preferences:**  Experiments showed the potential of using LLMs to provide preference data for training, leading to better performance than rule-based preferences, particularly in more complex scenarios.\n* **Data Efficiency:**  While promising, the method still relies on a considerable amount of preference data, raising the issue of sample efficiency as a key challenge for future research.",
  "takeaways": "This paper presents O-MAPL, an approach for training multi-agent systems using preference data instead of explicit rewards. Here's how a JavaScript developer can apply these insights to LLM-based multi-agent app development:\n\n**1. Collaborative Content Creation:**\n\n* **Scenario:** Imagine building a web app where multiple LLMs collaborate on writing a story.  Instead of defining a complex reward function for good storytelling, users can provide preferences on generated text passages (e.g., \"Passage A is more engaging than Passage B\").\n* **Implementation:**\n    * **Frontend:** Use a framework like React or Vue.js to create a user interface where story passages generated by different LLMs are presented side-by-side. Users can click buttons to indicate their preference.\n    * **Backend:** Node.js with a library like TensorFlow.js or WebDNN can implement the O-MAPL algorithm.  The frontend preference data feeds the training process, updating the LLM policies (generation strategies) directly without needing an explicit reward model. Langchain can be used to manage interactions between LLMs and external tools.\n\n**2. Multi-User Game AI:**\n\n* **Scenario:** Develop a real-time strategy game where each player is assisted by an LLM agent.  Collect preference data based on player choices during gameplay (e.g., \"Building a barracks now is better than building a farm\").\n* **Implementation:**\n    * **Frontend:**  A framework like Phaser or Babylon.js can handle game rendering and user input. Capture player actions as implicit preferences.\n    * **Backend:**  Node.js can collect gameplay data and preference signals. Use a library like TensorFlow.js or WebDNN to train the LLM agents based on this data using O-MAPL.\n\n**3. Decentralized Task Management:**\n\n* **Scenario:**  Build a project management app where multiple LLM agents handle different tasks (e.g., scheduling meetings, assigning tasks, generating reports). Preferences can be collected on task completion outcomes (e.g., \"Report A is more concise than Report B\").\n* **Implementation:**\n    * **Frontend:** Use React or Vue.js to visualize task progress and collect user feedback on task outcomes.\n    * **Backend:** Implement O-MAPL using TensorFlow.js or WebDNN within a Node.js environment. The preference data refines each agent's task handling policies.\n\n**4. Personalized Recommendation Systems:**\n\n* **Scenario:** Create an e-commerce site with multiple LLM agents recommending products.  Collect preference data by tracking user clicks, purchases, and ratings (e.g., \"Product X is preferred over Product Y\").\n* **Implementation:**\n    * **Frontend:**  Implement user tracking and preference collection using JavaScript and potentially a framework like React.\n    * **Backend:**  Node.js with TensorFlow.js or WebDNN trains the recommendation agents using O-MAPL based on user preferences, leading to personalized recommendations.\n\n**Key JavaScript Libraries & Frameworks:**\n\n* **TensorFlow.js/WebDNN:** For implementing neural networks and O-MAPL's core logic in the browser or on the server.\n* **React/Vue.js/Angular:** For building dynamic and responsive user interfaces.\n* **Node.js:** For building scalable backend services.\n* **Langchain.js:** For orchestrating LLM workflows.\n* **Game development libraries (Phaser, Babylon.js):** For handling game logic and rendering.\n\n\n**O-MAPL Advantages for JavaScript Developers:**\n\n* **Simplified Training:** No need to design complex reward functions; preference data is easier to obtain.\n* **End-to-End Learning:** Improved training stability and efficiency compared to two-phase approaches.\n* **Decentralized Execution:**  O-MAPL fits well with decentralized agent architectures common in web apps.\n\n\nBy incorporating these insights and utilizing appropriate JavaScript tools, developers can create more robust, adaptive, and user-aligned LLM-based multi-agent applications.  The shift to preference-based learning simplifies the development process and empowers users to directly shape the behavior of multi-agent AI systems in meaningful ways.",
  "pseudocode": "```javascript\n// O-MAPL Algorithm (JavaScript Adaptation)\n\nasync function oMAPL(theta, psi_q, psi_v, w_i, preferenceData) {\n  // 1. Input:\n  // theta: Parameters of the mixing network M_theta\n  // psi_q: Parameters of the local Q-value network q(o, a|psi_q)\n  // psi_v: Parameters of the local value network v(o|psi_v)\n  // w_i: Parameters of the local policy network pi_i(a_i|o_i; w_i)\n  // preferenceData: Offline dataset P of pairwise trajectory preferences\n\n  // 2. Output: \n  // Local optimized policies pi_i\n\n  for (let trainingStep = 0; trainingStep < numTrainingSteps; trainingStep++) {\n    // 3. Update q and theta to maximize L(psi_q, psi_v, theta)\n    const [new_psi_q, new_theta] = await maximizeL(psi_q, psi_v, theta, preferenceData);\n    psi_q = new_psi_q;\n    theta = new_theta;\n\n\n    // 4. Update v to minimize the Extreme-V J(psi_v)\n    const new_psi_v = await minimizeJ(psi_v, psi_q, theta, preferenceData);\n    psi_v = new_psi_v;\n\n\n    // 5. Update w_i to maximize the local WBC loss L_WBC(w_i)\n    const new_w_i = await maximizeWBC(w_i, psi_q, psi_v, theta, preferenceData);\n    w_i = new_w_i;\n\n\n  }\n\n  // 6. Return the trained local policies.\n  return getLocalPolicies(w_i); // Returns pi_i(a_i|o_i; w_i) for each agent i\n\n  // Helper functions (implementations not provided as they are complex and depend on\n  // specific deep learning libraries and the environment interactions)\n  async function maximizeL(psi_q, psi_v, theta, preferenceData) {/* ... */}\n  async function minimizeJ(psi_v, psi_q, theta, preferenceData) {/* ... */}\n  async function maximizeWBC(w_i, psi_q, psi_v, theta, preferenceData) {/* ... */}\n  function getLocalPolicies(w_i) {/* ... */} \n}\n\n\n\n```\n\n**Explanation of the O-MAPL Algorithm:**\n\nO-MAPL (Offline Multi-Agent Preference Learning) aims to train decentralized policies for multiple agents in a cooperative setting, using only offline data consisting of pairwise trajectory preferences (which trajectory is better).  It avoids explicit reward modeling, a common challenge in Reinforcement Learning. Instead, it directly learns agent policies from the preferences.\n\nThe core idea is to leverage the connection between reward functions and Q-functions in MaxEnt RL.  Instead of learning a reward function and *then* a Q-function based on it,  O-MAPL learns the Q-function directly from the preference data.\n\nHere's a breakdown of the algorithm's steps:\n\n1. **Input:**  The algorithm takes the parameters of various neural networks (local Q-networks `q`, local value networks `v`, mixing network `M_theta`, local policy networks `pi_i`), and the preference data as input.\n\n2. **Alternating Updates:** The algorithm performs alternating updates on the different network parameters in a loop:\n   - **Update `q` and `theta`:** Maximize the likelihood `L` of the observed preferences given the current Q-functions and mixing network. This step essentially learns how to represent the preferences in terms of Q-values.\n   - **Update `v`:** Minimize the \"Extreme-V\" loss `J`. This ensures consistency between the learned Q-function and value function `v`, which represents the expected cumulative future return. It approximates the log-sum-exp operation for computational efficiency.\n   - **Update `w_i`:** Maximize a weighted behavior cloning (WBC) objective. This step trains the decentralized policy networks `pi_i` to mimic the optimal behavior implied by the learned Q-function.  The weighting in WBC incorporates global information into the local policy updates.\n\n\n3. **Output:** After the training loop, the algorithm returns the trained local policies `pi_i` for each agent.\n\n\n**Purpose:**  The algorithm's purpose is to learn effective decentralized policies for multi-agent cooperative tasks, using only offline preference data. This is particularly useful when designing explicit reward functions is difficult or impractical. The algorithm's focus on preference-based learning and value decomposition makes it suitable for complex multi-agent scenarios with large state and action spaces.",
  "simpleQuestion": "Can I learn multi-agent preferences offline?",
  "timestamp": "2025-02-03T06:04:21.166Z"
}