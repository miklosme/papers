{
  "arxivId": "2504.09620",
  "title": "Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language Models via Decentralized Bayesian Inference",
  "abstract": "We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse knowledge of multiple vision-language models (VLMs) by learning from each other. Although existing methods that combine multiple models suffer from inference costs and architectural constraints, MHCG avoids these problems by performing decentralized Bayesian inference through a process resembling a language game. The knowledge fusion process establishes communication between two VLM agents alternately captioning images and learning from each other. We conduct two image-captioning experiments with two VLMs, each pre-trained on a different dataset. The first experiment demonstrates that MHCG achieves consistent improvement in reference-free evaluation metrics. The second experiment investigates how MHCG contributes to sharing VLMs' category-level vocabulary by observing the occurrence of the vocabulary in the generated captions.",
  "summary": "This paper introduces the Metropolis-Hastings Captioning Game (MHCG), a method for fusing the knowledge of multiple vision-language models (VLMs).  Inspired by emergent communication research, MHCG lets two VLMs act as agents, taking turns generating image captions and learning from each other's outputs via a decentralized Bayesian inference process.  This allows VLMs trained on different datasets to share knowledge without directly accessing each other's training data, improving performance on cross-dataset image captioning tasks.\n\nKey points for LLM-based multi-agent systems:\n\n* MHCG enables knowledge fusion between distinct LLMs without shared architectures or joint training, a common constraint in existing LLM ensemble methods.\n* The decentralized, game-like interaction allows agents to learn from each other while mitigating catastrophic forgetting of their original knowledge.\n*  The Bayesian approach offers a more principled way to combine LLM outputs compared to ad-hoc methods like simple averaging.\n* The concept of treating LLMs as agents in a communication game opens up new possibilities for multi-agent LLM application development.",
  "takeaways": "This paper introduces the Metropolis-Hastings Captioning Game (MHCG), a method for fusing knowledge between multiple Vision-Language Models (VLMs).  Here's how a JavaScript developer can apply these insights to LLM-based multi-agent projects, focusing on web development:\n\n**1. Decentralized Knowledge Sharing in Collaborative Web Apps:**\n\n* **Scenario:** Imagine building a collaborative design tool where multiple LLM agents assist users. Each agent specializes in a different design aspect (e.g., color palettes, typography, layout). MHCG allows these agents to refine their suggestions by learning from each other's outputs without direct access to each other's internal models.\n* **Implementation:**\n    * Represent each agent as a JavaScript object with its own LLM instance (e.g., using a browser-based LLM or through API calls to a server-side LLM).\n    * Implement the MHCG algorithm in JavaScript. The \"proposal\" step involves an agent generating a design suggestion (e.g., a CSS snippet).  The \"judgment\" step involves another agent evaluating the suggestion based on its own knowledge and a shared acceptance function. This evaluation can use metrics like CLIP-Score or domain-specific heuristics.\n    * Use a message-passing library (e.g., Socket.IO) for communication between agents.  This allows for decentralized communication, as described in the paper.\n\n**2. Content Moderation and Enrichment:**\n\n* **Scenario:**  Multiple LLMs moderate user-generated content. One LLM flags inappropriate content, another identifies key topics, and a third suggests relevant tags. MHCG can improve the consistency and accuracy of these agents by enabling them to learn from each other.\n* **Implementation:**\n    * Use a server-side JavaScript framework (e.g., Node.js) to host the agents.\n    * Each agent receives the user's content and generates its output. The MHCG algorithm facilitates the exchange and evaluation of these outputs.  The acceptance probability in MHCG can be based on consensus among agents or external validation data.\n\n**3. Personalized Recommendations in E-commerce:**\n\n* **Scenario:** LLM agents recommend products based on different criteria (e.g., user browsing history, trending items, similar products). MHCG allows these agents to refine recommendations by considering the outputs of other agents, leading to a more personalized and diverse set of suggestions.\n* **Implementation:**\n    * Integrate LLMs into the e-commerce platform's backend using a serverless architecture or Node.js.\n    * Each agent focuses on a specific recommendation strategy. MHCG enables these agents to learn from each other, improving the relevance and diversity of the final recommendations.\n\n**4. Interactive Storytelling and Game Development:**\n\n* **Scenario:** In a text-based adventure game, multiple LLMs control different non-player characters (NPCs). MHCG allows the NPCs to develop a shared understanding of the game world and react more consistently to player actions.\n* **Implementation:**\n    * Use a JavaScript game engine (e.g., Phaser) to manage game state and user interaction.\n    * Represent each NPC as an LLM agent that generates dialogue and actions.  MHCG facilitates communication and knowledge sharing between NPCs, leading to more engaging and believable interactions.\n\n**JavaScript Libraries and Frameworks:**\n\n* **TensorFlow.js/ONNX.js:**  For running LLMs directly in the browser.\n* **LangChainJS:** For simplifying the integration and management of LLMs within JavaScript applications.\n* **Socket.IO:** For real-time, bidirectional communication between agents.\n* **Node.js:** For building server-side applications to host and manage the multi-agent system.\n\n\nBy implementing these examples and leveraging the listed libraries, JavaScript developers can explore the potential of MHCG and multi-agent LLM systems in creating innovative and intelligent web applications.  The core idea is to decentralize knowledge, allowing agents to learn and adapt through interaction, leading to more robust and sophisticated applications.",
  "pseudocode": "The provided research paper contains pseudocode in Algorithm 1, which describes the Metropolis-Hastings Captioning Game (MHCG). Here's the JavaScript equivalent along with explanations:\n\n```javascript\nasync function metropolisHastingsCaptioningGame(agentA, agentB, numRounds, numObservations) {\n  // 1: Set pre-trained network parameters (assuming these are loaded into agentA and agentB)\n\n  for (let r = 0; r < numRounds; r++) {\n    // 2-8: Perception by both agents (using pre-trained encoders)\n    const latentRepresentations = [];\n    for (let d = 0; d < numObservations; d++) {\n      const observationA = getObservation(d, 'A'); // Replace with actual observation retrieval\n      const observationB = getObservation(d, 'B');\n      latentRepresentations.push({\n        A: await agentA.encodeImage(observationA),\n        B: await agentB.encodeImage(observationB)\n      });\n    }\n\n\n    let speaker = agentA;\n    let listener = agentB;\n\n    for (let k = 0; k < 2; k++) { // Two rounds per iteration, switching speaker/listener roles\n      for (let d = 0; d < numObservations; d++) {\n        // 12-14: Proposal by speaker agent (using the text decoder)\n        const proposedCaption = await speaker.decodeText(latentRepresentations[d][speaker.agentId]);\n\n        // 15-22: Judgment by listener agent (Metropolis-Hastings acceptance criterion)\n        const listenerLatent = latentRepresentations[d][listener.agentId];\n        const listenerCurrentCaption = await listener.decodeText(latentRepresentations[d][listener.agentId]);\n\n        const acceptanceProbability = await calculateAcceptanceProbability(proposedCaption, listenerLatent, listenerCurrentCaption, speaker, listener);\n        const randomNumber = Math.random();\n\n        if (randomNumber < acceptanceProbability) {\n          listener.updateCaption(listenerCurrentCaption, proposedCaption); // Assuming a method to store the accepted caption\n        }\n\n\n        // 24-31: Learning of text encoder and decoder, and image encoder and decoder. \n        //  This involves updating model parameters using gradient descent based on loss functions described in the paper (equations 4-8).\n        //  This requires access to training data (or experience replay buffer) and using an optimization library (not explicitly shown here).\n\n        await speaker.learn(latentRepresentations[d], proposedCaption, listener);\n        await listener.learn(latentRepresentations[d], proposedCaption, speaker);\n        \n      }\n      // Switch roles\n      [speaker, listener] = [listener, speaker];\n    }\n  }\n}\n\n\n\n// Helper function (simplified. Actual implementation depends on the specific probabilistic model and needs much more detail. See the original paper)\nasync function calculateAcceptanceProbability(proposedCaption, listenerLatent, listenerCurrentCaption, speaker, listener) {\n  const pProposed = await speaker.captionProbability(proposedCaption, listenerLatent);\n  const pCurrent = await listener.captionProbability(listenerCurrentCaption, listenerLatent);\n  \n  // (Simplified for illustration.  The original equation 3 includes other factors and prior probabilities not included in this simplification)\n  return Math.min(1, pProposed / pCurrent);\n\n\n}\n\n\n// Mock objects for agents (replace with actual agent implementation)\nconst agentA = {\n  agentId: 'A',\n  encodeImage: async (observation) => { /* Implementation for image encoding */ return 'z_a';},\n  decodeText: async (latent) => {/* Implementation for text decoding */ return 'caption_a';},\n  captionProbability: async (caption, latent) => {/* Implementation calculating caption probability */ return 0.5;},\n  learn: async (latentRepresentations, proposedCaption, otherAgent) => { /* Implementation for learning and updating model parameters */ },\n  updateCaption: (currentCaption, newCaption) => {}\n\n};\n\nconst agentB = {\n  agentId: 'B',\n  encodeImage: async (observation) => {/* Implementation for image encoding */ return 'z_b';},\n  decodeText: async (latent) => {/* Implementation for text decoding */ return 'caption_b';},\n  captionProbability: async (caption, latent) => {/* Implementation calculating caption probability */ return 0.7;},\n  learn: async (latentRepresentations, proposedCaption, otherAgent) => { /* Implementation for learning and updating model parameters */ },\n  updateCaption: (currentCaption, newCaption) => {}\n};\n\nconst numRounds = 30;  // Example, from the paper\nconst numObservations = 1; // Example: single image captioning\nmetropolisHastingsCaptioningGame(agentA, agentB, numRounds, numObservations);\n\n\n\nfunction getObservation(d, agentId) {\n  // Replace with your implementation to get the observation\n  // This could involve fetching data from a dataset or sensor\n  return `observation_${agentId}_${d}`; \n}\n```\n\n\n\n**Explanation of MHCG and its Purpose:**\n\nThe MHCG aims to improve the image captioning abilities of two pre-trained Vision-Language Models (VLMs) by enabling them to learn from each other's knowledge, which is assumed to be encoded in their respective parameters. This knowledge fusion addresses the limitations of standard VLM fusion methods like ensemble learning (high inference cost) and weight averaging (architecture constraints, potential performance degradation).\n\nThe core idea is inspired by the Metropolis-Hastings algorithm and the concept of emergent communication. Two agents (VLMs) engage in a \"captioning game.\" In each round, one agent acts as the \"speaker\" proposing a caption for an image, and the other acts as the \"listener,\" deciding whether to accept the proposed caption.  The listener's decision is probabilistic, based on the Metropolis-Hastings acceptance criterion (which considers the likelihood of the proposed caption given both agents' internal representations). Upon acceptance, the listener updates its internal parameters, effectively learning from the speaker's proposed caption.  This iterative process refines the captioning abilities of both agents, leading to better image descriptions and improved knowledge fusion.\n\n**Key Improvements over JavaScript Implementation compared to pseudocode:**\n\n* **Asynchronous operations:** VLM operations are asynchronous, using `async` and `await` which are crucial for practical implementations using modern JavaScript libraries.\n* **Helper functions:** The core logic of acceptance probability calculation is encapsulated into a separate function for clarity and reusability.\n* **Agent Objects:**  Represents agents as objects with methods for encoding, decoding, and learning, making the code more structured and easier to extend.\n* **Example Mock Objects:** Includes placeholder mock objects for agents to illustrate how the main function would be used.\n* **Observation Retrieval:** Includes a placeholder function `getObservation` for retrieving observations, making the code adaptable to various data sources.\n* **Comments:** More comments explain each step's purpose, relating it to the original pseudocode.\n\n\nThis improved JavaScript code provides a clearer, more practical, and extensible foundation for implementing the MHCG algorithm with real-world VLMs.  The key takeaway is the translation of the abstract algorithm into a more concrete form, suitable for JavaScript developers working on LLM-based multi-agent systems. Remember to replace the mock objects and placeholder functions with actual VLM implementations and data loading logic.",
  "simpleQuestion": "Can LLMs collaboratively improve image captioning?",
  "timestamp": "2025-04-15T05:05:13.891Z"
}