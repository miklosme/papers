{
  "arxivId": "2502.14815",
  "title": "Optimizing Model Selection for Compound AI Systems",
  "abstract": "Compound AI systems that combine multiple LLM calls, such as self-refine and multi-agent-debate, achieve strong performance on many AI tasks. We address a core question in optimizing compound systems: for each LLM call or module in the system, how should one decide which LLM to use? We show that these LLM choices have a large effect on quality, but the search space is exponential. We propose LLMSELECTOR, an efficient framework for model selection in compound systems, which leverages two key empirical insights: (i) end-to-end performance is often monotonic in how well each module performs, with all other modules held fixed, and (ii) per-module performance can be estimated accurately by an LLM. Building upon these insights, LLMSELECTOR iteratively selects one module and allocates to it the model with the highest module-wise performance, as estimated by an LLM, until no further gain is possible. LLMSELECTOR is applicable to any compound system with a bounded number of modules, and its number of API calls scales linearly with the number of modules, achieving high-quality model allocation both empirically and theoretically. Experiments with popular compound systems such as multi-agent debate and self-refine using LLMs such as GPT-40, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSELECTOR confers 5%-70% accuracy gains compared to using the same LLM for all modules.",
  "summary": "This paper explores how to choose the best large language model (LLM) for each step (\"module\") within a multi-step AI system (like a chatbot that first generates an answer, then refines it based on feedback).  The key idea is that different LLMs might excel at different subtasks.  Using a system called LLMSELECTOR, they demonstrate that carefully selecting the right LLM for each module, rather than using the same one for everything, significantly improves overall system performance (5-70% improvement in some cases). They accomplish this by using an LLM \"judge\" to estimate how well an LLM would perform on a given module.  This iterative process leads to better model allocation and overcomes the limitations of only tuning prompts or focusing on module interactions. The research is relevant to LLM-based multi-agent systems because it provides a method for optimizing the selection of LLMs within these complex systems, leading to greater efficiency and performance.",
  "takeaways": "This paper's core concept, LLMSELECTOR, offers exciting possibilities for JavaScript developers building multi-agent web apps with LLMs. Here's how a JavaScript developer can translate the insights into practical applications:\n\n**1. Building a Dynamic Task Allocation System:**\n\nImagine a project management web app where agents collaborate on tasks.  Instead of assigning all tasks to a single LLM, you can use LLMSELECTOR's principles.\n\n* **Modular Agents:** Design agents specialized in sub-tasks: brainstorming, task breakdown, progress tracking, etc.\n* **LLM Selection Logic:** Implement a JavaScript function that estimates an LLM's suitability for a given sub-task. This function could use simple heuristics (e.g., model size for complex tasks) or a dedicated \"diagnoser\" LLM as suggested in the paper. For the diagnoser, you could use a smaller, faster LLM.\n* **Dynamic Allocation:**  Based on the incoming task and LLM suitability scores, assign the task to the most appropriate agent/LLM. Libraries like LangChain provide convenient wrappers for interacting with different LLMs, making this switching relatively straightforward.\n\n**Example (Conceptual):**\n\n```javascript\n// Agent definitions (simplified)\nconst agents = {\n  brainstorm: { llm: 'gpt-3.5-turbo' }, // Default\n  breakdown: { llm: 'gpt-3.5-turbo' },\n  track: { llm: 'text-davinci-003' } // Better for concise summaries\n};\n\nfunction selectLLM(taskDescription) {\n  // Simple heuristic: longer descriptions -> larger model\n  if (taskDescription.length > 100) return 'gpt-4';\n  // ... other logic using a diagnoser LLM or external metrics\n  return 'gpt-3.5-turbo'; // Default\n}\n\n\nfunction allocateTask(task) {\n  const bestLLM = selectLLM(task.description);\n  if (task.type === 'brainstorm') {\n    agents.brainstorm.llm = bestLLM;\n    // Call brainstorm agent using agents.brainstorm.llm\n  } // ... similarly for other task types\n}\n```\n\n**2.  A/B Testing with Multi-LLM Front-Ends:**\n\nConsider an e-commerce website that uses LLMs for product descriptions, customer service chatbots, and personalized recommendations.\n\n* **Modularized Front-End Components:**  Build separate components for each feature.  Use a JavaScript framework like React, Vue, or Angular to manage these components effectively.\n* **LLM-Powered Variations:** For each component, create multiple versions powered by different LLMs.\n* **A/B Testing Framework:** Integrate an A/B testing library (e.g., Optimizely) to randomly serve different component variations to users. Track user engagement metrics (click-through rates, conversion rates, time on page) for each variation.\n* **Automated Model Selection:**  Based on the A/B testing results, automatically switch to the best-performing LLM for each component.\n\n**3. Building an Interactive Storytelling App:**\n\nImagine a collaborative storytelling app where multiple users contribute to a story. LLMs can be used to generate narrative suggestions, character dialogues, and plot twists.\n\n* **Agent Specialization:**  Design agents specialized in different narrative aspects: dialogue, description, plot development, etc.\n* **Contextual LLM Selection:** Use LLMSELECTOR's principles to choose the best LLM for each narrative step based on the current story context.\n* **User Interaction:** Allow users to rate or choose among different LLM-generated suggestions.  This feedback can be used to further refine the LLM selection process.\n* **Real-time Updates:**  Use WebSockets or similar technology to update the story collaboratively in real-time.\n\n**Key Considerations:**\n\n* **Cost Optimization:**  Balance the potential performance gains with the cost of calling multiple LLMs.  Experiment with smaller, less expensive models for tasks where performance isn't critical.\n* **Latency:** Switching between LLMs might introduce latency. Carefully optimize your JavaScript code and consider caching LLM responses to minimize delays.\n* **Monitoring and Evaluation:** Continuously monitor the performance of your multi-agent system and the effectiveness of your LLM selection logic. Adjust your strategy based on real-world data.\n\nBy applying these principles and adapting the JavaScript examples to your specific needs, you can build more sophisticated and efficient multi-agent AI web applications. Remember to carefully consider the practical implications of cost and latency when deploying these systems in production.  This is an evolving area of research, and experimentation is key to realizing the full potential of LLM-based multi-agent systems in web development.",
  "pseudocode": "```javascript\nfunction llmSelector(G, M, DTr, B) {\n  // Input:\n  //   G: Compound AI system architecture (represented as an object with nodes and edges)\n  //   M: Array of candidate LLMs (e.g., [\"GPT-4\", \"Claude-3.5\", ...])\n  //   DTr: Training dataset (array of {query: string, answer: string} objects)\n  //   B: Training budget (maximum number of LLM calls)\n\n  // Output:\n  //   f: Optimized model allocation (object mapping module IDs to LLM names)\n\n\n  // 1. Initialize with a random model allocation\n  let f = initializeModelAllocation(G, M); \n  let fz = {}; // Store allocation per data point\n  for (const z of DTr) {\n    fz[z.query] = {...f};\n  }\n  \n  let i = 1;\n  let cost = 0;\n  let stop = false;\n  const L = Object.keys(G.nodes).length; // Number of modules\n\n  // 2. Iterate until budget is reached or no improvement is possible\n  while (cost < B - M.length && !stop) {\n    const j = i % L; // Module to update in this iteration (0-indexed)\n\n    // 3. Find the best LLM for the selected module j \n    for (const z of DTr){\n        let bestLlmForModule = null;\n        let bestModulePerformance = -1;\n        for (const llm of M) {\n          fz[z.query][`module${j}`] = llm; // Try each LLM\n          const modulePerformance = await evaluateModulePerformance(G, fz[z.query], j, z);\n          if (modulePerformance > bestModulePerformance) {\n            bestModulePerformance = modulePerformance;\n            bestLlmForModule = llm;\n          }\n        }\n      fz[z.query][`module${j}`] = bestLlmForModule;\n    }\n      \n\n\n    // 4. Aggregate across all fz for current iteration's model allocation\n    f = aggregateModelAllocations(fz); \n\n    if (i > L) {\n      cost += M.length;\n      if (JSON.stringify(f) === JSON.stringify(aggregateModelAllocations(fz))) {\n              stop=true;\n            }\n    }\n\n    i++;\n  }\n\n  return f;\n}\n\n\n\n// Helper functions (placeholders - you'll need to implement these based on your specific setup):\n\nfunction initializeModelAllocation(G, M) {\n  // Randomly assigns an LLM from M to each module in G.\n  // ...\n}\n\nasync function evaluateModulePerformance(G, f, moduleId, dataPoint) {\n  // Evaluates the performance of a specific module (identified by moduleId) in the system G,\n  // using the model allocation f, on the given data point. This would typically involve an LLM call.\n  // Returns a numerical score (e.g., accuracy, F1-score, etc.).\n  // You'll likely use your LLM diagnoser here to assess module performance.\n  // ...\n}\n\n\nfunction aggregateModelAllocations(fz){\n    let aggregatedAllocation = {};\n    const L = Object.keys(Object.values(fz)[0]).length;\n    for (let j = 0; j < L; j++){\n        let llmCounts = {};\n        for (const allocation of Object.values(fz)){\n            const llm = allocation[`module${j}`];\n            llmCounts[llm] = (llmCounts[llm] || 0) + 1;\n        }\n        aggregatedAllocation[`module${j}`] = Object.keys(llmCounts).reduce((a, b) => llmCounts[a] > llmCounts[b] ? a : b); // Mode    \n    }\n\n    return aggregatedAllocation;\n}\n\n```\n\n**Explanation of the `llmSelector` Algorithm and its Purpose:**\n\nThe `llmSelector` algorithm aims to find the optimal assignment of Large Language Models (LLMs) to different modules within a compound AI system, maximizing the overall system's performance on a given task.  It addresses the challenge of selecting the best LLM for each module, as different LLMs may excel at different subtasks.\n\n**Algorithm Breakdown:**\n\n1. **Initialization:** The algorithm starts with a random allocation of LLMs to the modules in the compound system (`G`).\n2. **Iterative Optimization:** It then iteratively refines this allocation within a given budget (`B` - the maximum allowed number of LLM calls):\n   - **Module Nomination:** In each iteration, a module (`j`) is selected for optimization.\n   - **Model Selection:** The algorithm evaluates the performance of each candidate LLM (`M`) for the nominated module. It calls `evaluateModulePerformance`, which is a placeholder for your specific implementation. This function likely leverages your LLM diagnoser to estimate the module-wise performance with different LLMs.\n   - **Allocation Update:** The best-performing LLM is assigned to the nominated module.\n   - **Aggregation:** The code then takes the mode of model allocations across all training samples `fz` to derive the current best overall allocation `f`. \n   - **Stop Criteria:** The iterations continue until either the budget is exhausted or no further improvement in overall performance is observed within a certain number of iterations.\n3. **Return Optimal Allocation:** Finally, the algorithm returns the optimized model allocation `f`.\n\n**Purpose:**\n\nThe main purpose of the `llmSelector` algorithm is to automate the process of model selection in compound AI systems. It provides a systematic way to explore the space of possible LLM allocations, efficiently finding a high-performing configuration without needing to exhaustively test every combination. This is crucial because the search space grows exponentially with the number of modules, making exhaustive search impractical.  The algorithm also incorporates a budget constraint, which is important when dealing with the costs associated with calling commercial LLM APIs.",
  "simpleQuestion": "How best to choose LLMs for compound AI systems?",
  "timestamp": "2025-02-21T06:02:44.775Z"
}