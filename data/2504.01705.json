{
  "arxivId": "2504.01705",
  "title": "Sky of Unlearning (SOUL): Rewiring Federated Machine Unlearning via Selective Pruning",
  "abstract": "Abstract-The Internet of Drones (IoD), where drones collaborate in data collection and analysis, has become essential for applications such as surveillance and environmental monitoring. Federated learning (FL) enables drones to train machine learning models in a decentralized manner while preserving data privacy. However, FL in IoD networks is susceptible to attacks like data poisoning and model inversion. Federated unlearning (FU) mitigates these risks by eliminating adversarial data contributions, preventing their influence on the model. This paper proposes sky of unlearning (SoUL), a federated unlearning framework that efficiently removes the influence of unlearned data while maintaining model performance. A selective pruning algorithm is designed to identify and remove neurons influential in unlearning but minimally impact the model's overall performance. Simulations demonstrate that SoUL outperforms existing unlearning methods, achieves accuracy comparable to full retraining, and reduces computation and communication overhead, making it a scalable and efficient solution for resource-constrained IoD networks.",
  "summary": "This paper introduces SoUL (Sky of Unlearning), a system for removing specific data from a federated learning model trained by a network of drones (Internet of Drones or IoD). This addresses privacy concerns and allows for the removal of malicious data without retraining the entire model.  A key component is a selective pruning algorithm that removes only the parts of the neural network most affected by the unwanted data, preserving accuracy and efficiency.\n\nThe selective pruning algorithm enhances efficiency and reduces communication overhead, crucial aspects for LLM-based multi-agent systems. The decentralized nature of federated learning mirrors the distributed nature of multi-agent systems, making SoUL's approach relevant for managing updates and removing data from LLMs operating in a multi-agent environment.  The focus on minimizing communication overhead is particularly pertinent to large language models due to their size and computational demands.",
  "takeaways": "This paper introduces SOUL, a federated unlearning framework designed for resource-constrained environments like the Internet of Drones (IoD).  While the paper focuses on IoD, its core concepts of selective pruning for efficient unlearning are highly relevant to LLM-based multi-agent systems in web development. Here's how a JavaScript developer can apply these insights:\n\n**Scenario:** Imagine building a collaborative writing application with multiple LLMs acting as agents. Each agent specializes in a different writing style (e.g., formal, informal, creative). Users can select which agents contribute to their document.  Occasionally, users may want to \"unlearn\" the contribution of a specific agent to remove its stylistic influence without retraining the entire system.\n\n**Applying SOUL principles:**\n\n1. **Decentralized LLM Updates:**  Instead of having one monolithic LLM, each agent could be a smaller, specialized LLM running in the browser (client-side) or on a serverless function.  Agents would train locally on user data and periodically share updates (e.g., model weights or parameter deltas) with a central server.  This mirrors the distributed nature of SOUL. Libraries like TensorFlow.js can be used for client-side LLM training and inference.\n\n2. **Federated Unlearning with Selective Pruning:**  When a user requests to unlearn an agent's contribution:\n\n    * **Unlearning Model:** Create a separate \"unlearning model\" for the target agent.  Train this model on a synthetic dataset that mimics the agent's style but with randomized outputs.  This is similar to the random label assignment in SOUL.\n    * **Selective Pruning:** Instead of retraining all agents from scratch, analyze the weights of the main LLMs and the unlearning model.  Identify neurons (or parameters, in the context of LLMs) that are highly active in the unlearning model but less active in the main LLMs. These are the parameters that most strongly represent the stylistic influence to be removed.\n    * **Implementation in JavaScript:**  You could represent LLM weights as multi-dimensional arrays in JavaScript.  Use numerical libraries like NumJs to calculate L1 norms, percentiles, and implement the pruning mask logic described in the paper. This allows you to selectively zero out specific weights/parameters.  Libraries like Brain.js could be used for simpler LLM implementations where direct weight access is possible.\n    * **Server-Side Aggregation:** The central server then aggregates the pruned updates from all agents, effectively removing the unwanted stylistic influence.\n\n3. **Communication Efficiency:**  Selective pruning reduces the size of updates sent to the server, improving communication efficiency.  This is particularly relevant in web environments where bandwidth can be a constraint.\n\n**Practical Examples with Frameworks:**\n\n* **Node.js and Serverless Functions:** You can use Node.js and serverless platforms like AWS Lambda or Google Cloud Functions to implement the individual agents and the central server.\n* **Web Workers:**  For client-side processing, use Web Workers to offload LLM computations to separate threads, preventing UI blocking.\n* **React or Vue.js:** Use a frontend framework like React or Vue.js to manage the user interface and interact with the LLM agents.\n\n\n**Key Advantages for JavaScript Developers:**\n\n* **Improved performance:** Avoid expensive retraining, leading to faster unlearning.\n* **Enhanced privacy:**  Data remains distributed, aligning with growing privacy concerns.\n* **Scalability:** Easier to manage and scale with multiple, smaller LLMs compared to a single large model.\n\nBy adapting the principles of SOUL, JavaScript developers can create more efficient and user-centric multi-agent web applications powered by LLMs.  This approach allows for dynamic adaptation to user preferences and provides a framework for more granular control over the behavior of individual agents within a larger AI system.",
  "pseudocode": "The paper contains two pseudocode blocks which represent algorithms. Here are their JavaScript equivalents along with explanations:\n\n**Algorithm 1: SOUL (Sky of Unlearning)**\n\n```javascript\nasync function soul(model, totalClients, learningRate, unlearningData, unlearningClients, pruningThreshold) {\n  let globalModel = await model.initialize(); // Initialize global model\n  let unlearningModels = [];\n  for (let k = 0; k < totalClients; k++) {\n    unlearningModels.push(await model.initialize()); // Initialize unlearning models for each client\n  }\n\n  for (let round = 0; round < numRounds; round++) { // Global rounds (numRounds defined elsewhere)\n    let clientModels = [];\n    let clientUnlearningModels = [];\n\n    for (let k = 0; k < totalClients; k++) {\n      let clientModel = globalModel.clone(); // Clone the global model\n      let learningLoss = await model.computeLoss(clientModel, data[k]);\n      clientModel = await model.update(clientModel, learningLoss, learningRate);\n\n      if (unlearningClients.includes(k)) {\n        let unlearningModel = unlearningModels[k].clone();\n        let unlearningDataset = model.prepareUnlearningDataset(unlearningData[k]);\n        let unlearningLoss = await model.computeLoss(unlearningModel, unlearningDataset);\n        unlearningModel = await model.update(unlearningModel, unlearningLoss, learningRate);\n        clientModel = await model.prune(clientModel); // L1 pruning implementation assumed\n        unlearningModel = await model.prune(unlearningModel); // L1 pruning implementation assumed\n        clientUnlearningModels.push(unlearningModel);\n      }\n\n      clientModels.push(clientModel); \n    }\n\n    globalModel = await model.aggregate(clientModels);\n\n    // Unlearning on Server (if any unlearning requests)\n    if(clientUnlearningModels.length > 0) {\n        let aggregatedUnlearningModel = await model.aggregate(clientUnlearningModels);\n        let serverUnlearningLoss =  await model.computeUnlearningLoss(globalModel, aggregatedUnlearningModel) // Custom Loss implementation assumed\n        globalModel = await model.updateUnlearning(globalModel, serverUnlearningLoss, learningRate); \n        globalModel = await selectivePruning(globalModel, aggregatedUnlearningModel, pruningThreshold);\n    }\n\n     // Distribute the updated global model to all drones \n     // (Implementation depends on the communication framework)\n  }\n  return globalModel;\n}\n\nasync function selectivePruning(learningModel, unlearningModel, beta){\n    // Implementation as described in Algorithm 2 below.\n}\n```\n\n\n\n*Explanation:*  SOUL orchestrates Federated Unlearning (FU) in an Internet of Drones (IoD) setting. It iteratively trains a global model across multiple drones. When a drone requests to \"unlearn\" specific data,  a separate unlearning model is trained on modified data for that drone. This unlearning model's updates are incorporated into the global model at the server. The selective pruning algorithm optimizes this process to minimize the impact on the global model's performance.\n\n**Algorithm 2: Selective Pruning (SP)**\n\n```javascript\nasync function selectivePruning(learningParams, unlearningParams, beta) {\n  const l1Norm = (params) => params.reduce((sum, p) => sum + Math.abs(p), 0);  // Example L1 norm\n\n  const learningMagnitudes = learningParams.map(l1Norm);\n  const unlearningMagnitudes = unlearningParams.map(l1Norm);\n\n  const percentile = (arr, p) => arr.sort((a, b) => a - b)[Math.floor((p / 100) * arr.length)];\n  const learningThreshold = percentile(learningMagnitudes, beta);\n  const unlearningThreshold = percentile(unlearningMagnitudes, beta);\n\n\n  const unlearningMask = unlearningMagnitudes.map(mag => mag >= unlearningThreshold ? 1 : 0);\n  const learningMask = learningMagnitudes.map(mag => mag >= learningThreshold ? 1 : 0);\n\n\n  const pruningMask = unlearningMask.map((val, index) => val && !learningMask[index] ? 1 : 0);\n\n  const prunedParams = learningParams.map((param, index) => param * (1-pruningMask[index]) );\n\n  return prunedParams;\n}\n```\n\n*Explanation:* This algorithm identifies and removes the neurons (represented by weights/parameters) that are significantly active during the unlearning process but not as important for the original learning task. It uses L1-norm of weights as a measure of importance and a threshold based on a given percentile (`beta`). By applying this pruning mask, the function modifies the learning parameters to reduce the impact of the unlearned data while preserving the general model performance.\n\n\nKey Improvements and Assumptions in the JavaScript code:\n\n* **Asynchronous Operations:** The code uses `async` and `await` to handle potentially time-consuming operations like model initialization, loss computation, and updates.\n* **Model Abstraction:**  The code assumes a `model` object with methods like `initialize()`, `clone()`, `computeLoss()`, `update()`, `aggregate()`,`prune()` etc. These methods are not defined here but represent common machine learning operations. You would need to provide concrete implementations based on your chosen ML library (e.g., TensorFlow.js).\n* **Data Handling:** The code assumes data is pre-processed and accessible in a suitable format (e.g. `data[k]` for client k's data).\n* **L1 Pruning Implementation:** The `model.prune()` function represents the L1 pruning logic which is not explicitly defined in the original pseudocode.\n* **Communication Handling:** Distributing the global model to the drones is described as a comment, as its actual implementation would depend on the specific communication framework used in the IoD application.\n* **Unlearning Loss and Update:**  The concept of `computeUnlearningLoss` and `updateUnlearning` are higher level abstractions on the unlearning process on the server-side using the aggregated unlearning models from the clients.\n\nThis JavaScript translation provides a more concrete starting point for implementation but still requires integration with a specific machine learning library and adaptation to the details of your IoD environment.",
  "simpleQuestion": "How can I efficiently unlearn data in federated drone learning?",
  "timestamp": "2025-04-03T05:02:07.388Z"
}