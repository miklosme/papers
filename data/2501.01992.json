{
  "arxivId": "2501.01992",
  "title": "Disagree and Commit: Degrees of Argumentation-based Agreements",
  "abstract": "In cooperative human decision-making, agreements are often not total; a partial degree of agreement is sufficient to commit to a decision and move on, as long as one is somewhat confident that the involved parties are likely to stand by their commitment in the future, given no drastic unexpected changes. In this paper, we introduce the notion of agreement scenarios that allow artificial autonomous agents to reach such agreements, using formal models of argumentation, in particular abstract argumentation and value-based argumentation. We introduce the notions of degrees of satisfaction and (minimum, mean, and median) agreement, as well as a measure of the impact a value in a value-based argumentation framework has on these notions. We then analyze how degrees of agreement are affected when agreement scenarios are expanded with new information, to shed light on the reliability of partial agreements in dynamic scenarios. An implementation of the introduced concepts is provided as part of an argumentation-based reasoning software library.",
  "summary": "This paper introduces the concept of \"Disagree and Commit\" within multi-agent AI systems using formal argumentation.  It proposes a framework for measuring degrees of agreement among agents with differing preferences or using different reasoning methods (argumentation semantics), even when complete consensus isn't achievable.  It explores how these degrees of agreement change as new information is introduced, particularly in the context of value-based argumentation where agents' subjective values influence their reasoning.\n\nKey points for LLM-based multi-agent systems:\n\n* **Partial Agreements:**  The framework acknowledges that LLMs, like humans, might not always achieve full consensus, and offers a way to quantify and work with partial agreements.\n* **Value Alignment:** The emphasis on value-based argumentation is crucial for LLM agents, as aligning their values with human stakeholders is paramount. This research provides tools to model and measure this alignment.\n* **Dynamic Knowledge:**  The research investigates how agreements change with new information, a key aspect of LLM-based systems that constantly learn and update their knowledge. The framework offers ways to assess the reliability and stability of agreements in these dynamic settings.\n* **Simulating Disagreements:** The paper provides a software implementation that can be used to simulate disagreements between LLM agents, facilitating the study of value alignment, negotiation strategies, and the impact of new information on established agreements.",
  "takeaways": "This paper introduces the concept of \"Disagree and Commit\" for LLM-based multi-agent systems, allowing agents to reach partial agreements and proceed even without full consensus. Here's how JavaScript developers can apply these concepts in web development:\n\n**1. Building Collaborative Text Editors:**\n\n* **Scenario:** Multiple users collaboratively edit a document, each with an LLM agent suggesting edits and resolving conflicts.\n* **Implementation:**\n    * Represent the document's content as an abstract argumentation framework. Each sentence or paragraph can be an argument, and edits conflicting with existing content can be attacks.\n    * Use JavaScript libraries like `argdown-compiler` or a custom implementation to represent and manipulate the argumentation framework.\n    * Each user's LLM agent uses a chosen argumentation semantics (e.g., preferred, grounded) to determine acceptable edits.\n    * Calculate degrees of agreement (minimal, mean, median) between agents based on their proposed edits using the Hamming-based similarity measure.\n    * If a certain threshold of agreement is reached, automatically apply the agreed-upon edits. If not, highlight the conflicting edits for manual resolution by the users.\n* **Example:** User A's agent proposes to replace \"The cat sat\" with \"The feline lounged,\" while User B's agent suggests \"The cat slept.\" If the degree of agreement is below a threshold, both suggestions are highlighted for the users to decide.\n\n\n**2. Developing Interactive Storytelling Platforms:**\n\n* **Scenario:** Multiple users, each with an LLM agent representing a character, collaboratively create a story.\n* **Implementation:**\n    * Represent story elements (plot points, character actions, dialogue) as arguments in a value-based argumentation framework.\n    * Associate values like \"suspense,\" \"romance,\" \"humor\" with each story element.\n    * Each user's LLM agent has its own value preferences, influencing which story elements it promotes.\n    * Calculate degrees of agreement based on the proposed story elements and value preferences.\n    * The story progresses by incorporating elements with high agreement scores, allowing for diverse character motivations and partial conflicts to drive the narrative.\n* **Example:**  If Character A's LLM (valuing \"suspense\") suggests a sudden plot twist, while Character B's LLM (valuing \"romance\") proposes a heartfelt conversation, the platform could incorporate both, creating a complex narrative with elements of both suspense and romance, or let the users decide based on the degree of agreement score.\n\n**3. Creating Multi-User Project Management Tools:**\n\n* **Implementation:**\n    * Represent project tasks, deadlines, and resource allocation as arguments in an argumentation framework.\n    * Users' LLM agents can propose changes to the project plan, representing conflicting proposals as attacks.\n    * The \"Disagree and Commit\" framework can be used to reach partial agreements on project adjustments, even if there's not complete consensus.  A task can proceed if there's sufficient agreement on its core aspects, even if there are disagreements on minor details.\n* **Example:** If there's high agreement on task priorities but disagreement on the exact deadline, the task can be initiated with a provisional deadline, allowing the project to move forward while the deadline is further discussed.\n\n**JavaScript Frameworks and Libraries:**\n\n* **`argdown-compiler`:**  For parsing and visualizing argumentation frameworks, although you might need to extend it for advanced concepts like value-based argumentation.\n* **Custom implementations:**  Using standard JavaScript data structures to represent argumentation frameworks and implement the agreement algorithms and similarity measures.\n* **LLM integration:** Frameworks like `LangChainJS` for interfacing with LLMs.\n* **Frontend frameworks:** React, Vue.js, or Angular for building the user interface for collaborative editing, storytelling platforms, or project management tools.\n\n\n**Key Considerations for JavaScript Developers:**\n\n* **Scalability:**  Efficiently handling large argumentation frameworks is crucial. Consider optimization techniques.\n* **User interface:** Designing clear visualizations of argumentation frameworks and degrees of agreement.\n* **LLM prompt engineering:** Crafting effective prompts for LLMs to generate meaningful arguments and value preferences.\n* **Dynamic updates:**  Handling the dynamic addition of arguments and recalculating degrees of agreement efficiently.\n\nBy applying these insights, JavaScript developers can create innovative web applications that leverage the power of LLM-based multi-agent AI to achieve collaborative decision-making and problem-solving in various scenarios, even in the face of disagreement.",
  "pseudocode": "No pseudocode block found. However, several definitions and formulas can be interpreted algorithmically and translated to JavaScript. Here are a few key examples:\n\n**1. Intersection-based, Complement-based, and Hamming-based Similarity (Definition 7):**\n\n```javascript\nfunction iSimilarity(E, S, T) {\n  const intersectionSize = intersection(T, union(E, S)).length;\n  return intersectionSize === 0 ? 1 : intersection(T, E, S).length / intersectionSize;\n}\n\nfunction cSimilarity(E, S, T) {\n  const E_prime = difference(T, E);\n  const S_prime = difference(T, S);\n  const unionSize = intersection(T, union(E_prime, S_prime)).length;\n\n  return unionSize === 0 ? 1 : intersection(T, E_prime, S_prime).length / unionSize;\n}\n\n\n\nfunction hSimilarity(E, S, T) {\n  if (T.length === 0) return 1;\n\n  const E_prime = difference(T, E);\n  const S_prime = difference(T, S);\n\n\n  return (intersection(T, E, S).length + intersection(E_prime, S_prime).length) / T.length;\n}\n\n\n// Helper functions for set operations\nfunction union(...sets) {\n  return Array.from(new Set(sets.flat()));\n}\n\nfunction intersection(...sets) {\n  if (!sets.length) return [];\n  const firstSet = new Set(sets[0]);\n  return sets.slice(1).reduce((acc, set) => {\n    return acc.filter(x => new Set(set).has(x));\n  }, Array.from(firstSet));\n}\n\nfunction difference(setA, setB) {\n  const setB_Set = new Set(setB)\n  return Array.from(new Set(setA)).filter(x => !setB_Set.has(x));\n}\n\n\n// Example usage (from the paper):\nconst T = ['a', 'b', 'c'];\nconst E1 = ['a', 'b', 'c'];\nconst S1 = ['b', 'c'];\n\nconsole.log(\"i-similarity:\", iSimilarity(E1, S1, T));\nconsole.log(\"c-similarity:\", cSimilarity(E1, S1, T));\nconsole.log(\"h-similarity:\", hSimilarity(E1, S1, T));\n\n\nconst E2 = [];\nconst S2 = ['a'];\n\nconsole.log(\"h-similarity:\", hSimilarity(E2, S2, T));\n\n\n```\n\n* **Explanation:** These functions calculate the similarity between two sets of arguments (extensions) `E` and `S` with respect to a topic `T`.  They are fundamental for measuring agreement.  The helper functions provide basic set operations.\n\n\n\n**2. Two-Agent Degree of Satisfaction (Definition 8):**\n\n```javascript\nfunction twoAgentSatisfaction(AF, T, sigma, sigma_prime, similarityFunc) {\n  let maxSimilarity = 0;\n  for (const E of sigma_prime(AF)) {  // Assuming sigma_prime(AF) returns an array of extensions.\n    const similarity = Math.max(...sigma(AF).map(Ep => similarityFunc(Ep, E, T)))\n    maxSimilarity = Math.max(maxSimilarity, similarity);\n  }\n  return maxSimilarity;\n}\n\n// Example Placeholder (replace with actual argumentation framework and semantics)\nconst AF = { /* ... */ };\nconst T = ['a', 'b', 'c'];\nconst sigma = () =>  [[\"a\",\"c\"]];     // Placeholder - should compute extensions based on a semantics\nconst sigma_prime = () => [[\"b\"]];  // Placeholder - should compute extensions based on a semantics\n\n\nconsole.log(twoAgentSatisfaction(AF, T, sigma, sigma_prime, hSimilarity))\n\n```\n\n* **Explanation:**  This function calculates the degree of satisfaction between two agents represented by their respective argumentation semantics (`sigma` and `sigma_prime`).  It iterates through the extensions generated by `sigma_prime` and finds the maximum similarity to any extension generated by `sigma`, using the provided `similarityFunc`. This needs to be combined with a function (not defined in the paper or in this answer) that calculates argumentation framework extensions given a specific argumentation semantics, such as the ones also mentioned in the paper (grounded, preferred, stable, stage).\n\n\n**3. Degrees of Minimal, Mean, and Median Agreement (Definition 9):**\n\nImplementing these requires representing the Argumentation-based Agreement Scenario (AAS) in a suitable data structure and implementing the argumentation semantics functions (`sigma_i`).  The logic involves iterating through the powerset of the topic `T` and calculating the agreements for each subset. The concept of `median` is readily available through libraries or custom implementations.  This is a more extensive implementation and thus omitted in this answer; the key component for implementation are the calculation of degrees of satisfaction and the provided helper functions for median computation (which can also be found in many existing JavaScript libraries).\n\n\n\nThese JavaScript snippets provide a starting point for implementing the core concepts of the paper.  A full implementation would require further work, particularly in representing argumentation frameworks and implementing the various argumentation semantics. However, I hope this response helps in bridging the theoretical research with more practical JavaScript development for LLM-based multi-agent applications.",
  "simpleQuestion": "How can agents reach partial agreements reliably?",
  "timestamp": "2025-01-07T06:07:45.198Z"
}