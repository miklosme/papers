{
  "arxivId": "2410.03997",
  "title": "YOLO-MARL: YOU ONLY LLM ONCE FOR MULTI-AGENT REINFORCEMENT LEARNING",
  "abstract": "Advancements in deep multi-agent reinforcement learning (MARL) have positioned it as a promising approach for decision-making in cooperative games. However, it still remains challenging for MARL agents to learn cooperative strategies for some game environments. Recently, large language models (LLMs) have demonstrated emergent reasoning capabilities, making them promising candidates for enhancing coordination among the agents. However, due to the model size of LLMs, it can be expensive to frequently infer LLMs for actions that agents can take. In this work, we propose You Only LLM Once for MARL (YOLO-MARL), a novel framework that leverages the high-level task planning capabilities of LLMs to improve the policy learning process of multi-agents in cooperative games. Notably, for each game environment, YOLO-MARL only requires one time interaction with LLMs in the proposed strategy generation, state interpretation and planning function generation modules, before the MARL policy training process. This avoids the ongoing costs and computational time associated with frequent LLMs API calls during training. Moreover, the trained decentralized normal-sized neural network-based policies operate independently of the LLM. We evaluate our method across three different environments and demonstrate that YOLO-MARL outperforms traditional MARL algorithms.",
  "summary": "This research paper proposes a novel framework called YOLO-MARL (You Only LLM Once for Multi-Agent Reinforcement Learning) that enhances the training of multi-agent AI systems using large language models (LLMs). YOLO-MARL leverages the planning capabilities of LLMs to guide agents' decision-making without requiring constant interaction with the LLM during training. The key idea is to use the LLM once to generate a planning function that maps the system's state to optimal tasks for each agent. This function is then incorporated into the multi-agent reinforcement learning algorithm to provide additional rewards to agents based on their adherence to the planned tasks. This approach significantly reduces the computational overhead and communication instability associated with frequent LLM calls, while improving the overall performance and coordination of the multi-agent system.",
  "takeaways": "This paper introduces a novel concept, \"You Only LLM Once for MARL\" (YOLO-MARL), which significantly reduces the computational overhead and complexity of using LLMs in multi-agent reinforcement learning (MARL) scenarios. Here are some practical examples of how JavaScript developers could apply these insights to web-based multi-agent AI projects:\n\n**1. Collaborative Web-Based Games:**\n\n* Imagine building a real-time strategy game where multiple players controlled by LLMs collaborate to achieve a common goal. Directly using LLMs for each action would be extremely slow and expensive. \n* **YOLO-MARL Solution:**\n    * **Strategy Generation:** Use an LLM (e.g., through an API like Claude) to generate a high-level game strategy once at the start.  The prompt would describe the game rules, unit types, and victory conditions.\n    * **State Interpretation:**  Write a JavaScript function (using a library like TensorFlow.js) to convert the game state (unit positions, resources, etc.) into a format understandable by the LLM.\n    * **Planning Function:**  Have the LLM generate a JavaScript function (planning function) that takes the processed game state as input and outputs high-level task assignments for each agent (e.g., \"attack,\" \"defend,\" \"gather resources\"). \n    * **MARL Training:** Train smaller, more efficient MARL agents using a JavaScript library like ReinforceJS. These agents would receive the task assignments from the planning function and learn to execute them effectively within the game environment.\n\n**2. Decentralized Autonomous Agents for Web Applications:**\n\n* Consider a complex web application where multiple AI agents need to work together to manage tasks, such as content moderation, user support, or personalized recommendations.\n* **YOLO-MARL Solution:**\n    * **Strategy Generation:**  Define the overall goals and constraints of the system (e.g., \"maximize user engagement while minimizing harmful content\"). Use an LLM to generate an initial strategy.\n    * **State Interpretation:**  Develop a JavaScript module to process real-time user data, system logs, and other relevant information into a structured format for the LLM.\n    * **Planning Function:** The LLM generates a JavaScript function that takes this processed data and assigns roles or tasks to different agents (e.g., \"Agent 1 focuses on content filtering, Agent 2 handles user queries\").\n    * **MARL Training:**  Train individual MARL agents (potentially using Node.js and ReinforceJS) that specialize in their assigned tasks, learning to optimize their actions based on user feedback and system performance.\n\n**3. Collaborative Design and Creative Tools:**\n\n* Imagine a collaborative web-based design platform where multiple users, potentially assisted by LLM-powered agents, work together on a project (graphic design, music composition, etc.).\n* **YOLO-MARL Solution:**\n    * **Strategy Generation:** Use an LLM to define a high-level creative direction or style guide for the project based on initial user input.\n    * **State Interpretation:** Create JavaScript functions to interpret user actions on the design canvas, convert them into a representation the LLM can understand (e.g., \"User A added a blue rectangle,\" \"User B moved the circle to the left\").\n    * **Planning Function:** The LLM generates a JavaScript function that suggests design elements, color palettes, or creative actions for different agents (or users) based on the current state of the project. \n    * **MARL Training:** Use MARL agents to learn how to best assist users based on their preferences and previous interactions on the platform. This could involve suggesting tools, providing design feedback, or even generating variations of design elements.\n\n**JavaScript Libraries and Frameworks:**\n\n* **TensorFlow.js:** For processing data and potentially training MARL agents within the browser.\n* **ReinforceJS:** A reinforcement learning library for JavaScript that can be used to train agents.\n* **Node.js:** For building server-side components of your multi-agent system, especially for tasks like interacting with external LLMs through APIs.\n* **Socket.IO:** For real-time communication between agents and the web interface.\n* **Frontend Frameworks (React, Vue, Angular):** To build dynamic and interactive user interfaces for your web-based multi-agent application.\n\n**Benefits for JavaScript Developers:**\n\n* **Reduced LLM Costs and Latency:** YOLO-MARL makes LLM-powered multi-agent systems more practical by minimizing expensive LLM calls.\n* **Increased Efficiency:** Smaller, specialized MARL agents can react faster and adapt more quickly to changing environments.\n* **Scalability:** Decentralized agents trained with YOLO-MARL can handle complex tasks in larger web applications more efficiently.\n\nBy understanding and applying the principles of YOLO-MARL, JavaScript developers can unlock the potential of LLMs to create innovative and engaging web-based multi-agent AI applications.",
  "pseudocode": "```javascript\nfunction processState(observations, p = 2, f = 2) {\n  /**\n   * Processes the raw observation vector from the environment and structures it into\n   * a format that is more semantically meaningful for the Large Language Model (LLM).\n   *\n   * @param {Array} observations - An array representing the observation vector from the environment.\n   * @param {number} p - The number of agents in the environment.\n   * @param {number} f - The number of food items in the environment.\n   * @returns {Object} - An object containing structured information about food and agents in the environment.\n   */\n  let foodInfo = {};\n  let agentsInfo = {};\n  let obs = observations[0];\n  let offset = 0;\n\n  // Process food information\n  for (let foodIdx = 0; foodIdx < f; foodIdx++) {\n    let foodObs = obs.slice(offset, offset + 3);\n    offset += 3;\n    let currFoodPos = foodObs.slice(0, 2);\n    let currFoodLevel = foodObs[2];\n    let foodId = `food_${foodIdx}`;\n\n    // If food level is 0 and position is negative, food has been picked up\n    if (currFoodLevel === 0 && currFoodPos[0] < 0) {\n      foodInfo[foodId] = null;\n    } else {\n      foodInfo[foodId] = [currFoodPos, currFoodLevel];\n    }\n  }\n\n  // Process agent information\n  for (let agentIdx = 0; agentIdx < p; agentIdx++) {\n    let agentObs = obs.slice(offset, offset + 3);\n    offset += 3;\n    let currAgentPos = agentObs.slice(0, 2);\n    let currAgentLevel = agentObs[2];\n    let agentId = `agent_${agentIdx}`;\n    agentsInfo[agentId] = [currAgentPos, currAgentLevel];\n  }\n\n  return { foodInfo, agentsInfo };\n}\n```\n\n**Explanation:** This function is the **State Interpretation Module** described in the paper. Its purpose is to transform the raw, vector-based observation from the environment into a structured format that the LLM can easily understand and use to generate its planning function. \n\n**How it works:** \n\n1. **Input:** The function takes the raw observation vector, the number of agents, and the number of food items as input.\n2. **Food Processing:** It iterates through the food items, extracting their position and level from the observation vector. If a food item has a level of 0 and a negative position, it's considered picked up and marked as `null`. \n3. **Agent Processing:**  Similarly, it iterates through the agents, extracting their position and level.\n4. **Output:** The function returns an object containing two dictionaries: `foodInfo` with information about each food item and its status, and `agentsInfo` with information about each agent's position and level.\n\nThis structured output allows the LLM to comprehend the environment state in a semantically meaningful way, enabling it to generate more effective planning functions for the Multi-Agent Reinforcement Learning (MARL) training process.",
  "simpleQuestion": "Can LLMs improve MARL without constant calls?",
  "timestamp": "2024-10-08T05:02:38.758Z"
}