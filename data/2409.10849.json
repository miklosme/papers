{
  "arxivId": "2409.10849",
  "title": "SIFTOM: Robust Spoken Instruction Following through Theory of Mind",
  "abstract": "Abstract-Spoken language instructions are ubiquitous in agent collaboration. However, in human-robot collaboration, recognition accuracy for human speech is often influenced by various speech and environmental factors, such as background noise, the speaker's accents, and mispronunciation. When faced with noisy or unfamiliar auditory inputs, humans use context and prior knowledge to disambiguate the stimulus and take pragmatic actions, a process referred to as top-down processing in cognitive science. We present a cognitively inspired model, Spoken Instruction Following through Theory of Mind (SIFTOM), to enable robots to pragmatically follow human instructions under diverse speech conditions by inferring the human's goal and joint plan as prior for speech perception and understanding. We test SIFTOM in simulated home experiments (VirtualHome 2). Results show that the SIFTOM model outperforms state-of-the-art speech and language models, approaching human-level accuracy on challenging spoken instruction following tasks. We then demonstrate its ability at the task planning level on a mobile manipulator for breakfast preparation tasks.",
  "summary": "This paper introduces SIFTOM, a system that helps robots understand spoken instructions, even when those instructions are noisy or unclear. It does this by combining speech recognition (ASR) with a \"Theory of Mind\" (ToM) model. The ToM allows the robot to reason about what the human wants based on the context of the task and their actions, similar to how humans understand each other's intentions. \n\nFor LLM-based multi-agent systems, SIFTOM demonstrates:\n\n* **Robustness to noisy speech:** SIFTOM is more accurate at understanding instructions in noisy environments compared to systems that only use speech recognition. \n* **Contextual understanding:** By using visual information and ToM, SIFTOM can correctly interpret ambiguous commands where traditional ASR might fail.\n* **Efficiency in collaboration:** SIFTOM's ability to understand intentions leads to smoother and faster human-robot collaboration.",
  "takeaways": "This paper presents exciting opportunities for JavaScript developers venturing into the world of LLM-based multi-agent AI, especially within the context of web development. Here’s how you can apply these insights:\n\n**1. Building Robust Voice Assistants for Web Applications:**\n\n* **Scenario:** Imagine developing a voice-controlled project management dashboard. Users should be able to give natural language commands (e.g., \"Add a task to project X, deadline next Friday\").\n* **SIFTOM's relevance:** The paper highlights the limitations of traditional ASR in noisy environments. You can enhance your application's robustness by integrating SIFTOM-like reasoning.\n* **JavaScript Implementation:**\n    * **ASR:** Use JavaScript libraries like `Web Speech API` for speech-to-text.\n    * **LLM Integration:** Integrate a cloud-based LLM like OpenAI's API or explore self-hosted solutions like `transformers.js` for goal extraction from the transcribed text.\n    * **Contextual Reasoning (ToM):** This is where it gets interesting!  Use JavaScript to maintain a context object that tracks:\n        * User's current project\n        * Open tasks\n        * Recent interactions\n    * **Prioritize Actions:** If the ASR output is ambiguous (\"Add tusk\" instead of \"Add task\"), use the context object and LLM to infer the most likely intent (\"Add task\").\n\n**2. Collaborative Web Experiences with AI Agents:**\n\n* **Scenario:** Develop a multiplayer design tool where an AI agent can assist with tasks based on user conversations and actions.\n* **SIFTOM's contribution:** The paper demonstrates the power of combining visual cues (user actions on the design canvas) with speech input for more accurate intent understanding.\n* **JavaScript Application:**\n    * **Real-time Collaboration:** Use technologies like WebSockets (with libraries like `Socket.IO`) to synchronize user actions and AI agent responses.\n    * **Action Recognition:** Leverage JavaScript libraries for basic visual processing (e.g., shape detection, color analysis) to represent user actions.\n    * **Shared Context:**  Create a shared context object (similar to the previous example) that the LLM can access to reason about the most helpful action based on both conversation history and observed actions.\n\n**3. JavaScript Frameworks and Libraries:**\n\n* **TensorFlow.js:** For implementing and experimenting with custom machine learning models, including potential adaptations of SIFTOM's core concepts.\n* **Natural:** A JavaScript framework for building voice-controlled applications. You can integrate the principles of SIFTOM to make your applications more fault-tolerant.\n* **annyang:**  Another powerful JavaScript library for adding voice commands to your web projects.\n\n**Key Takeaways for JavaScript Developers:**\n\n* **Don't rely solely on ASR:**  Traditional ASR is prone to errors. Embrace context and multi-modal input (speech + visuals + user history) to build more robust AI agents.\n* **Context is King:**  Maintain a dynamic context object that encapsulates the current state of the application and user interactions. Use this to guide LLM responses and disambiguate ambiguous instructions.\n* **JavaScript is Ready:**  The JavaScript ecosystem has matured to the point where you have the tools and libraries to build sophisticated multi-agent applications with LLM capabilities. \n\nThis paper should inspire you to think beyond basic speech recognition and create truly intelligent web experiences. The future of web development involves agents that can understand us, even when we aren't perfectly clear.",
  "pseudocode": "```javascript\nfunction SIFTOM(planner, ASR, LLM, αη, Φ, θ, Ν, Κ, Ε) {\n  // Transcribe speech using ASR\n  const T = ASR(Φ); \n\n  // Query LLM to translate transcribed speech to a robot goal\n  let [gr, goalProbability] = LLM(gr, Ts, E); \n\n  // If LLM returns a plausible goal\n  if (goalProbability > 0) {\n    return gr; // Return the robot goal\n  } else {\n    // Infer most likely goals based on human actions \n    const [G1toK, goalProbabilities] = GoalInf(an, K);\n\n    // Sample most likely robot subgoals using a planner\n    const [g1toN, subgoalProbabilities] = planner(G) * goalProbabilities; \n\n    // Calculate posterior probabilities for each subgoal\n    const subgoalPosteriorProbabilities = []; \n    for (let i = 0; i < N; i++) { \n      const subgoalProbabilityFromSpeech = LLM(g[i], Ts, E);\n      const posteriorProbability = \n        subgoalProbabilities[i] * \n        αη * \n        subgoalProbabilityFromSpeech;\n      subgoalPosteriorProbabilities.push(posteriorProbability); \n    }\n\n    // Return the most likely robot subgoal\n    return g[indexOfMax(subgoalPosteriorProbabilities)]; \n  }\n}\n```\n\n**Explanation:**\n\nThis JavaScript code implements the SIFTOM (Spoken Instruction Following through Theory of Mind) algorithm for robustly understanding spoken instructions given to a robot, particularly in noisy environments.\n\n**Purpose:**\n\n- **Robust Instruction Understanding:** The primary goal of SIFTOM is to enable a robot to accurately interpret human instructions even when the speech recognition (ASR) output might be imperfect due to noise, accents, or other factors. \n- **Leveraging Context:**  It achieves this by incorporating both bottom-up (speech processing) and top-down (contextual reasoning) approaches, similar to how humans understand language.\n\n**Breakdown:**\n\n1. **Initialization:**\n   - `planner`: A function representing the robot's planner (determines actions to achieve goals).\n   - `ASR`: A speech recognition function (converts speech Φ to text).\n   - `LLM`: A large language model (understands language, maps text to goals).\n   - `αη`, `θ`, `Ν`, `Κ`, `Ε`: Parameters controlling goal inference and planning.\n\n2. **Speech Transcription (`T = ASR(Φ)`)**: The input speech is first transcribed into text.\n\n3. **Initial Goal Inference (`[gr, goalProbability] = LLM(...)`)**: The LLM tries to directly map the transcribed text to a robot goal (`gr`). If the probability (`goalProbability`) is above a threshold, it's considered reliable, and the goal is returned.\n\n4. **Contextual Reasoning (Top-down Stream):** If the initial goal inference is unreliable:\n   - **Goal Inference from Actions (`GoalInf`)**: The algorithm infers potential joint goals (human and robot) based on observed human actions (`an`).\n   - **Robot Subgoal Planning (`planner`)**: The planner generates likely robot subgoals (`g1toN`) given the inferred joint goals.\n   - **Bayesian Cue Integration:** The code combines the likelihood of subgoals from both speech (`subgoalProbabilityFromSpeech`) and planning (`subgoalProbabilities`) using Bayes' rule, weighted by a factor (`αη`). This produces a posterior probability for each subgoal.\n\n5. **Final Goal Selection:** The robot selects the subgoal with the highest posterior probability as the most likely interpretation of the human instruction.\n\n**In essence, SIFTOM enhances a robot's ability to follow instructions by not relying solely on potentially error-prone speech recognition but by also reasoning about the situation and the human's intent.**",
  "simpleQuestion": "How can LLMs understand noisy instructions?",
  "timestamp": "2024-09-18T05:01:24.561Z"
}