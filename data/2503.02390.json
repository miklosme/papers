{
  "arxivId": "2503.02390",
  "title": "ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for Reasoning Tasks",
  "abstract": "Multi-agent systems have emerged as a promising approach for enhancing the reasoning capabilities of large language models in complex problem-solving. However, current MAS frameworks are limited by poor flexibility and scalability, with underdeveloped optimization strategies. To address these challenges, we propose ReSo, which integrates task graph generation with a reward-driven two-stage agent selection process. The core of ReSo is the proposed Collaborative Reward Model, which can provide fine-grained reward signals for MAS cooperation for optimization. We also introduce an automated data synthesis framework for generating MAS benchmarks, without human annotations. Experimentally, ReSo matches or outperforms existing methods. ReSo achieves 33.7% and 32.3% accuracy on Math-MAS and SciBench-MAS SciBench, while other methods completely fail. Code is available at: ReSo",
  "summary": "ReSo is a new system designed to improve the reasoning abilities of Large Language Models (LLMs) by having them work together as a team (multi-agent system).  It breaks down complex problems into smaller parts, assigns each part to the best-suited LLM agent, and then combines the results. A key innovation is a “Collaborative Reward Model” that learns how well the agents are working together and uses this information to improve team performance over time.  ReSo also creates benchmark datasets to test multi-agent reasoning skills. This system achieves significantly better results on complex reasoning tasks compared to using single LLMs or existing multi-agent systems.  It dynamically assigns tasks, adapts to the strengths of different LLMs, and learns from data without hand-crafted instructions.  The results also showcase that ReSo is more efficient in terms of the number of tokens (pieces of text) it uses compared to other comparable systems.",
  "takeaways": "This paper introduces ReSo, a novel approach to building scalable and optimizable LLM-based multi-agent systems (MAS). Here's how a JavaScript developer can apply its insights to web development projects:\n\n**1. Dynamic Agent Database (DADB) in JavaScript:**\n\n* **Concept:** ReSo uses a DADB to store agent profiles (static traits like role, prompt, and available tools, and dynamic performance metrics like average reward and computational cost).  This allows for flexible agent selection based on task and performance history.\n* **JavaScript Implementation:**  You can implement the DADB using a JavaScript object or a more robust database solution like IndexedDB or a server-side database like MongoDB.  Each agent's profile would be a JSON object:\n\n```javascript\nconst agentDatabase = {\n  \"agent1\": {\n    \"model\": \"gpt-3.5-turbo\",\n    \"role\": \"CodeGenerator\",\n    \"promptTemplate\": \"Write JavaScript code for: {{task}}\",\n    \"tools\": [\"code editor API\"],\n    \"averageReward\": 0.8,\n    \"cost\": 0.01, // Could be token cost or time\n    \"taskCount\": 5\n  },\n  \"agent2\": { \n      // ... other agent profiles\n  }\n};\n```\n\n**2. Two-Stage Agent Selection with UCB and CRM:**\n\n* **Concept:** ReSo selects agents in two stages: (1) a coarse selection using Upper Confidence Bound (UCB) to balance exploration and exploitation of agents, and (2) fine-grained selection using a Collaborative Reward Model (CRM) to evaluate agent performance on specific subtasks.\n* **JavaScript Implementation:**\n\n```javascript\n// UCB Calculation (simplified)\nfunction calculateUCB(agent, task, totalSelections) {\n    const explorationConstant = 0.3; // Adjust as needed\n    return agent.averageReward + explorationConstant * Math.sqrt(Math.log(totalSelections) / agent.taskCount);\n}\n\n// Agent selection for a subtask\nfunction selectAgent(task, agentDatabase) {\n  const candidateAgents = Object.values(agentDatabase).sort((a, b) => calculateUCB(b, task, totalSelections) - calculateUCB(a, task, totalSelections)).slice(0, 3); // Top 3\n\n    // CRM evaluation (using a hypothetical CRM function)\n    const agentScores = candidateAgents.map(agent => ({ agent, score: evaluateCRM(agent, task) }));\n\n    const bestAgent = agentScores.sort((a, b) => b.score - a.score)[0].agent;\n\n  return bestAgent;\n}\n```\n\n**3. Task Decomposition in the Browser:**\n\n* **Concept:** ReSo decomposes complex tasks into a Directed Acyclic Graph (DAG) of subtasks.\n* **JavaScript Implementation:**  You could leverage existing JavaScript libraries for graph visualization and manipulation (e.g., Cytoscape.js, Vis.js) to display and manage the task graph in the browser. You could send the initial task to an LLM to generate the DAG, receive the DAG as JSON, and visualize it using these libraries.\n\n**4. Collaborative Reward Model (CRM) with LLMs:**\n\n* **Concept:** The CRM evaluates how well agents collaborate and solve subtasks.  It can be rule-based or another LLM.\n* **JavaScript Implementation:** Implement a rule-based CRM directly in JavaScript, or integrate with an LLM API.\n\n```javascript\n// Rule-based CRM (example: for code generation)\nfunction evaluateCRM(agent, task, agentOutput) {\n    // Check if the code compiles\n    try {\n      eval(agentOutput); // Be cautious with eval!\n      return 1; // Success\n    } catch (e) {\n      return 0; // Failure\n    }\n}\n\n// LLM-based CRM (using a hypothetical LLM API)\nasync function evaluateCRM(agent, task, agentOutput) {\n    const prompt = `Evaluate the following code for the task \"${task}\":\\n${agentOutput}`;\n    const llmResponse = await callLLM(prompt);  // Your LLM API call\n    return parseFloat(llmResponse); // Assuming LLM returns a score\n}\n\n```\n\n**5. Web Development Scenarios:**\n\n* **Multi-user collaborative code editor:** Multiple agents could assist users with code generation, debugging, and code review in real-time.\n* **Interactive storytelling or game development:** Agents could play different character roles, driving the narrative or gameplay based on user interactions.\n* **Personalized learning platforms:** Agents could adapt learning content and provide customized feedback to learners.\n* **Automated customer service chatbots:** A MAS could handle complex customer queries by routing subtasks to specialized agents.\n\n**Key Libraries and Frameworks:**\n\n* **LLM APIs:** LangChain.js, Llama.js (for local LLMs).\n* **Graph Visualization:** Cytoscape.js, Vis.js.\n* **Database:** IndexedDB, LocalForage (client-side), MongoDB, PostgreSQL (server-side).\n* **Frontend Frameworks:** React, Vue, Angular.\n\n\nBy combining these elements, JavaScript developers can create innovative and intelligent web applications that leverage the power of LLM-based multi-agent systems. Remember to address ethical considerations and potential biases in the agents and the CRM.  This technology has the potential to revolutionize web development but should be used responsibly.",
  "pseudocode": "```javascript\n// Task Graph Construction (ftask function - Section 3.2)\nfunction ftask(question) {\n  // 1. Use a fine-tuned LLM (e.g., Qwen2.5-7B-Instruct) to decompose \n  //    the question into subtasks and their dependencies (DAG format).\n  const dag = decomposeQuestionWithLLM(question); \n\n  // Example DAG structure (returned by decomposeQuestionWithLLM)\n  //  {\n  //    \"0\": [], // Subtask 0 depends on no other subtasks\n  //    \"1\": [0, 2], // Subtask 1 depends on subtasks 0 and 2\n  //    \"2\": []  // Subtask 2 depends on no other subtasks\n  //  }\n\n  //  The decomposeQuestionWithLLM function (not fully implemented here \n  //  as it requires LLM interaction) would handle the complex logic of:\n  //    - Breaking the question down into smaller, logical subproblems.\n  //    - Determining the order in which these subproblems need to be solved.\n  //    - Representing these relationships as a Directed Acyclic Graph (DAG).\n\n\n  // 2. Construct the graph object (G) from the returned DAG. \n  //    This part is simplified for illustration; a more robust implementation\n  //    would involve creating a Graph data structure.\n\n  const graph = {\n    nodes: Object.keys(dag).map(Number), // Subtask IDs become node IDs\n    edges: Object.entries(dag).flatMap(([taskId, dependencies]) => \n             dependencies.map(dep => ({ from: dep, to: Number(taskId) }))\n           )\n  };\n\n  return graph;\n}\n\n\n// Dynamic Agent Database Update (Section 3.4)\nfunction updateAgentProfile(agent, reward, cost) {\n  agent.taskCount++;\n  agent.averageReward = (agent.averageReward * (agent.taskCount - 1) + reward) / agent.taskCount;\n  agent.averageCost = (agent.averageCost * (agent.taskCount - 1) + cost) / agent.taskCount; \n}\n\n\n// UCB Calculation (Section 3.3.2)\nfunction calculateUCB(agent, subtask, totalSelections, explorationConstant = 0.3, epsilon = 0.01) {\n  const similarity = calculateSimilarity(agent, subtask);\n  const performance = agent.averageReward - costWeight * agent.averageCost; //Cost Weight defined elsewhere.\n  const qScore = similarity * performance;\n  return qScore + explorationConstant * Math.sqrt(Math.log(totalSelections) / (agent.taskCount + epsilon));\n}\n\n\n//Simplified Similarity Function (Heaviside)\nfunction calculateSimilarity(agent, subtask) {\n  const similarityScore = cosineSimilarity(agent.profileEmbedding, subtask.embedding); // Using cosine similarity, for instance.\n  return similarityScore >= similarityThreshold ? 1 : 0;\n}\n\n\n```\n\n**Explanation of Algorithms and their Purpose:**\n\n1. **`ftask(question)`:**  This function takes a natural language question as input and decomposes it into a Directed Acyclic Graph (DAG) of subtasks.  This DAG represents the dependencies between subtasks, specifying the order in which they must be solved.  The actual decomposition is assumed to be handled by a fine-tuned LLM (like Qwen2.5-7B-Instruct, as mentioned in the paper), which is abstracted away in the `decomposeQuestionWithLLM` function.\n\n2. **`updateAgentProfile(agent, reward, cost)`:** This function updates an agent's profile in the Dynamic Agent Database (DADB) based on the reward received for completing a subtask and the computational cost incurred.  It maintains a running average of rewards and costs, which are used in agent selection.\n\n3. **`calculateUCB(agent, subtask, totalSelections, explorationConstant, epsilon)`:**  This function calculates the Upper Confidence Bound (UCB) for an agent given a subtask.  UCB balances exploration (trying out potentially underutilized agents) and exploitation (choosing agents that have historically performed well). It incorporates similarity between agent and subtask, agent's past performance (reward minus a weighted cost), and a term that increases as the agent is selected less frequently.\n\n4. **`calculateSimilarity(agent, subtask)`:** This simplified function calculates the similarity between an agent's profile and a subtask, using a Heaviside function. If the similarity (calculated using cosine similarity in this example) is above a certain threshold, it returns 1; otherwise, it returns 0.  This determines whether the agent is considered a candidate for the subtask.\n\nThese JavaScript functions represent the core algorithms described in the ReSo paper, focusing on task decomposition, agent selection using UCB, and updating the DADB based on reward and cost. The LLM interaction for question decomposition and reward model evaluation are abstracted for clarity, but they are essential components of the overall ReSo system.",
  "simpleQuestion": "How can I build a scalable, reward-driven LLM multi-agent system?",
  "timestamp": "2025-03-05T06:04:50.749Z"
}