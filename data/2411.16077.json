{
  "arxivId": "2411.16077",
  "title": "SAGEval: The frontiers of Satisfactory Agent based NLG Evaluation for reference-free open-ended text",
  "abstract": "Large Language Model (LLM) integrations into applications like Microsoft365 suite and Google Workspace for creating/processing documents, emails, presentations, etc. has led to considerable enhancements in productivity and time savings. But as these integrations become more complex, it is paramount to ensure that the quality of output from the LLM-integrated applications are relevant and appropriate for use. Identifying the need to develop robust evaluation approaches for natural language generation, wherein references/ground labels doesn't exist or isn't amply available, this paper introduces a novel framework called SAGEval which utilizes a critiquing Agent to provide feedback on scores generated by LLM evaluators. We show that the critiquing Agent is able to rectify scores from LLM evaluators, in absence of references/ground-truth labels, thereby reducing the need for labeled data even for complex NLG evaluation scenarios, like the generation of JSON-structured forms/surveys with responses in different styles like multiple choice, likert ratings, single choice questions, etc.",
  "summary": "This paper introduces SAGEval, a novel framework for evaluating open-ended, reference-free text generated by LLMs, particularly focusing on complex formats like surveys and forms.  It uses a two-agent system: an initial LLM evaluator grades the text based on predefined criteria, and a second \"SAGE\" agent critiques those scores, suggesting adjustments and even new evaluation criteria. This approach aims to improve automatic evaluation by mimicking expert review processes and reducing reliance on labeled reference data, which is often lacking in real-world applications.  Key to LLM-based multi-agent systems is the SAGE agent's ability to refine initial evaluations, demonstrating a capacity for meta-evaluation and adaptation to open-ended text.  This multi-agent setup provides a more nuanced and human-aligned assessment compared to single-agent evaluation.",
  "takeaways": "This paper introduces SAGEval, a framework for evaluating open-ended, reference-free text generated by LLMs, particularly relevant for scenarios like generating forms, surveys, and lists. Here are practical examples of how a JavaScript developer can apply these insights to LLM-based multi-agent AI projects in web development:\n\n**1. Building Interactive Form Generators:**\n\n* **Scenario:** Imagine a web app where users describe a form they need, and the app generates it automatically.\n* **SAGEval Application:**  Use a first LLM agent (the \"Evaluator Agent\") to generate the form based on the user's input.  Implement a second LLM agent (the \"SAGE Agent\") in JavaScript using a library like LangChain or LlamaIndex.  The SAGE Agent critiques the Evaluator Agent's output based on criteria like clarity, completeness, relevance (to the user's description), user experience, and fairness (avoiding biased questions).  The SAGE Agent's feedback is then used to refine the form, potentially by prompting the Evaluator Agent for revisions.\n* **JavaScript Implementation:**\n    ```javascript\n    // Using LangChain (Illustrative)\n    import { LLMChain, PromptTemplate } from \"langchain\";\n    // ... (LLM setup) ...\n\n    const evaluationPrompt = new PromptTemplate({\n      template: \"Evaluate this generated form: {form}\\nCriteria: {criteria}\",\n      inputVariables: [\"form\", \"criteria\"]\n    });\n\n    const evaluationChain = new LLMChain({ llm: sageAgentLlm, prompt: evaluationPrompt}); \n\n    const feedback = await evaluationChain.call({ form: generatedForm, criteria: evaluationCriteria });\n    // ... (Process feedback and refine the form) ...\n    ```\n* **Framework Integration:** Integrate this system with a frontend framework like React or Vue.js to create a dynamic form-building interface.\n\n**2.  Automated Content Moderation for User-Generated Lists:**\n\n* **Scenario:** A community forum where users create lists of items (e.g., best movies, travel destinations).\n* **SAGEval Application:** Use a LLM agent to moderate these lists, checking for relevance to the forum's topic, semantic diversity (avoiding repetitive items), and fairness (no hateful or discriminatory entries). Implement the SAGE Agent to assess the quality of these moderation decisions.  For example, if the moderation agent flags a list as irrelevant, the SAGE Agent can review the decision, ensuring it's justified and not overly strict.\n* **JavaScript Implementation:** Similar to the form generation example, use LangChain or LlamaIndex to create the SAGE Agent in JavaScript. The SAGE Agent can be triggered whenever the moderation agent flags content.\n* **Node.js Integration:** Implement this as a serverless function in Node.js, triggered by database updates when new lists are posted.\n\n\n**3. Dynamic Survey Creation and Refinement:**\n\n* **Scenario:** A research platform where users can create and deploy surveys.\n* **SAGEval Application:**  Use an LLM to generate an initial survey based on user-defined goals. Then, use the SAGE Agent to analyze the initial draft, focusing on aspects like question clarity, audience understandability, question balance (avoiding leading questions), and potential biases.  The SAGE Agent's feedback is used to iteratively refine the survey before deployment.\n* **JavaScript Implementation:**  Use a JavaScript LLM library for both the survey generation and SAGE agent.  Consider a Node.js backend to handle the survey logic and data persistence.\n\n\n**Key JavaScript Considerations:**\n\n* **Asynchronous Operations:**  LLM interactions are asynchronous. Use `async/await` and promises effectively to manage these operations.\n* **Data Serialization:**  JSON is the standard format for exchanging data with LLMs. Ensure your data is correctly serialized and deserialized.\n* **Rate Limiting:**  Be mindful of LLM API rate limits. Implement strategies like queuing and caching to prevent issues.\n* **User Interface:** Provide clear feedback to users during the LLM interaction process.  Use loading indicators and progress bars appropriately.\n\n\nBy incorporating these principles and using appropriate JavaScript libraries and frameworks, developers can leverage the insights from the SAGEval paper to build more robust and user-friendly LLM-powered web applications.  The key takeaway is the use of a second LLM agent to critique and refine the output of the first, improving quality and addressing potential biases in the absence of pre-defined reference data.",
  "pseudocode": "The paper contains one formula, which can be considered analogous to pseudocode. Here's its JavaScript equivalent:\n\n```javascript\nfunction calculateWeightedScore(scores) {\n  let weightedScore = 0;\n  const n = scores.length;\n\n  for (let i = 0; i < n; i++) {\n    const score = scores[i];\n    const probability = calculateProbability(score, scores); // See next function\n    weightedScore += probability * score;\n  }\n\n  return weightedScore;\n}\n\nfunction calculateProbability(score, scores) {\n    const count = scores.filter(s => s === score).length;\n    return count / scores.length;\n}\n\n\n// Example usage:\nconst scores = [1, 2, 3, 4, 4, 5, 5, 5];\nconst finalScore = calculateWeightedScore(scores);\nconsole.log(finalScore); // Output will be a weighted average, skewed by frequency\n```\n\n**Explanation:**\n\nThe algorithm aims to calculate a weighted average score from an array of scores where the weight of each score is determined by its frequency within the array (its probability).  This mitigates potential skew from tied or unevenly distributed scores when multiple LLMs provide evaluations.  The `calculateProbability` helper function determines the frequency of a particular score within the provided array. `calculateWeightedScore` iterates through the array, calculating the weighted contribution of each score to the final weighted average. This approach is important in the context of LLM evaluations where avoiding bias in the individual LLM scorers is crucial for a more robust and reliable overall evaluation.",
  "simpleQuestion": "How can we reliably evaluate LLMs without ground truth?",
  "timestamp": "2024-11-26T06:05:56.555Z"
}