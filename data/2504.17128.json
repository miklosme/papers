{
  "arxivId": "2504.17128",
  "title": "PACE: A Framework for Learning and Control in Linear Incomplete-Information Differential Games",
  "abstract": "In this paper, we address the problem of a two-player linear quadratic differential game with incomplete information, a scenario commonly encountered in multi-agent control, human-robot interaction (HRI), and approximation methods for solving general-sum differential games. While solutions to such linear differential games are typically obtained through coupled Riccati equations, the complexity increases when agents have incomplete information, particularly when neither is aware of the other's cost function. To tackle this challenge, we propose a model-based Peer-Aware Cost Estimation (PACE) framework for learning the cost parameters of the other agent. In PACE, each agent treats its peer as a learning agent rather than a stationary optimal agent, models their learning dynamics, and leverages this dynamic to infer the cost function parameters of the other agent. This approach enables agents to infer each other's objective function in real time based solely on their previous state observations and dynamically adapt their control policies. Furthermore, we provide a theoretical guarantee for the convergence of parameter estimation and the stability of system states in PACE. Additionally, in our numerical studies, we demonstrate how modeling the learning dynamics of the other agent benefits PACE, compared to approaches that approximate the other agent as having complete information, particularly in terms of stability and convergence speed.",
  "summary": "This paper proposes PACE (Peer-Aware Cost Estimation), a framework for two AI agents to learn each other's goals during a continuous interaction, even when they start without knowing each other's intentions.  It focuses on scenarios where agents can observe the shared environment's state but not each other's direct actions.  PACE models both agents as learners, simulating each other's learning process to avoid biased estimations. Theoretical guarantees of convergence and stability are provided under specific conditions.\n\n\nKey points for LLM-based multi-agent systems:\n\n* **Incomplete Information:** Addresses the realistic scenario of agents needing to infer each other's goals from limited observations.\n* **Focus on Shared State:** Relies only on observing the shared environment state, which is often more practical than observing other agents' actions, particularly with LLMs where internal states are not always directly accessible.\n* **Model-Based Approach:** Leverages a simplified model-based approach (linear dynamics, quadratic costs) suitable for initial exploration and understanding in more complex LLM interaction scenarios.\n* **Learning Dynamics:** The core idea of modeling the learning dynamics of other agents could be potentially adapted to LLM-based systems by incorporating principles of how LLMs learn and adapt during interactions.\n* **Theoretical Foundation:** Offers theoretical guarantees which could serve as a starting point for developing more robust and predictable LLM-based multi-agent systems.",
  "takeaways": "This paper presents PACE (Peer-Aware Cost Estimation), a framework for multi-agent learning in scenarios with incomplete information, particularly relevant for LLM-based agents.  Here are some practical examples for JavaScript developers:\n\n**1. Collaborative Content Creation:**\n\n* **Scenario:** Imagine a web app where multiple LLM-based agents collaborate to write a story. Each agent has its own \"style\" (represented by its cost function/objective, unknown to others), learned and refined over time.  Initially, they might produce disjointed text. PACE allows them to learn each other's stylistic preferences (cost functions) by observing the generated text (shared state).\n* **Implementation:**\n    * **LLM Integration:** Use a JavaScript library like `langchain` to interface with your chosen LLMs.\n    * **Agent Representation:**  Each agent can be a JavaScript object with properties for its LLM instance, current cost function estimation, and update logic.\n    * **Shared State:** The evolving story text acts as the shared state.\n    * **PACE Implementation:** Implement the core PACE algorithm in JavaScript, calculating loss functions based on the text generated by each agent, updating beliefs about others' cost functions (e.g., using Tensorflow.js for gradient descent), and adjusting individual agent policies.\n    * **Visualization:** Use a frontend framework like React or Vue.js to dynamically visualize the story's evolution and individual agent contributions.\n\n\n**2. In-Game AI for Multi-Player Browser Games:**\n\n* **Scenario:**  Develop a real-time multi-player strategy game where LLM agents control different factions. Each faction has hidden objectives and strategies.  Using PACE, these LLM agents can infer each other's strategic preferences by observing game state changes (e.g., troop movements, resource allocation).\n* **Implementation:**\n    * **Game State:**  Represent the game state in a JavaScript object, accessible to all agents.\n    * **Agent Actions:** Use JavaScript functions to map LLM outputs to in-game actions.\n    * **PACE Integration:** Implement PACE to let agents learn about opponents.  Track game state changes in a history stack. Each agent can run a local simulation of other agents using their estimated cost functions, minimizing the difference between observed and simulated trajectories.\n    * **Framework Considerations:** Node.js and Socket.IO could manage real-time communication and game state updates.\n\n\n**3. Personalized Chatbots for Customer Service:**\n\n* **Scenario:** Multiple specialized chatbots handle different customer inquiries (e.g., billing, technical support, sales). Each chatbot has a slightly different objective (e.g., maximizing customer satisfaction vs. upselling). PACE can allow these chatbots to learn about each other’s objectives by observing customer interaction history (shared state) and adapt their responses accordingly.\n* **Implementation:**\n    * **Chatbot Framework:** Use a platform like `botpress` or build a custom chatbot system with Node.js.\n    * **Shared State:** Store conversation logs in a central database accessible by all chatbots.\n    * **PACE Algorithm:** Implement PACE in JavaScript. Each chatbot uses its estimated models of other chatbots and conversation histories to refine its understanding of other chatbot objectives. This improved understanding enables smoother handoffs between chatbots and more personalized responses based on the context of previous interactions with other bots.\n\n**Key JavaScript Considerations:**\n\n* **Numerical Computation:** Use libraries like `math.js` or `NumJs` for matrix operations, gradient calculations, and other numerical computations required by PACE.\n* **Asynchronous Operations:** Implement asynchronous operations for tasks like LLM inference, database updates, and agent belief updates to prevent blocking the main thread.\n* **Data Visualization:** Use JavaScript charting libraries like `Chart.js` or `D3.js` to visualize learning progress and agent behavior.\n\nBy implementing PACE, JavaScript developers can build more sophisticated and dynamic LLM-based multi-agent systems.  These agents will be able to adapt to each other in real-time, even with limited initial information, leading to more robust and collaborative behavior in various web applications.",
  "pseudocode": "```javascript\n// PACE Algorithm for Agent k in JavaScript\n\nclass Agent {\n  constructor(k, A, Bk, B_k, Qk, Q_k_initial, Pk_initial, P_k_initial, alpha, N) {\n    this.k = k; // Agent identifier (i or j)\n    this.A = A;\n    this.Bk = Bk;\n    this.B_k = B_k;\n    this.Qk = Qk;\n    this.Q_k = Q_k_initial; // Initial estimate of the other agent's Q\n    this.Pk = Pk_initial; // Initial Riccati solution for agent k\n    this.P_k = P_k_initial; // Initial estimate of the other agent's Riccati solution\n    this.alpha = alpha; // Learning rate\n    this.N = N; // History stack size\n    this.history = []; // History stack: [[x, uk, P_k, Pk], ...]\n    this._k = this.k === 'i' ? 'j' : 'i'; // Other agent's identifier\n  }\n\n  // ... (helper functions for matrix operations, Riccati equation solver, etc.) \n\n  policyUpdate(x) {\n      // Update own policy based on current estimates\n      this.Pk = this.solveRiccati(this.P_k, this.Qk); // Placeholder for Riccati solver\n      const uk = this.matrixMultiply(this.matrixMultiply(-this.Bk.transpose(), this.Pk),x); \n      return uk;\n  }\n\n  beliefUpdate(x) {\n    // Update history stack\n    this.history.push([x, this.policyUpdate(x), this.P_k, this.Pk]);\n    if (this.history.length > this.N) {\n      this.history.shift(); // Maintain stack size\n    }\n\n    // Trajectory generation and loss computation for each τ in history\n    let Lk = 0;\n    let L_k = 0;\n    for (const [tau_x, _, tau_P_k, tau_Pk] of this.history) {\n      const P_k_theta_k = this.solveRiccatiFromHistory(tau_P_k, tau_Pk,this.Q_k, this.Qk, tau_x);\n      const u_k_hat = this.matrixMultiply(this.matrixMultiply(this.matrixMultiply(-this.B_k.transpose(), P_k_theta_k), tau_x));\n      const x_k_hat = this.simulateDynamics(tau_x, u_k_hat, this.k === 'i' ? this.policyUpdate(tau_x): this.policyUpdate(tau_x), this.k, tau_x); // Simulate using estimated u_k\n      \n      const e_k = this.subtractMatrices(tau_x, x_k_hat);\n      Lk += this.normSquared(this.subtractMatrices(x, x_k_hat));\n      L_k += this.normSquared(e_k);\n\n    }\n    Lk /= this.history.length; //Normalize by history length\n    L_k /= this.history.length;\n\n   // Parameter updates (gradient descent) using calculated Losses from each history entry\n    this.Q_k = this.gradientDescent(this.Q_k, Lk, this.alpha); //Placeholder for Gradient Descent\n    // predict P_k for the next timestep.\n    this.P_k = this.solveRiccati(this.Pk, this.Q_k);\n\n\n  }\n\n\n\n  step(x) {\n    const uk = this.policyUpdate(x);\n    this.beliefUpdate(x);\n    return uk;\n  }\n}\n\n\n\n// Example usage (simplified): Initialize matrices, create agents, and run simulation.\nconst A = /* ... */; \nconst Bi = /* ... */; \nconst Bj = /* ... */; \nconst Qi = /* ... */; \nconst Qj = /* ... */;\n\nconst agent_i = new Agent('i', A, Bi, Bj, Qi, /* ... initial estimates ...,*/ 0.15, 35);\nconst agent_j = new Agent('j', A, Bj, Bi, Qj, /* ... initial estimates ...,*/ 0.15, 35);\n\nlet x = /* initial state */;\nfor (let t = 0; t < simulationSteps; t++) {\n  const ui = agent_i.step(x);\n  const uj = agent_j.step(x);\n  x = /* update state based on ui, uj */;\n} \n```\n\n**Explanation of the Algorithm and its Purpose:**\n\nThe PACE (Peer-Aware Cost Estimation) algorithm allows multiple agents (in this case, two) in a dynamic game to learn each other's cost functions online while simultaneously controlling their actions. It addresses scenarios where agents have incomplete information, meaning they are not initially aware of each other's objectives.  The key features are:\n\n1. **Policy Update:** Each agent updates its control policy based on its current estimate of its own and its peer's cost parameters. This involves solving a Riccati equation.\n\n2. **Belief Update:** This is the core of the learning process. Each agent maintains a history stack of past states, control actions, and Riccati solution estimates. They use this history to:\n   - **Trajectory Generation:** Estimate the trajectory of the system based on their current belief about the other agent's policy.\n   - **Loss Computation:** Calculate the difference between the estimated trajectory and the actual observed trajectory. This difference is used as a loss function.\n   - **Parameter Update:** Update their belief about the other agent's cost parameters using gradient descent to minimize the loss.  Crucially, they model the other agent *also* as a learner using a similar gradient-descent update, preventing biased estimations.\n\n3. **History Stack:**  This stack stores past data, ensuring the algorithm doesn't rely solely on immediate observations, providing a richer learning signal and improved stability.\n\n4. **Gradient Descent:** The algorithm uses gradient descent to update the estimates of the other agent's cost function parameters.  Although the loss function is non-convex, gradient descent is empirically shown to be effective in these situations.\n\n\nThe JavaScript code provides a more detailed, implementation-oriented view of the algorithm. It shows how the concepts from the paper translate into practical code structures, using a class-based representation for agents, methods for policy updates, belief updates, and helper functions for matrix operations (which are assumed to be implemented elsewhere). This adaptation aims to make the algorithm more accessible to JavaScript developers interested in implementing multi-agent systems.",
  "simpleQuestion": "Can LLMs learn opponent costs in real-time games?",
  "timestamp": "2025-04-25T05:03:58.791Z"
}