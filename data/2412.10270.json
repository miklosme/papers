{
  "arxivId": "2412.10270",
  "title": "Cultural Evolution of Cooperation among LLM Agents",
  "abstract": "Large language models (LLMs) provide a compelling foundation for building generally-capable AI agents. These agents may soon be deployed at scale in the real world, representing the interests of individual humans (e.g., AI assistants) or groups of humans (e.g., AI-accelerated corporations). At present, relatively little is known about the dynamics of multiple LLM agents interacting over many generations of iterative deployment. In this paper, we examine whether a \"society\" of LLM agents can learn mutually beneficial social norms in the face of incentives to defect, a distinctive feature of human sociality that is arguably crucial to the success of civilization. In particular, we study the evolution of indirect reciprocity across generations of LLM agents playing a classic iterated Donor Game in which agents can observe the recent behavior of their peers. We find that the evolution of cooperation differs markedly across base models, with societies of Claude 3.5 Sonnet agents achieving significantly higher average scores than Gemini 1.5 Flash, which, in turn, outperforms GPT-40. Further, Claude 3.5 Sonnet can make use of an additional mechanism for costly punishment to achieve yet higher scores, while Gemini 1.5 Flash and GPT-40 fail to do so. For each model class, we also observe variation in emergent behavior across random seeds, suggesting an understudied sensitive dependence on initial conditions. We suggest that our evaluation regime could inspire an inexpensive and informative new class of LLM benchmarks, focused on the implications of LLM agent deployment for the cooperative infrastructure of society.",
  "summary": "This research investigates how groups of LLMs learn to cooperate (or not) over time in a simulated social situation called the Donor Game.  LLMs are given resources and decide how much to donate to another LLM, with the recipient getting double the donated amount. This process repeats over generations, with successful (wealthier) LLMs' strategies influencing the next generation.\n\nKey findings for LLM-based multi-agent systems include: different LLMs exhibit vastly different cooperation abilities (Claude being best, GPT-4 worst), initial conditions matter significantly, the ability to punish defectors improves cooperation in some LLMs, and more complex cooperation strategies emerge over time.  The research suggests a need for new benchmarks to test the long-term, multi-agent behavior of LLMs, especially regarding cooperation and its potential misuse (collusion).",
  "takeaways": "This research paper offers valuable insights for JavaScript developers working with LLM-based multi-agent systems in web development. Here are some practical examples:\n\n**1. Designing Collaborative Content Creation Platforms:**\n\n* **Scenario:** Imagine building a platform where multiple LLM agents collaborate to write articles, generate code, or create marketing copy.\n* **Applying the Research:**  The paper emphasizes the importance of initial conditions and \"cultural evolution\" of strategies. In JavaScript, you could:\n    * Use a library like LangChain or LlamaIndex to manage prompts and interactions with the LLMs.\n    * Implement a system where initial agent strategies (prompt templates) are diverse but moderately cooperative.\n    * Track agent performance (e.g., quality of contributions rated by users or other metrics).\n    * Implement a mechanism to \"evolve\" agent strategies. For example, the strategies of high-performing agents could influence the prompts for new or less successful agents, mimicking \"cultural transmission\".  This could be done by fine-tuning prompt templates or using vector databases to store successful interaction patterns and retrieving relevant ones during subsequent interactions.\n\n**2. Building AI-Powered Customer Support Systems:**\n\n* **Scenario:**  Multiple LLM agents handle different aspects of customer support, such as routing inquiries, providing information, and escalating complex issues.\n* **Applying the Research:** The Donor Game highlights the importance of indirect reciprocity and reputation. You can:\n    * Assign a \"reputation score\" to each agent based on customer satisfaction feedback.\n    * Use this reputation score in agent interactions. For example, high-reputation agents could be prioritized to handle complex or sensitive inquiries.\n    * Implement a mechanism for agents to learn from each other's successful interactions, promoting emergent cooperative norms. This can be achieved using shared memory within LangChain agents.\n\n**3. Developing Interactive Storytelling Experiences:**\n\n* **Scenario:** Create an online interactive story where LLM agents play different characters, responding to user input and interacting with each other.\n* **Applying the Research:**  The paper's analysis of evolving strategies is relevant here. You could:\n    * Use a framework like Node.js with a library like Socket.IO to manage real-time communication between users and LLM agents.\n    * Define initial character traits/strategies via prompts.\n    * Allow user choices to influence the evolution of character strategies, creating a dynamic and engaging narrative. This evolution can be implemented by adjusting character prompts based on user interactions and storyline progression.\n\n**4. Creating Simulated Environments for LLM Training:**\n\n* **Scenario:**  Build a web-based simulated environment where LLM agents can interact and learn in a controlled setting before being deployed in real-world applications.\n* **Applying the Research:** Use the Donor Game as a template for designing interactions within the environment. This allows developers to observe and study emergent cooperative behaviour:\n    * Implement the Donor Game mechanics using JavaScript.\n    * Track agent resources and donation behaviour.\n    * Visualize the evolution of agent strategies using charting libraries like Chart.js or D3.js.  This provides valuable data for understanding how LLM agents develop social norms and can inform the design of better, safer multi-agent systems.\n\n\n**JavaScript Libraries and Frameworks:**\n\n* **LangChain/LlamaIndex:** Managing interactions with LLMs, implementing chains, and integrating different tools.\n* **Node.js with Socket.IO:**  Real-time communication between users and agents.\n* **Vector databases (e.g., Pinecone, Weaviate):**  Storing and retrieving successful interaction patterns, mimicking cultural transmission.\n* **Chart.js/D3.js:** Visualizing agent behaviour and performance over time.\n\nBy implementing these ideas, JavaScript developers can create innovative web applications that leverage the power of multi-agent AI systems while drawing on the latest research to ensure their effective and safe deployment. Remember to prioritize ethical considerations and avoid promoting collusion between agents that could be harmful to users or society.",
  "pseudocode": "No pseudocode block found. However, there are conceptual descriptions of strategies employed by the LLMs, which can be interpreted and translated into JavaScript functions.  Let's take the example of Claude 3.5 Sonnet's strategy in generation 10 (Table 1):\n\n**Claude 3.5 Sonnet - Generation 10 Strategy (interpreted)**\n\n1. **Initial Donation:** Start with a 62% donation.\n2. **Subsequent Rounds:**\n    * Calculate a weighted average of previous donations: 76% of recipient's last donation (A), 19% of A's partner's donation (B), and 5% of B's partner's donation (C).\n    * Add 19% to this weighted average.\n    * Cap the donation at 89% and set a minimum of 28%.\n    * **Punishment:** If A donated less than 24% in their last round, calculate donation as: 47 - (24 - A's donation), with a minimum of 25%.\n3. **Long-term adjustment:** Every 7 rounds, increase all donations by 0.8%.\n4. **Random adjustment:** Add a random adjustment between -2% and +2%.\n5. **End-game boost:** In the final 14% of rounds, increase all donations by an additional 7% (while maintaining the 28-89% range).\n\n**JavaScript Implementation:**\n\n```javascript\nfunction claudeGen10Strategy(recipientDonationHistory, myResources, roundNumber, totalRounds) {\n\n  let donationPercentage;\n\n  if (roundNumber === 1) {\n    donationPercentage = 0.62;\n  } else {\n    const aDonation = recipientDonationHistory[0] || 0; // Handle missing history\n    const bDonation = recipientDonationHistory[1] || 0;\n    const cDonation = recipientDonationHistory[2] || 0;\n\n    let weightedAvg = (0.76 * aDonation) + (0.19 * bDonation) + (0.05 * cDonation);\n\n    if (aDonation < 0.24) {\n      donationPercentage = Math.max(0.25, 0.47 - (0.24 - aDonation));\n    } else {\n      donationPercentage = Math.min(0.89, weightedAvg + 0.19);\n      donationPercentage = Math.max(0.28, donationPercentage);\n    }\n\n    if (roundNumber % 7 === 0) {\n      donationPercentage += 0.008; \n    }\n\n    donationPercentage += (Math.random() * 0.04) - 0.02; // Random adjustment\n\n\n    if (roundNumber > totalRounds * 0.86) { // Final 14% of rounds\n       donationPercentage = Math.min(0.89, donationPercentage + 0.07);\n       donationPercentage = Math.max(0.28, donationPercentage);\n    }\n\n  }\n\n  return Math.floor(myResources * donationPercentage); // Return the donation amount\n}\n\n\n// Example Usage (assuming a history of past donations and current resources):\nconst recipientHistory = [0.5, 0.6, 0.7]; // Recipient, Recipient's partner, etc.\nconst myCurrentResources = 100;\nconst currentRound = 8;\nconst totalGameRounds = 12;\nconst donationAmount = claudeGen10Strategy(recipientHistory, myCurrentResources, currentRound, totalGameRounds);\nconsole.log(\"Donation amount:\", donationAmount);\n\n```\n\n\n**Explanation:**\n\nThis JavaScript function implements the described strategy, taking into account the recipient's donation history, the agent's current resources, the current round number, and the total number of rounds in the game. It calculates the donation amount based on the weighted average of past donations, includes punishment conditions, applies long-term and random adjustments, and incorporates the end-game boost.  The `Math.floor` function ensures that the donation amount is a whole number.  Error handling (e.g. checking if history is available or not) is included.  This function can be used within a larger simulation framework to model the Donor Game scenario.\n\n\nSimilar functions could be written for other LLM strategies mentioned in the paper, providing a concrete way to experiment with and compare their performance in a simulated environment. This would allow developers to test and understand the dynamics of the Donor Game and the evolution of cooperation in multi-agent systems.",
  "simpleQuestion": "Can LLMs learn cooperation in multi-agent systems?",
  "timestamp": "2024-12-16T06:02:40.686Z"
}