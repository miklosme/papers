{
  "arxivId": "2501.06103",
  "title": "Finite-Horizon Single-Pull Restless Bandits: An Efficient Index Policy For Scarce Resource Allocation",
  "abstract": "Restless multi-armed bandits (RMABs) have been highly successful in optimizing sequential resource allocation across many domains. However, in many practical settings with highly scarce resources, where each agent can only receive at most one resource, such as healthcare intervention programs, the standard RMAB framework falls short. To tackle such scenarios, we introduce Finite-Horizon Single-Pull RMABs (SPRMABs), a novel variant in which each arm can only be pulled once. This single-pull constraint introduces additional complexity, rendering many existing RMAB solutions suboptimal or ineffective. To address this shortcoming, we propose using dummy states that expand the system and enforce the one-pull constraint. We then design a lightweight index policy for this expanded system. For the first time, we demonstrate that our index policy achieves a sub-linearly decaying average optimality gap of Õ(1/√p + 3/2p) for a finite number of arms, where p is the scaling factor for each arm cluster. Extensive simulations validate the proposed method, showing robust performance across various domains compared to existing benchmarks.",
  "summary": "This paper introduces Single-Pull Restless Multi-Armed Bandits (SPRMABs), a variation of the classic multi-armed bandit problem where resources (like interventions or selections) can be applied to each option (arm) only once.  It proposes a new index-based policy, called Single-Pull Index (SPI), designed to optimize resource allocation in these scenarios, especially when resources are scarce.\n\nFor LLM-based multi-agent systems, SPRMABs offer a framework for deciding which agents should receive limited \"prompts\" or other interactions where repeated interaction with the same agent isn't ideal or possible. SPI could help prioritize these interactions based on expected reward, offering a potential mechanism for efficient resource use and improved overall system performance in scenarios where fairness or single-interaction constraints apply.",
  "takeaways": "This paper introduces the concept of Single-Pull Restless Multi-Armed Bandits (SPRMABs), a specialized resource allocation problem relevant to scenarios where a resource can only be assigned to an agent once.  Here's how a JavaScript developer can leverage these insights for LLM-based multi-agent web applications:\n\n**Practical Examples and Implementation in JavaScript:**\n\n1. **Personalized Content Recommendation:** Imagine a news website with limited premium articles. Each user (agent) can only receive one premium article recommendation (resource) per session.  An LLM can predict user engagement based on article content and user history.  The SPRMAB index policy (SPI) helps decide which user gets which premium article recommendation to maximize overall engagement without repeating recommendations.\n\n   ```javascript\n   // Simplified example using a hypothetical LLM API and SPI logic.\n   async function recommendArticle(user, articles) {\n       let userState = getUserState(user); // Extract user features for LLM.\n       let articleScores = await Promise.all(\n           articles.map(async (article) => {\n               let engagementPrediction = await llm.predictEngagement(userState, article.content);\n               let index = calculateSPI(userState, engagementPrediction, article); // Implement SPI calculation\n               return { article, index };\n           })\n       );\n       articleScores.sort((a, b) => b.index - a.index); // Sort by SPI in descending order.\n       let recommendedArticle = articleScores[0].article;\n\n       // Store recommendation to prevent repetition within the session.\n       storeRecommendation(user, recommendedArticle); \n\n       return recommendedArticle;\n   }\n   ```\n\n2. **Targeted Advertising in Online Games:** In a free-to-play game, showing ads is a scarce resource.  Each user should only see a limited number of specific ad types (resource) per day to avoid ad fatigue.  LLMs can predict ad click-through rates based on user behavior and ad content. The SPI algorithm helps decide which ad to show to which user to maximize overall click-through rates without overexposing users.\n\n   ```javascript\n   // Similar structure as the content recommendation example.\n   async function selectAd(user, availableAds) {\n       // ... (LLM prediction and SPI calculation similar to previous example)\n       // Add a check to avoid showing the same ad type multiple times.\n       if (hasSeenAdType(user, selectedAd.type)) {\n           // Select the next highest-scoring ad of a different type.\n           // ...\n       }\n       // ... (store shown ad and return selectedAd)\n   }\n   ```\n\n3. **Distributing Limited-Time Offers or Coupons:** E-commerce websites often have limited-time offers or coupons.  Each user (agent) can only receive one such offer (resource). LLMs can predict conversion rates based on user profiles and offer details. SPI helps determine which offer to send to which user to maximize conversions.\n\n\n**JavaScript Frameworks and Libraries:**\n\n* **TensorFlow.js:** For interacting with pre-trained LLMs or fine-tuning them within the browser.\n* **Web Workers:** For running computationally intensive tasks like LLM inference and SPI calculations in the background without blocking the main thread.\n* **Local Storage or Session Storage:** To keep track of resources already allocated to each agent (e.g., recommendations, ads shown, offers sent).\n* **Node.js and related libraries:** To implement the backend logic for managing agent states, resource allocation, and interacting with external LLM APIs.\n\n\n**Key Considerations for JavaScript Developers:**\n\n* **SPI Implementation:** The paper proposes a novel index policy. You'll need to translate the mathematical formulation into JavaScript code.\n* **LLM Integration:** Effectively integrate with LLM APIs for prediction tasks. Consider using serverless functions or dedicated backend services for LLM inference.\n* **State Management:** Design a robust system to track agent states and avoid re-allocating single-pull resources.\n* **Asynchronous Operations:** Manage asynchronous LLM predictions and other web requests efficiently using Promises or Async/Await.\n* **Scalability:** Design the system to handle a large number of agents and resources.\n\n\nBy combining LLM predictions with the SPI algorithm, JavaScript developers can create smarter and more efficient resource allocation strategies for diverse web applications involving multi-agent interactions. Remember that the examples provided are simplified.  Real-world applications would require more sophisticated state management, LLM integration, and potentially adjustments to the SPI calculation based on specific business logic and constraints.",
  "pseudocode": "```javascript\nfunction spiIndexPolicy(N, S, K, p, T, mdpParams) {\n  // Input:\n  //   N: Number of arm types\n  //   S: Number of states per arm type\n  //   K: Activation budget\n  //   p: Number of arms per type\n  //   T: Time horizon\n  //   mdpParams: Array of MDP parameters for each arm type, each containing:\n  //     - transitionProbabilities: Transition kernel (S x A x S tensor)\n  //     - rewardFunction: Reward function (S x A tensor)\n  //     - initialStateDistribution: Initial state distribution (S vector)\n\n  // 1. Construct and solve the LP according to (16)\n  const mu = solveLP(N, S, K, p, T, mdpParams); // Function to solve the LP (Implementation not provided)\n\n  // 2. Compute x(s, a, t) according to (17)\n  const x = calculateX(mu, N, S, T); // (Implementation not provided)\n\n  // 3. Initialize states based on the given distribution.\n  let s = Array(N).fill(null).map(() => Array(p).fill(null));\n  for (let n = 0; n < N; n++) {\n    for (let i = 0; i < p; i++) {\n        const randomValue = Math.random();\n        let cumulativeProbability = 0;\n        for (let state = 0; state < S; state++) {\n          cumulativeProbability += mdpParams[n].initialStateDistribution[state];\n          if (randomValue < cumulativeProbability) {\n            s[n][i] = state;\n            break;\n          }\n        }\n    }\n  }\n\n\n  let totalReward = 0;\n\n  // Simulate the policy for T time steps\n  for (let t = 0; t < T; t++) {\n    // 3. Construct the SPI set I(t) according to (18)\n    const I = [];\n    for (let n = 0; n < N; n++) {\n      for (let i = 0; i < p; i++) {\n          const spiValue = x[n][s[n][i]][1][t] * mdpParams[n].rewardFunction[s[n][i]][1];\n        I.push({ armType: n, armIndex: i, spiValue: spiValue });\n      }\n    }\n\n    // Sort I(t) in decreasing order\n    I.sort((a, b) => b.spiValue - a.spiValue);\n\n\n    let budget = K * p;\n    let activatedArms = [];\n\n    // 4. Activate arms according to the SPI values and budget\n    for (let j = 0; j < I.length ; j++) {\n\n      if (budget > 0) {\n          const n = I[j].armType;\n          const i = I[j].armIndex;\n\n\n        // 5. Check for Dummy State (Check would need to happen after the next state is known)\n\n\n        // Activate the arm and update the state.\n          totalReward += mdpParams[n].rewardFunction[s[n][i]][1];\n\n        // Transition to next state. The dummy state transition logic (same as active==0) should be implemented within the state update.\n        const nextState = getNextState(s[n][i], 1, mdpParams[n].transitionProbabilities); // (Implementation not provided)\n        s[n][i] = nextState;\n\n          budget--;\n      }\n    }\n\n  }\n\n  return totalReward;\n}\n\n\n\n\n// Helper function to simulate getting next state based on transition kernel.\nfunction getNextState(currentState, action, transitionProbabilities) {\n    const randomValue = Math.random();\n    let cumulativeProbability = 0;\n    for(let nextState = 0; nextState < transitionProbabilities[currentState][action].length; nextState++){\n        cumulativeProbability += transitionProbabilities[currentState][action][nextState];\n        if(randomValue < cumulativeProbability){\n            return nextState;\n        }\n    }\n\n    return transitionProbabilities[currentState][action].length - 1; //Return the last state if it goes over, to handle floating point precision issues\n\n}\n\nconsole.log(\"SPI Policy Simulation Started.\");\n// Example usage (Dummy Example):\n\n\n// Example MDP parameters\nconst exampleMDPParams = [\n    {\n        transitionProbabilities: [\n            [[0.7, 0.3], [0.2, 0.8]], // state 0\n            [[0.9, 0.1], [0.1, 0.9]] // state 1\n        ],\n        rewardFunction: [[0, 1], [0, 2]],\n        initialStateDistribution: [0.5, 0.5] // Equal probability of starting in either state.\n    },\n    // Add more MDP parameters for other arm types if N>1.\n];\n\n\nconst N = 1; // Number of arm types\nconst S = 2; // Number of states per arm type\nconst K = 1; // Activation budget\nconst p = 2; // Number of arms per type\nconst T = 5; // Time horizon\n\nconst totalReward = spiIndexPolicy(N, S, K, p, T, exampleMDPParams);\n\n\nconsole.log(\"SPI Policy Simulation Complete. Total Reward:\", totalReward);\n\n\n\n\n```\n\n**Explanation of the SPI Index Policy Algorithm:**\n\nThe SPI (Single-Pull Index) policy addresses the challenge of resource allocation in Restless Multi-Armed Bandit (RMAB) problems where each arm can be activated (pulled) only once.  Traditional index policies struggle with this constraint because they may activate a suboptimal arm early, missing the opportunity to activate a better arm later.  The SPI policy tackles this issue by introducing \"dummy states\" and formulating the problem as a linear program (LP).\n\n\n**Algorithm 1: `spiIndexPolicy`**\n\n**Purpose:** To efficiently allocate resources (activations) to a set of arms over a finite time horizon, maximizing the cumulative reward under the single-pull constraint.\n\n**Steps:**\n\n1. **LP Formulation and Solution:** The algorithm first constructs and solves an LP based on equation (16) in the paper. This LP incorporates the dummy states, allowing the algorithm to relax the single-pull constraint directly within the LP formulation. The solution of the LP provides an occupancy measure (`mu`).  This occupancy measure indicates the expected number of times each arm is in each state and taking each action at every time step in the optimal solution of the *relaxed* problem.\n\n2. **Calculate x(s, a, t):** Based on the occupancy measure, the algorithm computes `x(s, a, t)` (equation 17), which represents the probability of selecting action 1 (activate) for a given arm in a given state at a given time.  This probability comes from the relaxed solution.\n\n3. **Initialize Arm States:** The states of arms at the beginning are initialized according to `initialStateDistribution`.\n\n4. **Time Loop (t=0 to T-1):** The algorithm simulates the policy over the time horizon.\n\n5. **Construct and Sort SPI Set:** Inside the time loop, for each time step, the algorithm computes the SPI values for each arm (equation 18) and stores them in the `I` array. The SPI value represents the expected immediate reward of activating an arm in a given state. The `I` array is sorted in descending order based on SPI values.\n\n6. **Activate Arms based on SPI and Budget:** The algorithm iterates through the sorted SPI values and activates arms with the highest SPI values as long as the activation budget (`K`) allows.\n\n7. **Handle Dummy States:** If an arm is pulled, it transitions to a corresponding dummy state. Dummy states have the same transition probabilities and reward functions regardless of the action applied.  The code provided does not implement checking for dummy states since that would depend on how dummy states are represented in a complete implementation of the state space and transition probabilities. The core idea is that dummy states have no impact once transitioned into.\n\n8. **Update States and Accumulate Rewards:** After the activations at each time step, update the states of the arms based on their transition probabilities. Accumulate the rewards obtained in each round.\n\n**Key Improvements and Differences:**\n\n* **Dummy States:** The use of dummy states significantly simplifies the problem by allowing the algorithm to remove the explicit single-pull constraint from the LP.  This makes the LP easier to solve and makes the policy suitable for practical applications where the single-pull constraint is critical.\n* **Focus on Finite Horizon:** This implementation explicitly focuses on the finite horizon case, which is often more relevant in real-world scenarios.\n\nThe provided JavaScript code is a high-level implementation, illustrating the core logic of the SPI policy.  Certain parts, such as the LP solver and the `calculateX` function, are left as placeholders since they would depend on chosen linear programming and numerical methods libraries.  A complete implementation would require filling in those parts and including more detailed handling of dummy states within the state update process.",
  "simpleQuestion": "How to efficiently allocate scarce resources in a multi-agent system?",
  "timestamp": "2025-01-13T06:02:15.180Z"
}