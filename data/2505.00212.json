{
  "arxivId": "2505.00212",
  "title": "Which Agent Causes Task Failures and When?\nOn Automated Failure Attribution of LLM Multi-Agent Systems",
  "abstract": "Failure attribution in LLM multi-agent systems—identifying the agent and step responsible for task failures—provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI ol and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available in the public repository.",
  "summary": "This paper introduces the problem of automated failure attribution in Large Language Model (LLM) multi-agent systems: automatically identifying *which* agent is responsible for a task failure and *when* the decisive error occurred.  A new dataset, \"Who&When,\" containing annotated failure logs from 127 LLM multi-agent systems, is created to facilitate this research. Three LLM-based failure attribution methods (all-at-once, step-by-step, and binary search) are evaluated on Who&When, revealing that providing the LLM with the complete conversation improves agent identification, while incremental processing is better for pinpointing error steps.  While even advanced reasoning LLMs struggle with the task, combining methods and statistical analysis shows promise. This research highlights the complexity of failure analysis in LLM multi-agent systems and the need for further research in automated failure attribution.",
  "takeaways": "This paper introduces the concept of automated failure attribution in LLM-based multi-agent systems and its importance for debugging and refinement. Here's how a JavaScript developer can apply these insights to web development:\n\n**1. Understanding Decisive Errors:**\n\nThe core concept is identifying \"decisive errors\" – the specific agent action at a particular step that directly leads to task failure.  In a web app context, imagine a multi-agent system for e-commerce:\n\n* **Agent 1 (Product Search):** Queries a product database.\n* **Agent 2 (Recommendation):** Suggests related products.\n* **Agent 3 (Cart Management):** Adds products to the cart.\n\nA decisive error could be Agent 1 failing to retrieve the correct product data, cascading into incorrect recommendations and cart issues.  JavaScript developers need to structure their agent interactions so that these errors can be isolated.\n\n**2. Implementing Failure Attribution Methods in JavaScript:**\n\nThe paper proposes three methods, adaptable to JavaScript:\n\n* **All-at-Once:** Analyze the entire agent interaction log at once. Use LangChain.js with an LLM to process the complete log and identify the faulty agent and step.\n\n```javascript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst llm = new OpenAI();\nconst prompt = new PromptTemplate({\ntemplate: \"The problem is: {problem}. \\nThe conversation history is: {conversation}\\n Identify the agent and step where the first mistake occurred and explain why.\",\ninputVariables: [\"problem\", \"conversation\"]\n})\nconst res = await llm.call(await prompt.format({problem: userQuery, conversation: agentLogs}))\nconsole.log(res);\n```\n\n* **Step-by-Step:** Analyze each agent action sequentially.  This is ideal for long interaction logs.  After each agent performs an action, send the log up to that point to the LLM for analysis. If an error is detected, halt the system.\n\n```javascript\nasync function analyzeStep(problem, currentLog, llm) {\n  // ... Construct prompt similar to All-at-Once, but with only currentLog\n  const res = await llm.call( /* formatted prompt */ );\n  // ... Parse response to check for error indication\n  if ( /* error detected */ ) {\n    return { error: true, agent: /* ... */, step: /* ... */ };\n  }\n  return { error: false };\n}\n```\n\n* **Binary Search:**  Iteratively narrow down the error location by analyzing halves of the log. This can be more efficient than step-by-step.  Use a recursive function to implement the binary search logic.\n\n```javascript\nasync function binarySearchError(problem, log, llm, start, end) {\n // ... base case for single step\n // ... recursive calls with upper/lower halves of the log.\n}\n```\n\n**3. Practical Scenarios and Frameworks:**\n\n* **Chatbots:**  Debug multi-agent chatbot interactions where agents handle different parts of the conversation (e.g., greeting, information retrieval, booking).  Use a framework like Botpress.\n* **Collaborative Editing:**  Analyze agent interactions in a collaborative document editor where agents might handle formatting, grammar checks, or content suggestions. Integrate with frameworks like Tiptap or ProseMirror.\n* **Game Development:** Use multi-agent AI for game characters and analyze their interactions during gameplay to identify bugs or improve AI strategies. Implement with Phaser or Babylon.js.\n\n**4. Logging and Data Structures:**\n\nUse structured logging to capture agent actions, timestamps, and relevant context.  JSON is a good format for storing and transmitting logs to the LLM.\n\n```javascript\nconst logEntry = {\n  agent: \"ProductSearch\",\n  timestamp: Date.now(),\n  action: \"queryDatabase\",\n  query: \"laptop\",\n  result: /* ... */\n};\n```\n\n\n**5. Visualizations:**\n\nVisualize agent interactions and identified errors.  Use JavaScript charting libraries like Chart.js or D3.js to show the flow of the multi-agent system and highlight decisive errors.\n\n\n**6. Context Length Limitations:**\n\nBe mindful of context length limitations of LLMs.  The paper highlights the \"space-in-the-needle\" problem.  For very long interactions, consider strategies like summarization or breaking down the analysis into smaller chunks. LangChain.js provides several utility functions that will assist with this.\n\n\nBy implementing these strategies, JavaScript developers can effectively apply automated failure attribution to improve the reliability and robustness of LLM-based multi-agent web applications. This research inspires developers to think critically about how to structure, monitor, and debug complex agent interactions in the rapidly evolving world of web development.",
  "pseudocode": "The paper contains pseudocode blocks for two algorithms: Step-by-Step and Binary Search.  Here are their JavaScript equivalents:\n\n**Algorithm 1: Step-by-Step**\n\n```javascript\nfunction stepByStep(query, failureLog) {\n  for (let i = 0; i < failureLog.length; i++) {\n    // 1. Provide query and log segment to LLM\n    const llmInput = { query, logSegment: failureLog.slice(0, i + 1) };\n    const llmOutput = await getLlmResponse(llmInput); // Simulate LLM call\n\n    // 2. Check if LLM indicates error at step i\n    if (llmOutput.errorAtStep === i) {\n      // 3. Identify responsible agent\n      const responsibleAgent = identifyAgent(failureLog[i]); // Extract agent from log entry\n      return { responsibleAgent, errorStep: i };\n    }\n  }\n\n  // No error found\n  return null;\n}\n\n// Simulate getting a response from an LLM.  Replace with actual LLM interaction.\nasync function getLlmResponse(input) {\n  // In a real application, send 'input' to the LLM API and process the response.\n  // This is a placeholder.  The LLM should return an object like { errorAtStep: <step number> }\n  // if an error is found, or null otherwise.\n  return { errorAtStep: null }; // Replace with actual LLM logic.\n}\n\n\n// Placeholder function to extract the agent name from a log entry.\nfunction identifyAgent(logEntry) {\n  // Logic to parse the log entry and extract the agent's name.\n  // This will depend on the structure of your log entries.\n  return \"agent_name\"; // Replace with actual parsing logic\n}\n\n\n\n\n```\n\n**Explanation:**\n\nThis algorithm iteratively feeds segments of the failure log to an LLM. At each step, the LLM checks for an error. If an error is detected, the algorithm returns the responsible agent and the step number. This approach allows for finer-grained analysis as it examines each step individually. It's analogous to a debugger stepping through code.\n\n\n**Algorithm 2: Binary Search**\n\n```javascript\nasync function binarySearch(query, failureLog) {\n  let low = 0;\n  let high = failureLog.length - 1;\n\n  while (low < high) {\n    const mid = Math.floor((low + high) / 2);\n    // 1. Extract log segment\n    const logSegment = failureLog.slice(low, mid + 1);\n\n    // 2. Provide query and log segment to LLM\n    const llmInput = { query, logSegment};\n    const llmOutput = await getLlmResponse(llmInput); // Simulate LLM call\n\n    // 3. Check if LLM indicates error in the segment\n    if (llmOutput.errorInSegment) {\n      high = mid;\n    } else {\n      low = mid + 1;\n    }\n  }\n  // Error found at 'low'\n  const responsibleAgent = identifyAgent(failureLog[low]); // Extract agent from log entry\n\n  return { responsibleAgent, errorStep: low };\n}\n\n//  getLlmResponse and identifyAgent are the same as in the previous example\n```\n\n**Explanation:**\n\nThis algorithm employs a binary search strategy to locate the error more efficiently than a linear search. It repeatedly splits the failure log in half and asks the LLM whether the error is in the upper or lower half. This process continues until the search space is narrowed down to a single step, thus identifying the likely location of the decisive error.\n\n\n**Key Improvements and Considerations for Real-World Use:**\n\n* **Actual LLM Interaction:** The provided JavaScript code uses placeholder functions (`getLlmResponse` and `identifyAgent`) to simulate LLM interaction.  You'll need to replace these with actual calls to your chosen LLM API and implement logic to parse the LLM responses appropriately.  This will be the most significant part of integrating these algorithms.\n* **Prompt Engineering:** Pay careful attention to prompt engineering.  The clarity and specificity of your prompts will heavily influence the LLM's ability to identify errors effectively.  Experiment with different prompt formulations to optimize performance.\n* **Error Handling:** Implement robust error handling to manage cases where the LLM fails to respond, provides ambiguous outputs, or raises exceptions.\n* **Asynchronous Operations:** LLM interactions are typically asynchronous. The provided code utilizes `async/await` to handle this. Make sure to incorporate proper asynchronous handling within your application's architecture.\n* **Cost Optimization:**  LLM usage can be costly. Consider caching LLM responses and optimizing your prompts to minimize the number of tokens used.  The binary search method is designed to be more efficient than the step-by-step method in terms of LLM calls.\n* **Evaluation Metrics:**  Use the agent-level accuracy, step-level accuracy, and step-level accuracy with tolerance metrics described in the paper to rigorously evaluate the performance of your implementation.  This is crucial to understanding the strengths and weaknesses of different approaches.\n* **Hybrid Approach:**  Consider implementing the hybrid approach suggested in the paper, combining all-at-once and step-by-step methods.\n\n\nBy implementing these JavaScript versions and incorporating these considerations, you can begin experimenting with automated failure attribution in your own LLM-based multi-agent systems.  This will provide valuable insights for debugging and improving the reliability of your agent-based applications.",
  "simpleQuestion": "How to automate LLM multi-agent failure attribution?",
  "timestamp": "2025-05-02T05:06:05.820Z"
}