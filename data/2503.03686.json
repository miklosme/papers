{
  "arxivId": "2503.03686",
  "title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems",
  "abstract": "LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability and high inference costs. In this paper, we simplify the process of building an MAS by reframing it as a generative language task, where the input is a user query and the output is a corresponding MAS. To address this novel task, we unify the representation of MAS as executable code and propose a consistency-oriented data construction pipeline to create a high-quality dataset comprising coherent and consistent query-MAS pairs. Using this dataset, we train MAS-GPT, an open-source medium-sized LLM that is capable of generating query-adaptive MAS within a single LLM inference. The generated MAS can be seamlessly applied to process user queries and deliver high-quality responses. Extensive experiments on 9 benchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms 10+ baseline MAS methods on diverse settings, indicating MAS-GPT's high effectiveness, efficiency and strong generalization ability. Code will be available at https://github.com/rui-ye/MAS-GPT.",
  "summary": "This paper introduces MAS-GPT, a large language model (LLM) trained to generate entire multi-agent systems (MAS) tailored to specific user queries, represented as executable Python code.  This simplifies MAS creation, improves adaptability to diverse tasks, and reduces the high inference costs associated with existing methods that require manual design or multiple LLM calls. Key points include representing MAS as executable code, a consistency-oriented data construction pipeline focusing on both inter- and intra-consistency of query-MAS pairs, and supervised fine-tuning of a medium-sized LLM to generate these query-specific MAS.  The approach shows promising results on various benchmarks, demonstrating effectiveness and generalizability across different LLMs used to execute the generated agents.  It also highlights potential for augmenting the reasoning performance of advanced LLMs.",
  "takeaways": "This paper introduces MAS-GPT, a novel approach to building adaptable multi-agent systems for web applications powered by LLMs. Let's explore how JavaScript developers can leverage these concepts:\n\n**1. Generating Dynamic Agent Teams with LangChain.js:**\n\nImagine building a collaborative writing tool. Instead of a fixed agent structure, you can use LangChain.js and MAS-GPT's principles to dynamically create agent teams based on user needs.  A prompt to MAS-GPT might look like this:\n\n```\nUser Query: \"Write a short story about a robot learning to love.\"\n\nMAS-GPT Prompt:  \"Given the query 'Write a short story about a robot learning to love,' generate a JavaScript LangChain chain representing a multi-agent system.  Include agent roles, prompts, and the chain structure.\"\n```\n\nMAS-GPT might then return a LangChain chain definition in JavaScript:\n\n```javascript\nconst { LLMChain, SequentialChain } = require(\"langchain\");\nconst { OpenAI } = require(\"langchain/llms/openai\");\n\nconst llm = new OpenAI({ temperature: 0.7 });\n\nconst ideaAgent = new LLMChain({\n  llm,\n  prompt: \"Generate three creative ideas for a short story about a robot learning to love.\"\n});\n\nconst storyAgent = new LLMChain({\n  llm,\n  prompt: (prev) => `Write a short story based on this idea: ${prev.text}`\n});\n\nconst refinementAgent = new LLMChain({\n  llm,\n  prompt: (prev) => `Refine the following story, focusing on emotional depth: ${prev.text}`\n});\n\nconst chain = new SequentialChain({\n  chains: [ideaAgent, storyAgent, refinementAgent],\n  inputVariables: [], // No initial input needed\n  outputVariables: [\"text\"],\n});\n\nchain.call({}).then((res) => console.log(res.text));\n```\n\nThis dynamic chain generation allows your application to tailor agent teams to specific writing tasks, leading to more relevant and creative outputs.\n\n**2. Building Adaptive Chatbots with Socket.IO and Node.js:**\n\nFor customer support chatbots, MAS-GPT can generate specialized agent configurations depending on the customer's query.  Use Node.js and Socket.IO to manage real-time communication between agents and the user.\n\n```\nUser Query: \"I'm having trouble connecting my printer to Wi-Fi.\"\n\nMAS-GPT Prompt: \"Given the user query 'I'm having trouble connecting my printer to Wi-Fi,' generate a multi-agent system configuration in JSON format for a customer support chatbot. Define agent roles, prompts, and the communication flow.\"\n```\n\nMAS-GPT might return a JSON configuration:\n\n```json\n{\n  \"agents\": [\n    { \"role\": \"Diagnostics\", \"prompt\": \"Gather information about the user's printer model and operating system.\" },\n    { \"role\": \"Troubleshooting\", \"prompt\": \"Provide step-by-step instructions based on the diagnostics.\" },\n    { \"role\": \"Escalation\", \"prompt\": \"If troubleshooting fails, escalate to a human agent.\" }\n  ],\n  \"flow\": [\"Diagnostics\", \"Troubleshooting\", \"Escalation\"]\n}\n```\n\nYour Node.js server can then instantiate these agents using an LLM library and orchestrate their interactions through Socket.IO, providing a more adaptable and helpful customer experience.\n\n\n**3. Experimenting with Consistency-Oriented Data Pipelines:**\n\nThe paper emphasizes the importance of data consistency. To experiment with this, create a small JavaScript dataset of query-MAS pairs for a specific domain (e.g., travel planning). Use a simple scoring function to evaluate the generated plans. Then, implement the inter- and intra-consistency refinement strategies in JavaScript to see how they affect the dataset's quality and the subsequent performance of a smaller LLM trained on this refined dataset.  Libraries like TensorFlow.js can help with basic LLM training or fine-tuning.\n\n**Key Takeaways for JavaScript Developers:**\n\n* **Dynamic Agent Generation:**  Move beyond static agent configurations and embrace the power of dynamic agent team creation using LangChain.js and similar tools.\n* **Adaptive Systems:**  Build applications that tailor their behavior based on user needs, leading to a more personalized and efficient user experience.\n* **Data Consistency:**  Focus on data quality and consistency when training your own MAS generation models, following the principles outlined in the paper.\n* **JavaScript Frameworks:** LangChain.js for chain building, Node.js and Socket.IO for real-time communication, and TensorFlow.js for experimentation with LLM training are valuable tools for implementing these concepts.\n\nBy incorporating these insights, JavaScript developers can be at the forefront of developing the next generation of intelligent, adaptable, and truly collaborative web applications powered by multi-agent LLM systems.",
  "pseudocode": "```javascript\n// Listing 1: Case 1: Multi-agent system generated by MAS-GPT. \n// MAS-GPT can generate query-specific MAS. \n// MAS-GPT designs five independent responding agents, each responsible for different aspects of the task.\n\nimport { LLM, generate_and_extract_code, execute_code } from './utils'; // Assuming utils.js contains these functions\n\nclass MAS {\n  constructor(modelList) {\n    this.llm = new LLM(modelList);\n  }\n\n  forward(taskInfo) {\n    // Step-by-step instructions for each agent to reason and generate an answer\n    const instructions = [\n      `Task: ${taskInfo}\\n\\nFocus on analyzing the relationship between events A and B in terms of independence. Solve the task.`,\n      `Task: ${taskInfo}\\n\\nFocus on analyzing the relationship between events A and B in terms of mutual exclusivity. Solve the task.`,\n      `Task: ${taskInfo}\\n\\nFocus on the implications of P(A∪B) and P(A∩B) on the relationship between events A and B. Solve the task.`,\n      `Task: ${taskInfo}\\n\\nFocus on the definitions and properties of probability union and intersection. Solve the task.`,\n      `Task: ${taskInfo}\\n\\nFocus on identifying any logical inconsistencies or impossibilities in the given choices. Solve the task.`\n    ];\n\n\n    // Call the LLM to generate each solution (using map for cleaner syntax)\n    const cotResults = instructions.map(instruction => this.llm.call_llm(instruction));\n\n\n    // Get the instruction for the final decision-making agent based on all generated solutions\n    const finalDecisionInstruction = this.getFinalDecisionInstruction(taskInfo, cotResults);\n\n    // Call the LLM to process the final decision-making instruction and generate the final answer\n    const finalDecisionResult = this.llm.call_llm(finalDecisionInstruction);\n\n    return finalDecisionResult;\n  }\n\n  getFinalDecisionInstruction(taskInfo, cotResults) {\n    let instruction = `Task:\\n${taskInfo}\\n\\n`;\n\n    // Append each solution from cotResults to the instruction\n    cotResults.forEach((result, i) => {\n      instruction += `Solution ${i + 1}:\\n${result}\\n\\n`;\n    });\n\n    // Add the final prompt\n    instruction += \"Given all the above solutions, reason over them carefully and provide a final answer to the task.\";\n\n    return instruction;\n  }\n\n\n\n\n  // Helper function to generate and execute code  (from Listing 2)\n  generate_code_get_output(taskInfo) {\n     const codeGenerationInstruction = `...`; // Same as in original Python code\n\n     const [answer, code] = generate_and_extract_code(this.llm, codeGenerationInstruction);\n     const output = execute_code(code);\n     return [answer, output];\n  }\n\n  organize(taskInfo, answer, result) {\n     // ... (same logic as Python code)\n  }\n\n\n  get_final_solution(taskInfo, solutions) {\n     // ... (same logic as Python code)\n  }\n}\n\nexport default MAS;\n\n\n```\n\n**Explanation of Listing 1:**\n\nThis JavaScript code implements a multi-agent system for solving probability-related multiple-choice questions.  The system works as follows:\n\n1. **Initialization:**  The `MAS` class takes a list of LLMs (likely different models or configurations) during construction.\n\n2. **`forward(taskInfo)`:** This is the main entry point. It receives the problem description (`taskInfo`).\n\n3. **Agent Instructions:**  It creates a list of instructions, each tailored for a specific agent, focusing on different aspects of probability (independence, mutual exclusivity, implications of probabilities, definitions of union and intersection, and logical inconsistencies).\n\n4. **Agent Execution:** The `call_llm` method (assumed to be in `utils.js`) is called for each instruction, simulating the execution of each agent. The results are stored in `cotResults`.\n\n5. **Final Decision Agent:**  The `getFinalDecisionInstruction` method constructs a prompt for a final decision-making agent. This prompt includes the original question and the responses from each of the previous agents.\n\n6. **Final Answer:** The `call_llm` method is invoked again with the final decision prompt, producing the final answer to the problem.\n\n**Purpose:** The purpose of this multi-agent system is to decompose a complex probability question into smaller, more manageable subproblems. Each agent specializes in one aspect, and their combined knowledge is used by the final agent to produce a more robust and accurate answer.\n\n\n\n```javascript\n// Listing 2 and 3:  Similar structure, adapted to JavaScript\n\n\n// Listing 3 is almost identical in structure; it just adds a self-reflection stage.\n// I've omitted it here to avoid redundancy, but the core idea remains the same.\n```\n\n\n**Explanation of Listing 2 and 3:**\n\nListings 2 and 3 follow a similar multi-agent approach.  Listing 2 handles math problems either by generating and executing code or by direct reasoning. Listing 3 incorporates a \"self-reflection\" stage where each agent revises its initial solution. The main steps are comparable to Listing 1: individual agents work on the task, and a final agent combines their results.\n\n\n**No other pseudocode blocks requiring JavaScript conversion were found in the provided paper excerpt.**",
  "simpleQuestion": "How can LLMs build efficient multi-agent systems?",
  "timestamp": "2025-03-06T06:03:20.599Z"
}