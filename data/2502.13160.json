{
  "arxivId": "2502.13160",
  "title": "Understanding Dynamic Diffusion Process of LLM-based Agents under Information Asymmetry",
  "abstract": "Large language models have been used to simulate human society using multi-agent systems. Most current social simulation research emphasizes interactive behaviors in fixed environments, ignoring information opacity, relationship variability and diffusion diversity. In this paper, we study the dynamics of information diffusion in 12 asymmetric open environments defined by information content and distribution mechanisms. We first present a general framework to capture the features of information diffusion. Then, we designed a dynamic attention mechanism to help agents allocate attention to different information, addressing the limitations of LLM-based attention. Agents start by responding to external information stimuli within a five-agent group, increasing group size and forming information circles while developing relationships and sharing information. Additionally, we observe the emergence of information cocoons, the evolution of information gaps, and the accumulation of social capital, which are closely linked to psychological, sociological, and communication theories. \"The truth is rarely pure and never simple.\" The Importance of Being Earnest",
  "summary": "This paper explores how LLM-powered agents spread information in scenarios where some agents have more information than others. It focuses on how this information asymmetry affects group dynamics, communication patterns, and individual actions.\n\nKey points for LLM-based multi-agent systems:\n\n* **Dynamic Attention Mechanism:** The paper introduces a custom attention mechanism to help agents prioritize important information from multiple sources, mimicking how humans filter information. This addresses limitations of standard LLM attention in complex social simulations.\n* **Open Environment:**  The simulation allows agents to interact with a growing number of other agents, creating a more realistic and dynamic social environment than fixed group sizes.\n* **Information Diffusion Patterns:** The study analyzes how different types of information (gossip, policy, legal cases, etc.) and distribution methods affect how quickly and widely information spreads, highlighting the emergence of information gaps and echo chambers.\n* **Social Behaviors:** The agents demonstrate social motivations in their interactions, including cooperation, support, and discussion, offering insights into how social factors influence information spread.\n* **Social Capital and Information Cocoons:** The simulation observes how some agents accumulate social capital by connecting with others, while others become trapped in information cocoons, reinforcing existing beliefs. These observations connect the simulation to existing social theories.\n* **Agent Action Prompting:** The provided prompts clearly define how agents should make decisions based on their attention, relationships, and the information they receive, illustrating a practical approach to controlling agent behavior.",
  "takeaways": "This research paper offers valuable insights for JavaScript developers building LLM-based multi-agent applications, particularly in understanding and mitigating the challenges of information asymmetry. Here's how a JavaScript developer can apply these insights:\n\n**1. Implementing the Dynamic Attention Mechanism:**\n\nThe paper's core contribution is the Dynamic Attention Algorithm. A JavaScript developer can implement this using libraries like TensorFlow.js or Brain.js. Here's a simplified example:\n\n```javascript\n// Simplified Dynamic Attention Mechanism\nfunction dynamicAttention(messages, relationships, actions, currentRound) {\n  let weightedMessages = {};\n\n  messages.forEach(message => {\n    let weight = 0;\n    // Relationship weight\n    weight += relationships[message.sender] || 0; // Positive/negative relationship adds/subtracts weight\n\n    // Complexity weight (simplified)\n    if (message.content.length > 100) { //  Longer messages are considered more complex\n      weight += 1;\n    }\n\n\n    // Entropy and past interaction calculations (more complex logic required here)\n    // ...\n\n\n    weightedMessages[message.id] = weight;\n  });\n\n  return weightedMessages;\n}\n\n// Example Usage\nconst messages = [\n    {id:1, sender:'agent1', content:'Short message'},\n    {id:2, sender:'agent2', content:'This is a much longer and therefore more complex message'},\n];\n\nconst relationships = { agent1: 1, agent2: -1 }; // agent1 has a positive relationship\nconst actions = []; // Previous action data\nconst currentRound = 1;\nconst weightedMessages = dynamicAttention(messages,relationships,actions,currentRound);\n\nconsole.log(weightedMessages); // Output weighted messages\n```\n\nThis weighted information can then be fed to the LLM when prompting for agent decisions.  This prioritizes important information and prevents the LLM from getting \"lost in the middle\" of a long conversation history.  Libraries like Langchain can help manage this interaction.\n\n**2. Simulating Information Diffusion in a Web App:**\n\nImagine a collaborative web application where multiple LLM-based agents work together on a task, such as brainstorming ideas or writing a story.  Using a framework like React or Vue.js, you can visualize the information flow between agents.\n\n* **Visualizing Information Asymmetry:**  Display the information each agent possesses using different colors or visual cues. This would clearly illustrate how information asymmetry impacts the group's progress.\n* **Simulating different distribution mechanisms:**  Create controls that let you simulate different information distribution scenarios (broadcast, unicast, etc.) and observe how it affects the agents' behavior and overall outcome.  For example, a chat interface could be modified to restrict communication channels based on chosen settings.\n* **Implementing Social Capital:** Develop a mechanism to track the number and quality of interactions between agents, visually representing the \"social capital\" accumulated over time.  This could be a network graph or a simple score displayed next to each agent.\n\n**3. Mitigating Information Cocoons:**\n\nThe research highlights the emergence of information cocoons. In a web app, this could manifest as agents reinforcing each other's biases due to limited information diversity. To mitigate this:\n\n* **Introduce \"noise\":**  Periodically introduce new information or agents with diverse perspectives into the system.  This can be automated using a timer and random information generation.\n* **Promote exploration:** Design the agent decision-making process to encourage exploring alternative sources of information, even if they initially seem less relevant.  This can be incorporated into the prompt engineering.\n* **Cross-validation:**  Implement a mechanism for agents to cross-validate their information with other agents outside their immediate circle, simulating information seeking from diverse sources.\n\n**4. Practical Web Development Scenarios:**\n\n* **Multi-agent content creation:** Imagine a website where multiple LLM-based agents collaborate on writing articles, each agent specializing in a specific aspect (research, writing style, fact-checking). The dynamic attention mechanism would help agents prioritize information relevant to their role.\n* **Simulated social networks:**  Build a simulated social network with LLM-based agents to study how information spreads and how different network topologies influence opinion formation.\n* **Collaborative design tools:** Develop a design tool where multiple LLM-based agents work together on a project, each agent focusing on different design elements.  The insights on information asymmetry and social capital could help optimize the collaboration process.\n\n\nBy applying the research insights and leveraging JavaScript frameworks and libraries, developers can build more robust and realistic LLM-based multi-agent applications, paving the way for innovative web experiences and deeper understanding of social dynamics.  These examples provide a solid starting point for developers interested in bringing the theoretical concepts of multi-agent AI into the practical realm of web development.",
  "pseudocode": "```javascript\n// Algorithm 1: Dynamic Attention Algorithm\n\nfunction dynamicAttention(receivedMessages, turnNumber, actions, subjectiveRelationships) {\n  const currentMsgs = {};\n  const prevMsgs = {};\n  for (const [t, s, m] of receivedMessages) {\n    if (t === turnNumber) {\n      currentMsgs[s] = m;\n    } else {\n      prevMsgs[s] = prevMsgs[s] || [];\n      prevMsgs[s].push(m);\n    }\n  }\n\n  const weightDict = {};\n  for (const s in currentMsgs) {\n    const r = subjectiveRelationships.get(s) || 'gen'; // Default to 'gen' if not found\n    const w = (r === 'pos' || r === 'neg') ? 1 : -1;\n    weightDict[s] = [w, currentMsgs[s]];\n  }\n\n\n  const maxAgent = getMaxAgent(calcEntropy(Object.values(currentMsgs)));\n  if (maxAgent !== 0) {\n    for (const s in weightDict) {\n      weightDict[s][0] += (s === maxAgent) ? 1 : -1;\n    }\n  }\n\n  for (const s in weightDict) {\n    if (s in prevMsgs) {\n      const prevEntropy = calcEntropy(prevMsgs[s]);\n      const currEntropy = calcEntropy([...prevMsgs[s], weightDict[s][1]]);\n      weightDict[s][0] += (currEntropy > prevEntropy) ? 1 : -1;\n    }\n  }\n\n  if (actions.length > 0) {\n    const topAgent = mostFrequent(actions); // Assumes actions is an array of agent IDs\n    for (const s in weightDict) {\n      weightDict[s][0] += (s === topAgent) ? 1 : -1;\n    }\n  }\n    \n  const attentionInformation = {};\n  for (const s in weightDict) {\n      attentionInformation[s] = { message: weightDict[s][1], weight: weightDict[s][0] };\n  }\n  return attentionInformation;\n}\n\n\nfunction calcEntropy(messages) {\n  // Implement entropy calculation based on message content (e.g. using character frequencies).\n  // Example: Calculate the frequency of characters, words, or phrases in the message.\n  // Here's a simplified (and not very meaningful) example:\n\n    if (!messages || messages.length === 0) return 0;\n  let entropy = 0;\n    const charCounts = {};\n    for (const message of messages) {\n        for (const char of message) {\n            charCounts[char] = (charCounts[char] || 0) + 1;\n        }\n    }\n    const totalChars = Object.values(charCounts).reduce((a,b) => a+b,0);\n    for(const char in charCounts){\n        const p = charCounts[char]/totalChars;\n        entropy -= p * Math.log2(p);\n    }\n\n  return entropy;\n}\n\nfunction getMaxAgent(entropies) {\n  // Returns the agent ID with the maximum entropy from a dict of entropies.\n    // Example, assumes entropies is an object like { agent1: 0.5, agent2: 0.8 }\n    let maxEntropy = -Infinity;\n    let maxAgent = 0;\n    for (const agent in entropies){\n        if (entropies[agent] > maxEntropy){\n            maxEntropy = entropies[agent];\n            maxAgent = agent;\n        }\n    }\n    return maxAgent;\n}\n\nfunction mostFrequent(arr) {\n  // Returns the most frequent element in an array.\n    const counts = {};\n    let maxCount = 0;\n    let mostFrequent = null;\n    for (const el of arr) {\n        counts[el] = (counts[el] || 0) + 1;\n        if(counts[el] > maxCount) {\n            maxCount = counts[el];\n            mostFrequent = el;\n        }\n    }\n    return mostFrequent;\n}\n\n\n// Example usage:\nconst receivedMessages = [\n  [1, 'agent1', 'message1'],\n  [1, 'agent2', 'message2'],\n  [2, 'agent1', 'message3'],\n  [2, 'agent2', 'message4'],\n];\nconst turnNumber = 2;\nconst actions = ['agent2', 'agent2', 'agent1'];\nconst subjectiveRelationships = new Map([\n  ['agent1', 'pos'],\n  ['agent2', 'neg'],\n]);\nconst attentionInfo = dynamicAttention(receivedMessages, turnNumber, actions, subjectiveRelationships);\nconsole.log(attentionInfo);\n\n\n```\n\n\n\n**Explanation of the Algorithm and its Purpose:**\n\nThe *Dynamic Attention Algorithm* aims to simulate how an LLM-based agent might prioritize information received from multiple sources in a multi-agent environment.  It addresses the limitation of LLMs struggling with long contexts by dynamically weighting incoming messages based on several factors:\n\n1. **Relationship with Sender:**  Messages from agents with positive or negative relationships are initially given higher weight.\n\n2. **Information Complexity (Not implemented):** The pseudocode mentions prioritizing \"high complexity\" messages but doesn't specify how complexity is measured. The JavaScript code does not include this functionality but a more sophisticated implementation might consider metrics like message length, vocabulary diversity, or semantic depth.\n\n3. **Information Change:** The algorithm calculates the change in entropy between the current message and previous messages from the same sender. A larger change in entropy suggests more novel information and results in a higher weight.  Entropy is a key concept in information theory and is used here to capture the amount of unexpected content.\n\n4. **Interaction Frequency:** Messages from agents with whom the agent has interacted more frequently are given higher weight.\n\n\nThe algorithm's output is a set of weighted messages, which can then be used by the LLM to make more informed decisions about its actions in the next turn.  This dynamic attention mechanism helps the agent focus on the most relevant information and adapt to the evolving dynamics of the multi-agent environment.  The JavaScript implementation provides a foundation for more advanced features like improved entropy measures.",
  "simpleQuestion": "How do LLMs diffuse info in asymmetric networks?",
  "timestamp": "2025-02-20T06:05:15.000Z"
}