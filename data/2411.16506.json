{
  "arxivId": "2411.16506",
  "title": "Online Guidance Graph Optimization for Lifelong Multi-Agent Path Finding",
  "abstract": "We study the problem of optimizing a guidance policy capable of dynamically guiding the agents for lifelong Multi-Agent Path Finding based on real-time traffic patterns. Multi-Agent Path Finding (MAPF) focuses on moving multiple agents from their starts to goals without collisions. Its lifelong variant, LMAPF, continuously assigns new goals to agents. In this work, we focus on improving the solution quality of PIBT, a state-of-the-art rule-based LMAPF algorithm, by optimizing a policy to generate adaptive guidance. We design two pipelines to incorporate guidance in PIBT in two different ways. We demonstrate the superiority of the optimized policy over both static guidance and human-designed policies. Additionally, we explore scenarios where task distribution changes over time, a challenging yet common situation in real-world applications that is rarely explored in the literature.",
  "summary": "This paper explores optimizing traffic flow in Lifelong Multi-Agent Path Finding (LMAPF), a problem where multiple agents navigate a map with dynamically assigned goals.  The core idea is using a learned *online guidance policy* to adjust the map's edge weights in real-time based on traffic patterns, thus improving throughput (goals reached per timestep). This contrasts with static *offline* approaches. The online policy, optimized via CMA-ES, is integrated with two LMAPF algorithms: PIBT (direct planning) and GPIBT (guide-path planning).  Experiments demonstrate that the online policy surpasses offline guidance and human-designed policies in throughput, especially in scenarios with shifting task distributions.  Moreover, the online policy can mitigate deadlocks in PIBT.\n\nFor LLM-based multi-agent systems, this research highlights the potential of learning adaptive guidance policies to coordinate agent behavior dynamically.  Using learned policies offers greater flexibility than predefined rules and adapts to changing circumstances.  This dynamic coordination is especially relevant for complex, evolving scenarios requiring real-time responsiveness, as demonstrated by the dynamic task distribution and deadlock mitigation results.",
  "takeaways": "This research paper presents exciting possibilities for JavaScript developers working with LLM-based multi-agent systems, especially in dynamic web environments. Here's how its insights translate into practical examples:\n\n**1. Dynamic Guidance for Chatbots in a Customer Service Application:**\n\nImagine a website with multiple LLM-powered chatbots handling customer inquiries.  Instead of statically assigning chatbots to customers, you can apply the concept of \"online guidance.\" Track real-time metrics like chatbot load, customer wait times, and topic complexity of incoming messages (analogous to \"traffic\" in the paper).  A guidance policy, potentially a small neural network implemented using TensorFlow.js, can then dynamically adjust the routing of incoming messages.  If a chatbot specializing in billing is overloaded, the guidance policy can direct billing-related inquiries to a less busy general chatbot or queue them, optimizing for overall system throughput and customer satisfaction.\n\n**Example (Conceptual using Node.js and TensorFlow.js):**\n\n```javascript\n// Simplified guidance policy (neural network)\nconst guidanceModel = tf.sequential();\n// ... define model layers\n\n// Real-time metrics\nconst chatbotLoads = { chatbot1: 0.8, chatbot2: 0.2, /* ... */ };\nconst waitTimes = { chatbot1: 120, chatbot2: 10, /* ... */ };\n\n// Incoming message\nconst message = { text: \"I have a billing question.\", topic: \"billing\" };\n\n// Input features for guidance policy\nconst inputTensor = tf.tensor([/* ... features from metrics and message ... */]);\n\n// Predict routing probabilities\nconst routingProbabilities = guidanceModel.predict(inputTensor);\n\n// Route message based on probabilities\n// ... logic to assign message to the optimal chatbot\n```\n\n\n**2. Collaborative Content Creation with LLMs:**\n\nConsider a multi-agent system where LLMs collaborate on writing different sections of a document.  Each LLM can be specialized in a particular writing style (e.g., technical, creative, legal).  The paper's \"guide-path planning\" approach can be used here. An initial plan is created, assigning sections to LLMs based on their strengths. However, as LLMs progress, some might be faster or encounter difficulties.  A guidance policy can dynamically adjust the plan.  If an LLM gets stuck, a section can be reassigned to a different LLM or divided into smaller parts for parallel processing. This dynamic allocation improves the efficiency of the collaborative writing process.\n\n**Example (Conceptual using a message queue like Redis):**\n\n```javascript\n// Initial plan\nconst plan = { section1: \"LLM1\", section2: \"LLM2\", /* ... */ };\n\n// LLM reports progress\nredisClient.publish(\"progress\", { llm: \"LLM1\", section: \"section1\", status: \"blocked\" });\n\n// Guidance policy reacts to progress updates\nredisClient.on(\"message\", (channel, message) => {\n  if (channel === \"progress\") {\n    const { llm, section, status } = JSON.parse(message);\n    if (status === \"blocked\") {\n      // Guidance policy logic to reassign section or adjust plan\n      // ... potentially using a decision tree or rule-based system\n    }\n  }\n});\n```\n\n**3. Multi-Agent Game Development:**\n\nIn a web-based multi-agent game, LLMs can control non-player characters (NPCs).  The \"dynamic task distribution\" discussed in the paper is relevant here.  The demand for different types of NPCs (e.g., merchants, guards, quests givers) might change based on player actions.  A guidance policy can dynamically adjust the spawning and behavior of NPCs.  If players are congregating in a particular area, more merchant NPCs can be spawned there.  This dynamic adaptation enhances player immersion and gameplay.\n\n**4. JavaScript Libraries and Frameworks:**\n\n* **TensorFlow.js:**  For implementing neural network-based guidance policies.\n* **LangChain and LlamaIndex:** These frameworks can facilitate building and deploying LLM-based agents and integrating with other systems.\n* **Redis or other message queues:** For communication and coordination between agents.\n* **Node.js with Socket.IO or WebSockets:** For real-time updates in web applications.\n\nBy applying the concepts from this paper, JavaScript developers can create more adaptive and robust LLM-based multi-agent applications that respond effectively to dynamic changes in web environments.  This opens up new possibilities for building sophisticated systems with enhanced user experience and improved efficiency.",
  "pseudocode": "```javascript\nfunction searchGuidePath(start, goal, onlinePolicy, edgeUsage) {\n  // Initialize open and closed lists\n  const OPEN = new PriorityQueue((a, b) => a.f - b.f); // Priority queue for OPEN list\n  const CLOSE = new Set();\n\n  // Create root node\n  const root = { loc: start, g: 0, f: heuristic(start), parent: null };\n  OPEN.enqueue(root);\n\n  while (!OPEN.isEmpty()) {\n    const curr = OPEN.dequeue();\n    CLOSE.add(curr.loc.toString()); // Use string representation for Set\n\n    if (curr.loc.equals(goal)) {\n      return reconstructPath(curr); // Function to reconstruct path from goal to start (not shown)\n    }\n\n    const obs = windowedObservation(curr.loc, 5, edgeUsage); // Window size is 5\n    const costs = onlinePolicy(obs).map(cost => cost + 1); // Add 1 to all costs\n\n    for (let i = 0; i < curr.loc.neighbors.length; i++) {\n      const neighbor = curr.loc.neighbors[i];\n      const costToNeighbor = costs[i];\n      const next = { loc: neighbor, g: curr.g + costToNeighbor, f: curr.g + costToNeighbor + heuristic(neighbor), parent: curr };\n\n      if (!CLOSE.has(next.loc.toString())) {\n          const existing = OPEN.find(node => node.loc.equals(next.loc))\n        if (!existing) {\n          OPEN.enqueue(next);\n        } else if (next.g < existing.g) {\n          OPEN.update(existing, next); // Replace existing with next if better path found\n\n        }\n\n      }\n\n\n    }\n  }\n\n  return null; // No path found\n}\n\n\nfunction heuristic(loc) {\n  // Admissible heuristic - can be Manhattan distance, Euclidean distance, etc.\n  // Must underestimate or be equal to the true cost to the goal.\n  // Example: Manhattan distance\n  return Math.abs(loc.x - goal.x) + Math.abs(loc.y - goal.y);\n}\n\nfunction reconstructPath(node) {\n\n    const path = [];\n    while (node) {\n      path.push(node.loc);\n      node = node.parent;\n    }\n    return path.reverse();\n}\n\n\n\n\nfunction windowedObservation(loc, winSize, edgeUsage) {\n    // Extract the windowed edge usage for the given location\n    // ... Implementation depends on your data structure for edgeUsage\n\n    // Example (assuming edgeUsage is a 3D array):\n    const obs = [];\n    const halfWin = Math.floor(winSize / 2);\n    for (let i = 0; i < 4; ++i){\n        obs.push([]);\n        for (let dx = -halfWin; dx <= halfWin; dx++) {\n            obs[i].push([]);\n            for (let dy = -halfWin; dy <= halfWin; dy++) {\n\n                const x = loc.x + dx;\n                const y = loc.y + dy;\n                if (x >= 0 && x < width && y >= 0 && y < height) {\n                    obs[i][dx + halfWin].push(edgeUsage[i][y][x]);\n\n                } else {\n\n                    obs[i][dx + halfWin].push(0); // Or some default value\n\n                }\n            }\n\n        }\n\n    }\n    return obs;\n}\n\n\n\n\n\n```\n\n**Explanation:**\n\nThe provided JavaScript code implements the `searchGuidePath` function, which corresponds to Algorithm 1 in the paper. This function searches for a guide path for an agent in the lifelong multi-agent path finding (LMAPF) problem.  Here's a breakdown:\n\n1. **A* Search:** The core of the algorithm is A* search, a well-known pathfinding algorithm. It uses a priority queue (`OPEN` list) to manage nodes to explore, prioritized by their `f` value (estimated cost to the goal).  The `CLOSE` list keeps track of visited nodes.\n\n2. **Guidance Policy Integration:**  The crucial difference from standard A* is the integration of the `onlinePolicy`. This policy takes a \"windowed observation\" (`obs`) of the current location and edge usage and returns the costs for moving to neighboring locations. This is where the learned guidance comes into play, dynamically adapting the search based on the current traffic situation.\n\n3. **Lazy Evaluation:** The code implements lazy evaluation of edge weights. It computes the weights only when a location is being expanded using online policy, avoiding unnecessary computations, especially since the guidance graph updates frequently.\n\n4. **Heuristic Function:** An admissible heuristic function (`heuristic(loc)`) is required for A* to find optimal paths. This function estimates the cost from a location to the goal and must never overestimate the actual cost.\n\n5. **Path Reconstruction:** Once the goal is reached, the `reconstructPath` function (not explicitly provided in the algorithm, but implemented in the code) traces back from the goal node to the start node using the parent pointers to build the final path.\n\n\n6. **Windowed Observation:** The `windowedObservation` function extracts a section of the `edgeUsage` data around the current location. This provides local traffic information to the `onlinePolicy`.\n\n7. **Priority Queue:** A priority queue is used to ensure that nodes with the lowest `f` values are explored first, crucial for A*'s efficiency.\n\nThis algorithm allows for dynamic adaptation of guide paths in LMAPF by incorporating the learned guidance policy, resulting in better performance in scenarios with changing traffic patterns, as demonstrated in the paper.",
  "simpleQuestion": "How can I optimize agent guidance for dynamic MAPF?",
  "timestamp": "2024-11-26T06:03:00.714Z"
}