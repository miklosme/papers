{
  "arxivId": "2502.02875",
  "title": "Heterogeneous Value Decomposition Policy Fusion for Multi-Agent Cooperation",
  "abstract": "Value decomposition (VD) has become one of the most prominent solutions in cooperative multi-agent reinforcement learning. Most existing methods generally explore how to factorize the joint value and minimize the discrepancies between agent observations and characteristics of environmental states. However, direct decomposition may result in limited representation or difficulty in optimization. Orthogonal to designing a new factorization scheme, in this paper, we propose Heterogeneous Policy Fusion (HPF) to integrate the strengths of various VD methods. We construct a composite policy set to select policies for interaction adaptively. Specifically, this adaptive mechanism allows agents' trajectories to benefit from diverse policy transitions while incorporating the advantages of each factorization method. Additionally, HPF introduces a constraint between these heterogeneous policies to rectify the misleading update caused by the unexpected exploratory or suboptimal non-cooperation. Experimental results on cooperative tasks show HPF's superior performance over multiple baselines, proving its effectiveness and ease of implementation.",
  "summary": "This paper introduces Heterogeneous Policy Fusion (HPF), a new way to train AI agents for cooperation in tasks requiring teamwork.  HPF combines different existing methods (specifically Value Decomposition methods) for training individual agents within a multi-agent system, choosing the best approach adaptively based on performance. This allows the system to learn faster and more effectively than using any single existing method alone. It prevents agents from getting stuck in suboptimal behavior by encouraging them to learn from methods that find optimal joint actions.\n\nKey points for LLM-based multi-agent systems: HPF could enhance multi-agent systems where agents need to work together effectively, especially in partially observable environments where agents only have limited information. This is relevant to LLM-based agents interacting in complex scenarios.  The adaptive policy selection within HPF could be useful for dynamically selecting appropriate LLM prompting strategies or specialized LLMs within a multi-agent setup depending on the current situation.  The focus on efficient training in HPF is also highly relevant to LLM-based systems, given the computational cost involved.",
  "takeaways": "This paper's core idea, Heterogeneous Policy Fusion (HPF), translates beautifully into practical JavaScript scenarios for LLM-based multi-agent web apps.  Imagine building a collaborative writing application, a real-time strategy game, or a decentralized marketplace using LLMs as agents.  HPF helps these agents learn to cooperate more effectively.  Here's how a JavaScript developer can apply these insights:\n\n**1. Representing Agents and Policies:**\n\n* **Agents:**  Represent each agent as a JavaScript object with properties like `id`, `llmInstance` (using a library like LangChainJS for interaction with the chosen LLM),  `localObservations`, `utilityFunction`, and `policy`.\n* **Policies:** Define policies as functions that take observations as input and return an action (e.g., generating text, making a move in the game, placing a bid).  You could have multiple policy functions:\n    * `simplePolicy`: A basic policy based on simple rules or heuristics, representing the VD methods with network constraints. This policy would be fast but potentially limited in its ability to find the truly optimal action.\n    * `advancedPolicy`: An LLM-powered policy where the LLM reasons about the observations and generates a sophisticated action, representing the VD methods with surrogate targets. This policy would be more powerful but computationally expensive.\n\n**2. Implementing HPF in JavaScript:**\n\n* **Policy Set (Î ):**  Create an array of policy functions, `policySet = [simplePolicy, advancedPolicy]`.\n* **Adaptive Policy Selection:** Based on current state and observations, use a mechanism similar to the paper's Boltzmann policy-based selection (Equation 5) to probabilistically choose a policy from `policySet`. You could estimate `Qk` (value of each policy) based on past performance, heuristics, or even a separate evaluation LLM:\n\n```javascript\nfunction choosePolicy(observations, Qvalues) {\n  let total = policySet.reduce((acc, _, index) => acc + Math.exp(Qvalues[index] / temperature), 0);\n  let probabilities = policySet.map((_, index) => Math.exp(Qvalues[index] / temperature) / total);\n\n  let randomValue = Math.random();\n  let cumulativeProbability = 0;\n  for (let i = 0; i < probabilities.length; i++) {\n    cumulativeProbability += probabilities[i];\n    if (randomValue < cumulativeProbability) {\n      return policySet[i];\n    }\n  }\n}\n```\n\n* **Interaction and Learning:**  Use the chosen policy to generate an action for each agent. Store the experience (state, observations, actions, reward) in a replay buffer.  Periodically train both `simplePolicy` and `advancedPolicy` using the data from the replay buffer.  This allows the simple policy to learn from the advanced policy's more sophisticated decisions.\n* **Instructive Constraint (L1):**  Implement a regularization term (using KL divergence as in the paper or a simpler metric) to encourage the simple policy's action distribution to be similar to the advanced policy's for the same observations. This can be added to the loss function during training. LangChainJS could help compute the necessary probabilities for this term.\n\n**3. Web Development Context:**\n\n* **Frontend (e.g., React, Vue):** Use a state management library (Redux, Vuex) to store agent states, observations, and UI elements. Implement the policy selection and action generation logic within component methods or actions.\n* **Backend (e.g., Node.js):**  Manage the replay buffer and the training process. Use TensorFlow.js or another machine learning library to train the policies.\n* **Real-time Collaboration (e.g., Socket.IO):**  Enable real-time communication between agents by broadcasting actions and observations over websockets.\n\n**4. Practical Example (Collaborative Writing):**\n\nImagine two LLM agents collaboratively writing a story.  `simplePolicy` could be a grammar-based policy that generates grammatically correct but perhaps less creative sentences.  `advancedPolicy` could use a powerful LLM to generate more imaginative and contextually relevant sentences.  HPF would allow the `simplePolicy` to learn from the `advancedPolicy`, improving its creativity over time, while the `advancedPolicy` provides the creativity boost, allowing the application to be computationally more efficient.\n\nBy adopting these strategies, JavaScript developers can harness the power of HPF to create more sophisticated and efficient multi-agent web applications powered by LLMs. The key advantage is balancing performance and complexity: leveraging simple agents for speed and complex agents for refined decision-making, all within a robust learning framework. This allows us to gradually move towards more complex interactions while maintaining manageable computational costs, opening exciting new possibilities for intelligent web applications.",
  "pseudocode": "```javascript\nclass HPF {\n  constructor(alphaParams, betaParams, learningRate, batchSize, updateInterval) {\n    this.alphaParams = alphaParams; // Parameters for VD policy with surrogate target (e.g., WQMIX)\n    this.betaParams = betaParams; // Parameters for VD policy with network constraints (e.g., QMIX)\n    this.targetAlphaParams = [...alphaParams]; // Target network parameters for alpha\n    this.targetBetaParams = [...betaParams]; // Target network parameters for beta\n    this.learningRate = learningRate;\n    this.batchSize = batchSize;\n    this.updateInterval = updateInterval;\n    this.replayBuffer = [];\n    this.step = 0;\n  }\n\n  selectAction(state, observations) {\n    let utilitiesAlpha = observations.map(obs => this.calculateUtility(obs, this.alphaParams));\n    let utilitiesBeta = observations.map(obs => this.calculateUtility(obs, this.betaParams));\n\n    let qAlpha = this.calculateQ(utilitiesAlpha, \"additive\"); // Or \"optimistic\"\n    let qBeta = this.calculateQ(utilitiesBeta, \"additive\"); // Or \"optimistic\"\n\n    let probabilities = this.calculateProbabilities(qAlpha, qBeta);\n    let chosenPolicy = this.samplePolicy(probabilities);\n\n    let actions = observations.map(obs => this.chooseAction(obs, chosenPolicy === \"alpha\" ? this.alphaParams : this.betaParams));\n    return { actions, chosenPolicy };\n  }\n\n\n  calculateUtility(observation, params) {\n    // Calculate individual agent utility using the provided observation and policy parameters. \n    // This will depend on the specific VD method (e.g. WQMIX or QMIX networks).\n    // Replace this with your actual utility calculation logic.\n\n    // Example (replace with your network logic):\n    return someNeuralNetwork(observation, params);  \n  }\n\n  calculateQ(utilities, method) {\n     if (method === \"additive\") {\n        return utilities.reduce((sum, utility) => sum + utility, 0);\n     } else if (method === \"optimistic\") {\n       // Calculate the joint action-value function for the optimal joint action using the target network for the chosen policy\n       // This is more complex and depends on how you implement the target network for joint optimal action selection.\n       // Placeholder implementation:\n       return this.calculateJointQOptimal(utilities);\n     }\n  }\n\n  calculateJointQOptimal(utilities) {\n      // Placeholder for joint optimal Q calculation.  \n      //  This needs to be implemented based on your specific target network setup for the VD policies.\n      return 0;\n  }\n\n  calculateProbabilities(qAlpha, qBeta) {\n      let temp = 1; // Temperature parameter\n      let expAlpha = Math.exp(qAlpha / temp);\n      let expBeta = Math.exp(qBeta / temp);\n\n      return {\n        alpha: expAlpha / (expAlpha + expBeta),\n        beta: expBeta / (expAlpha + expBeta)\n      };\n\n  }\n  samplePolicy(probabilities) {\n    let randomValue = Math.random();\n    return randomValue < probabilities.alpha ? \"alpha\" : \"beta\";\n  }\n\n\n  chooseAction(observation, params) {\n    //  Epsilon-greedy action selection.\n    // Replace this with your actual action selection using a neural network for the chosen policy and its parameters.\n\n    if (Math.random() < this.epsilon) {\n       return this.getRandomAction();\n    } else {\n      return this.getGreedyAction(observation, params); // Replace with network logic\n    }\n  }\n\n  getRandomAction() {\n    // Replace with your action space logic. Example:\n    return Math.floor(Math.random() * this.actionSpaceSize);\n  }\n  getGreedyAction(observation, params) {\n     // Replace with your actual greedy action selection based on Q values from the chosen policy network.\n     return 0; // Placeholder\n  }\n\n\n  update(reward, nextState) {\n        // Store transition in replay buffer\n        // Sample batch from replay buffer\n        // Calculate targets for both VD methods\n        // Calculate loss and update parameters\n        // ... (See the original pseudocode for the update logic)\n  }\n\n\n // ... (Rest of the HPF class implementation including: update(), loss calculation, target network updates, etc. )\n}\n\n\n```\n\n\n**Explanation and Purpose of Algorithm 1 (Add-HPF-WQ)**\n\nThe Add-HPF-WQ algorithm integrates two Value Decomposition (VD) methods for Multi-Agent Reinforcement Learning (MARL): WQMIX (`alpha` policy) and QMIX (`beta` policy).  Its purpose is to improve sample efficiency and find better policies than using either VD method alone.\n\n\n1. **Policy Extension and Adaptive Composition:** HPF doesn't create a new VD method. Instead, it maintains two existing VD policies (WQMIX and QMIX) and adaptively chooses between them during interaction with the environment.  The selection is probabilistic and based on the estimated Q-values of each policy (using either the additive or optimistic approach).\n\n2. **Boltzmann Policy Selection:** The `calculateProbabilities()` function uses a Boltzmann distribution to create a probability distribution over the two policies.  Higher Q-values lead to higher selection probabilities.\n\n3. **Mixed Experience Trajectories:**  Because policies are sampled at each step, the collected experience trajectories are mixed, containing transitions resulting from both WQMIX and QMIX. This allows both policies to learn from a richer set of experiences.\n\n4. **Instructive Constraint (Not Shown in JavaScript):**  Although not explicitly implemented in this simplified JavaScript version, the original algorithm uses an instructive constraint (KL-divergence based) between the utility functions of the two policies.  This constraint encourages the QMIX policy (with limited representation) to learn from the WQMIX policy (with full representation), helping to address the potential issue of relative overgeneralization.\n\n5. **Centralized Training, Decentralized Execution:**  Like other VD methods, HPF follows the CTDE paradigm. It uses global information during training but relies only on local observations during execution.\n\n\n\n\nThis JavaScript code provides a basic structure and illustrates the key concepts.  A full implementation requires integrating specific neural networks for WQMIX, QMIX, and the utility function calculations, along with the training loop and update logic.  Furthermore, the KL-divergence-based instructive constraint needs to be added to complete the HPF algorithm.",
  "simpleQuestion": "How can I best combine different value decomposition methods for multi-agent RL?",
  "timestamp": "2025-02-06T06:04:18.513Z"
}