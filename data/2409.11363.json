{
  "arxivId": "2409.11363",
  "title": "CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark",
  "abstract": "AI agents have the potential to aid users on a variety of consequential tasks, including conducting scientific research. To spur the development of useful agents, we need benchmarks that are challenging, but more crucially, directly correspond to real-world tasks of interest. This paper introduces such a benchmark, designed to measure the accuracy of AI agents in tackling a crucial yet surprisingly challenging aspect of scientific research: computational reproducibility. This task, fundamental to the scientific process, involves reproducing the results of a study using the provided code and data. We introduce CORE-Bench (Computational Reproducibility Agent Benchmark), a benchmark consisting of 270 tasks based on 90 scientific papers across three disciplines (computer science, social science, and medicine). Tasks in CORE-Bench consist of three difficulty levels and include both language-only and vision-language tasks. We provide an evaluation system to measure the accuracy of agents in a fast and parallelizable way, saving days of evaluation time for each run compared to a sequential implementation. We evaluated two baseline agents: the general-purpose AutoGPT and a task-specific agent called CORE-Agent. We tested both variants using two underlying language models: GPT-40 and GPT-40-mini. The best agent achieved an accuracy of 21% on the hardest level of tasks, showing the vast scope for improvement in automating routine scientific tasks. Having agents that can reproduce existing work is a necessary step towards building agents that can conduct novel research and could verify and improve the performance of other research agents. We hope that CORE-Bench can improve the state of reproducibility and spur the development of future research agents.",
  "summary": "This paper introduces CORE-Bench, a benchmark designed to evaluate the ability of AI agents to automatically reproduce the results of scientific papers. The benchmark focuses on computational reproducibility, a crucial aspect of scientific research.\n\nKey points relevant to LLM-based multi-agent systems:\n\n* The paper investigates whether general-purpose language models (LLMs) like AutoGPT, can be adapted to the specialized task of computational reproducibility.\n* Findings indicate that task-specific modifications to LLM-based agents significantly enhance their accuracy in reproducing research results.\n* Despite improvements with task-specific prompting, the best performing agent still achieved only 21% accuracy on the most challenging tasks, highlighting the need for further research in this area. \n* The paper emphasizes the potential of adaptable, LLM-based agents for automating complex scientific tasks and ultimately contributing to automating scientific research.",
  "takeaways": "This paper introduces CORE-Bench, a benchmark for evaluating AI agents on their ability to reproduce computational results from scientific papers. While it doesn't directly use JavaScript, the insights are incredibly valuable for JavaScript developers working on LLM-based multi-agent systems, especially for web development. Here's how you can apply these insights:\n\n**1. Building Reproducible Multi-Agent Systems:**\n\n* **Environment Standardization:** CORE-Bench emphasizes isolated virtual machines for testing.  JavaScript developers can use Docker to create consistent environments for their agents, ensuring reliable execution across different machines. This is especially important for web agents interacting with external APIs or services.\n* **Task Decomposition:** CORE-Bench tasks involve code execution, information extraction, and result reporting. You can break down complex multi-agent interactions in web apps similarly. For example, an agent handling user input can pass extracted information to another agent responsible for fetching data from a web API, followed by a final agent presenting the results. Libraries like LangChain can be used to chain these agent interactions. \n* **Testing Frameworks:** Use JavaScript testing frameworks like Jest or Mocha to create unit tests for individual agents and integration tests for multi-agent interactions. This mirrors the evaluation harness used in CORE-Bench, ensuring robust and reliable agent behavior.\n\n**2. Practical Web Development Scenarios:**\n\n* **Automated Content Generation:** Imagine building a multi-agent system to generate website content from research papers.  One agent could use an LLM to summarize the paper, another could extract relevant figures using a library like Tesseract.js for OCR, and a third could assemble the website content using a framework like React.\n* **Intelligent Chatbots for Research:** Create a multi-agent chatbot that helps users find information in research papers. One agent could interact with the user, another could query an LLM like GPT-4 to retrieve relevant paper sections, and a third could use a library like PDF.js to display those sections to the user.\n* **Collaborative Web Design:** Develop multi-agent systems where one agent gathers user design preferences, another agent suggests design templates using an LLM, and a third agent integrates user feedback to refine the design, all facilitated by real-time collaboration using a framework like Socket.IO.\n\n**3. JavaScript Frameworks and Libraries:**\n\n* **LangChain:** This framework provides tools to manage LLM interactions, chain agents, and manage prompts, directly applicable to the challenges of building multi-agent systems inspired by CORE-Bench.\n* **TensorFlow.js or Brain.js:**  These libraries can be used to build agents that process and analyze data, similar to the agents in CORE-Bench needing to extract results from code outputs.\n* **Node.js:** This runtime environment is ideal for building server-side logic for multi-agent web apps, allowing agents to communicate and coordinate tasks.\n\n**Key Takeaway:**\n\nCORE-Bench highlights the need for robust, reproducible, and well-tested multi-agent systems. By applying its principles, JavaScript developers can create sophisticated web applications that leverage LLMs effectively, leading to more intelligent and reliable web experiences.",
  "pseudocode": "No pseudocode block found.",
  "simpleQuestion": "Can AI agents reliably reproduce scientific research?",
  "timestamp": "2024-09-18T05:02:16.986Z"
}