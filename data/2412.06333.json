{
  "arxivId": "2412.06333",
  "title": "Augmenting the action space with conventions to improve multi-agent cooperation in Hanabi",
  "abstract": "The card game Hanabi is considered a strong medium for the testing and development of multi-agent reinforcement learning (MARL) algorithms, due to its cooperative nature, hidden information, limited communication and remarkable complexity. Previous research efforts have explored the capabilities of MARL algorithms within Hanabi, focusing largely on advanced architecture design and algorithmic manipulations to achieve state-of-the-art performance for a various number of cooperators. However, this often leads to complex solution strategies with high computational cost and requiring large amounts of training data. For humans to solve the Hanabi game effectively, they require the use of conventions, which often allows for a means to implicitly convey ideas or knowledge based on a predefined, and mutually agreed upon, set of \"rules\". Multi-agent problems containing partial observability, especially when limited communication is present, can benefit greatly from the use of implicit knowledge sharing. In this paper, we propose a novel approach to augmenting the action space using conventions, which act as special cooperative actions that span over multiple time steps and multiple agents, requiring agents to actively opt in for it to reach fruition. These conventions are based on existing human conventions, and result in a significant improvement on the performance of existing techniques for self-play and cross-play across a various number of cooperators within Hanabi.",
  "summary": "This research explores improving multi-agent cooperation in the card game Hanabi by incorporating \"conventions\" – pre-defined, mutually agreed-upon rules – into the agents' action space.  These conventions, inspired by human Hanabi strategies, enable implicit communication between agents without direct message passing. The results demonstrate faster learning and improved performance, especially in scenarios with three or more players, as well as better robustness in cross-play (agents cooperating with previously unseen partners).\n\n\nKey points for LLM-based multi-agent systems:\n\n* **Implicit Communication:** Conventions offer a way for agents to convey intentions without explicit language, potentially reducing the complexity of communication protocols in LLM-based systems.\n* **Action Space Augmentation:**  Adding conventions to the action space offers a higher-level abstraction for decision-making, similar to \"options\" in reinforcement learning, potentially simplifying the learning process for LLMs.\n* **Cross-play/Zero-Shot Coordination:** The success of convention-based agents in cross-play scenarios suggests their potential for LLM agents to cooperate effectively without prior joint training, enhancing adaptability in dynamic multi-agent environments.",
  "takeaways": "This paper presents a novel approach to improve multi-agent cooperation in partially observable environments by augmenting the action space with \"conventions.\"  These conventions, inspired by human strategies in the card game Hanabi, act as high-level cooperative actions that agents can subscribe to, facilitating implicit communication and coordination. Let's translate this concept into practical examples for JavaScript developers working on LLM-based multi-agent web applications:\n\n**Scenario 1: Collaborative Writing Application**\n\nImagine building a collaborative writing app where multiple LLM agents assist users in co-authoring documents.  Partial observability arises as each agent might only have access to a specific section of the document or a particular user's writing style preferences.  Conventions can be implemented to streamline the writing process:\n\n* **\"Introduce Topic\" Convention:**  When one agent finishes a paragraph, it can initiate the \"Introduce Topic\" convention.  Other agents, upon observing this action, can subscribe to the convention and contribute relevant points, examples, or supporting arguments related to the introduced topic, ensuring coherence and flow.\n\n* **JavaScript Implementation:**\n\n```javascript\n// Agent 1 initiates \"Introduce Topic\"\nagent1.setAction(\"CONVENTION_INTRODUCE_TOPIC\", { topic: \"Benefits of AI\" });\n\n// Agent 2 observes and subscribes\nif (observedAction === \"CONVENTION_INTRODUCE_TOPIC\") {\n  const topic = observedAction.data.topic; \n  const contribution = llm.generateText(`Supporting points for ${topic}`);\n  agent2.setAction(\"CONVENTION_CONTRIBUTE_TOPIC\", { text: contribution });\n}\n\n// LangChain framework could handle agent interaction and chain execution\n```\n\n\n**Scenario 2: Multi-Agent Chatbot for Customer Service**\n\nConsider a customer service scenario with multiple specialized chatbot agents, each handling different aspects of customer inquiries (e.g., order status, technical support, billing).  Conventions can help them seamlessly hand off conversations:\n\n* **\"Transfer to Billing\" Convention:** If the order status agent detects a billing-related issue, it initiates the \"Transfer to Billing\" convention. The billing agent, observing this, subscribes and takes over the conversation, providing specialized assistance.\n\n* **JavaScript Implementation (using a message queue like Redis):**\n\n```javascript\n// Order status agent publishes a transfer request\nredisClient.publish(\"agent_actions\", JSON.stringify({ \n  agentId: 'order_status',\n  action: 'CONVENTION_TRANSFER_BILLING',\n  data: { customerId: 123, issue: 'payment_failed' }\n}));\n\n// Billing agent subscribes to the channel\nredisClient.subscribe(\"agent_actions\", (message) => {\n  const action = JSON.parse(message);\n  if (action.action === 'CONVENTION_TRANSFER_BILLING') {\n      // Handle billing issue\n  }\n});\n```\n\n**Scenario 3: Real-time Collaborative Design Tool**\n\nIn a multi-user design tool (e.g., Figma, Canva equivalent), LLM agents can assist users with design suggestions. Conventions can ensure design consistency:\n\n* **\"Maintain Color Palette\" Convention:**  An agent can initiate the \"Maintain Color Palette\" convention.  Subsequent design suggestions from other agents would adhere to the defined palette, creating a visually harmonious design.\n\n* **JavaScript Implementation (using a shared state like Socket.io):**\n\n```javascript\n// Agent 1 sets the color palette\nsocket.emit('convention', { type: 'SET_COLOR_PALETTE', palette: ['#FF0000', '#0000FF'] });\n\n// Agent 2 receives the convention and adheres to it\nsocket.on('convention', (data) => {\n if (data.type === 'SET_COLOR_PALETTE') {\n  designAgent.setColorPalette(data.palette);\n }\n});\n```\n\n\n**Key JavaScript Considerations:**\n\n* **Communication Layer:**  Establish a robust communication mechanism (e.g., message queues, shared state using Socket.io, WebRTC) for agents to broadcast and observe actions.\n\n* **Convention Definition:** Create a clear and consistent convention schema (e.g., using JSON) to define the convention type, parameters, and expected behavior.\n\n* **LLM Integration:** Leverage JavaScript LLM libraries or frameworks (e.g., LangChain, Transformers.js) to generate agent responses and actions based on observed conventions.\n\n* **Experimentation:**  Start with simple conventions and gradually increase complexity as needed. Thoroughly test and evaluate the impact of conventions on agent cooperation and overall application performance.\n\n\nBy adopting these principles, JavaScript developers can harness the power of conventions to build more robust, cooperative, and intelligent LLM-based multi-agent web applications. This approach simplifies coordination, reduces communication overhead, and enables more sophisticated agent interactions, ultimately leading to a more seamless and engaging user experience.",
  "pseudocode": "The paper contains three pseudocode blocks describing algorithms for independent Q-learning in a turn-based environment, with and without conventions, and with an augmented action-convention space. Here are their JavaScript conversions with explanations:\n\n**Algorithm 1: Independent Q-learning in a turn-based environment**\n\n```javascript\nfunction independentQLearning(alpha, gamma, numPlayers, epsilon) {\n  // Initialize Q-values for all observations and actions\n  const Q = new Map(); // Use a Map to store Q-values for (observation, action) pairs\n\n  for (let episode = 0; episode < numEpisodes; episode++) { // Run for a specified number of episodes\n    let environment = resetEnvironment();\n    let t = 0;\n\n    while (!isTerminal(environment)) {\n      for (let i = 0; i < numPlayers; i++) {\n        const observation = getObservation(environment, i);\n\n        // Choose action using epsilon-greedy policy\n        let action;\n        if (Math.random() < epsilon) {\n          action = getRandomAction();\n        } else {\n          action = getBestAction(Q, observation);\n        }\n\n        const { nextEnvironment, reward } = takeAction(environment, action, i);\n        const nextObservation = getObservation(nextEnvironment, i);\n        const far = calculateFAR(nextEnvironment, i, numPlayers, gamma); // Calculate Forward Accumulated Reward\n\n        // Update Q-value\n        const oldQ = getQValue(Q, observation, action);\n        const nextBestQ = Math.max(...getAllQValues(Q, nextObservation).values()); // Use ... for spreading Map values to array from ES6\n        const newQ = oldQ + alpha * (far + gamma * nextBestQ - oldQ);\n        setQValue(Q, observation, action, newQ);\n\n        environment = nextEnvironment;\n        t++;\n      }\n    }\n  }\n\n\n  // Helper functions (replace with your environment's logic)\n  function resetEnvironment() { /* ... */ }\n  function isTerminal(environment) { /* ... */ }\n  function getObservation(environment, player) { /* ... */ }\n  function getRandomAction() { /* ... */ }\n  function getAllQValues(Q, observation) { /* Returns a Map of actions to Q-values*/ }\n  function getBestAction(Q, observation) { /* ... */ }\n\n  function getQValue(Q, observation, action) {\n    const key = `${observation}-${action}`;\n    return Q.has(key) ? Q.get(key) : 0;  // Return 0 if Q-value doesn't exist\n  }\n\n  function setQValue(Q, observation, action, value) {\n    const key = `${observation}-${action}`;\n    Q.set(key, value);\n  }\n\n   function takeAction(environment, action, player) { /* ... */ }\n  function calculateFAR(environment, playerIndex, numPlayers, gamma) { /* ... */ }\n\n}\n\n\n\n```\n\n*Explanation:* This algorithm implements independent Q-learning, where each agent learns its own Q-function based on its individual observations and rewards in a turn-based environment.  The `calculateFAR` function computes the Forward Accumulated Reward, a crucial aspect in turn-based MARL. Helper functions are placeholders for the specific game environment logic. Epsilon-greedy action selection is used for exploration.\n\n**Algorithm 2: Independent Q-learning with conventions in a turn-based environment**\n\n```javascript\n// ... (similar initialization and helper functions as Algorithm 1, with additions for conventions)\n\nfunction independentQLearningWithConventions(alpha, gamma, numPlayers, epsilon, conventions) {\n  const Q = new Map(); // Initialize Q-values\n\n\n  // Loop through episodes and time steps (similar structure as Algorithm 1)\n\n        // ... inside the player loop ...\n\n        // Determine available and active conventions\n        let availableConventions = [];\n        let activeConventions = [];\n\n        for (let convention of conventions) {\n          if (convention.I(observation)) {\n            availableConventions.push(convention);\n          } else if (convention.F(observation)) {\n            activeConventions.push(convention);\n          }\n        }\n\n        // Choose convention or action based on Q-values and policy\n        let chosenConvention;\n\n         if (Math.random() < epsilon) {\n           const allOptions = availableConventions.concat(activeConventions).concat(getAllActions()); // Combine for random choice\n           chosenConvention = allOptions[Math.floor(Math.random() * allOptions.length)];\n\n        } else {\n            chosenConvention = getBestOption(Q, observation, availableConventions, activeConventions);\n        }\n\n\n\n        // Execute chosen convention or action\n        let action;\n        if (isConvention(chosenConvention)) {\n          if (availableConventions.includes(chosenConvention)) {\n            action = chosenConvention.pi(observation);\n          } else if (activeConventions.includes(chosenConvention)) {\n            action = chosenConvention.pF(observation);\n          }\n        } else { // Regular action\n            action = chosenConvention; // actions can be selected directly without translation\n        }\n\n        // ... (Rest of the inner loop - taking action, getting rewards, updating Q)\n\n\n  // Helper functions (replace with your environment's and convention logic)\n  function getBestOption(Q, observation, availableConventions, activeConventions) { /* ... */ }\n  function isConvention(option) {/* ... returns true if object is a convention */}\n  function getAllActions() { /* returns array of possible primitive actions */}\n  // ... other helper functions for conventions\n}\n```\n\n*Explanation:*  This algorithm extends Algorithm 1 by incorporating *conventions*. Each convention has an initial condition (`I`) and a final condition (`F`), and associated policies (`pi`, `pF`).  Agents now choose between starting/continuing a convention or taking a regular action. Note the lack of direct communication about which convention is being followed.\n\n\n**Algorithm 3: Independent Q-learning with an augmented action-convention space in a turn-based environment**\n\n```javascript\n// ... (Similar initialization and helper functions as Algorithm 2)\n\nfunction independentQLearningWithAugmentedSpace(alpha, gamma, numPlayers, epsilon, conventions) {\n    const Q = new Map(); // Initialize Q-values\n\n  // ... (Outer loop structure remains similar)\n\n        // ... inside the player loop ...\n\n        //  Augmenting the action space\n        const augmentedActions = getAllActions().concat(conventions); // directly concatenate actions and convention objects\n\n\n        // Choose an action or convention\n        let chosenOption;\n        if (Math.random() < epsilon){\n           chosenOption = augmentedActions[Math.floor(Math.random()* augmentedActions.length)];\n        } else {\n           chosenOption = getBestAugmentedAction(Q, observation, augmentedActions);\n        }\n\n\n        // Translate to an environment action if it was a convention\n        let action;\n\n        if (isConvention(chosenOption)) {\n            // Find the convention in the original conventions array\n            const originalConvention = conventions.find(c => c === chosenOption);\n            if (originalConvention.I(observation)) { // Check Initial conditions\n               action = originalConvention.pi(observation);\n            } else if (originalConvention.F(observation)){ // Check Final condition\n               action = originalConvention.pF(observation);\n            }\n         } else {\n            action = chosenOption; // chosen option is a primitive action\n         }\n        // ... (Rest of the inner loop remains similar - taking action, updating Q)\n\n        // ... (Helper functions similar to Algorithm 2 + adjustments for augmented space)\n         function getBestAugmentedAction(Q, observation, augmentedActions) { /* ... */ }\n\n}\n\n```\n\n*Explanation:* This algorithm further extends Algorithm 2 by directly incorporating conventions into the action space. The core logic is similar to Algorithm 2 but simplifies the action selection process as conventions are selected directly from the augmented action space. The convention policies (`pi` and `pF`) translate the chosen convention into a primitive action based on the current observation when the appropriate condition (`I` and `F` respectively) are met.\n\n\nThese JavaScript implementations provide a starting point for understanding and implementing multi-agent Q-learning with conventions in a turn-based environment. Remember to adapt the helper functions to match the specifics of your game and convention definitions. Using a proper JavaScript game framework and libraries for data structures like `Map` can significantly improve code clarity and efficiency.",
  "simpleQuestion": "Can conventions improve Hanabi MARL performance?",
  "timestamp": "2024-12-10T06:07:58.829Z"
}