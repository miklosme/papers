{
  "arxivId": "2411.07464",
  "title": "BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating Machine Learning Tasks",
  "abstract": "Large Language Models (LLMs) excel in diverse applications including generation of code snippets, but often struggle with generating code for complex Machine Learning (ML) tasks. Although existing LLM single-agent based systems give varying performance depending on the task complexity, they purely rely on larger and expensive models such as GPT-4. Our investigation reveals that no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama perform far worse than GPT-4 in a single-agent setting. With the motivation of developing a cost-efficient LLM based solution for solving ML tasks, we propose an LLM Multi-Agent based system which leverages combination of experts using profiling, efficient retrieval of past observations, LLM cascades, and ask-the-expert calls. Through empirical analysis on ML engineering tasks in the MLAgentBench benchmark, we demonstrate the effectiveness of our system, using no-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and expert to serve occasional ask-the-expert calls for planning. With 94.2% reduction in the cost (from $0.931 per run cost averaged over all tasks for GPT-4 single agent system to $0.054), our system is able to yield better average success rate of 32.95% as compared to GPT-4 single-agent system yielding 22.72% success rate averaged over all the tasks of MLAgentBench.",
  "summary": "This paper introduces BudgetMLAgent, a multi-agent system designed to automate machine learning tasks cost-effectively using less expensive LLMs.  It demonstrates that combining less expensive LLMs like Gemini-Pro with techniques like profiling (assigning distinct roles/personas to agents), cascades (chaining LLMs of increasing cost), retrieval of past actions, and occasional calls to more powerful LLMs (like GPT-4) for expert planning can achieve better performance at a much lower cost than using a single, expensive LLM like GPT-4 alone.  The system achieves significant cost reduction (up to 94.2%) while maintaining or even exceeding the performance of single-agent systems in several ML tasks.  BudgetMLAgent addresses the economic challenges of using large LLMs by leveraging the strengths of smaller, more affordable LLMs in a cooperative multi-agent framework.",
  "takeaways": "This paper offers several valuable insights for JavaScript developers building LLM-based multi-agent applications, especially focusing on cost-effectiveness:\n\n**1. Multi-Agent Profiling & Specialization:**\n\n* **Concept:**  Instead of a single LLM trying to do everything, the paper suggests using multiple specialized agents, each with a distinct role (Planner, Code Editor, File Reader, etc.). This mirrors real-world software development teams.\n* **JavaScript Implementation:**  You can create different JavaScript classes or modules, each representing an agent.  Each agent would interact with a specific LLM tailored for its task. For instance:\n    * **Planner Agent:** Uses LangChain's `AgentExecutor` with custom tools and an LLM proficient in planning (e.g., GPT-4, or potentially a fine-tuned smaller model).\n    * **Code Editor Agent:** Interacts with a code-specialized LLM (e.g., CodeLlama) via an API call, receiving code modification instructions from the Planner and returning the edited code.\n    * **File System Agent:**  Handles file system interactions using Node.js's `fs` module, abstracting those operations away from the other agents.\n\n**Example:**\n\n```javascript\n// Planner Agent (using LangChain)\nclass PlannerAgent {\n  constructor(llm) { this.llm = llm; }\n  async plan(problem) { /* ... uses LLM to create a plan ... */ }\n}\n\n// Code Editor Agent\nclass CodeEditorAgent {\n  async editCode(instructions, code) { \n    const editedCode = await callCodeLlamaAPI(instructions, code);\n    return editedCode;\n  }\n}\n\n\nconst planner = new PlannerAgent(new OpenAI(...)); // GPT-4\nconst editor = new CodeEditorAgent(); \n\nconst problem = \"Create a React component to display user data\";\nconst plan = await planner.plan(problem); // Get plan from GPT-4\n// ... execute plan steps, using editor.editCode where needed ...\n```\n\n**2. LLM Cascades:**\n\n* **Concept:** Chain LLMs from cheapest to most expensive. Start with a free/cheap model; if it fails, escalate to a more powerful (and costly) one.\n* **JavaScript Implementation:**  Create a chain of LLM calls.  You can manage this logic within an agent or create a separate \"Cascade Manager\" module.\n\n**Example:**\n\n```javascript\nasync function getLLMResponse(prompt) {\n  try {\n    return await callGeminiAPI(prompt); // Try Gemini first\n  } catch (e) {\n    try {\n      return await callGPT3API(prompt); // Then GPT-3.5-turbo\n    } catch (e) {\n      return await callGPT4API(prompt); // Finally GPT-4 \n    }\n  }\n}\n```\n\n**3. Ask-the-Expert (Lifeline):**\n\n* **Concept:** Allow the Planner agent to request help from a more powerful LLM when it's stuck.  Limit these calls to control costs.\n* **JavaScript Implementation:** Include a specific action in your Planner agent's action space (e.g., \"consultExpert\").  Implement logic to track lifeline usage and restrict further expert calls once the limit is reached.\n\n**Example (Extending the Planner Agent):**\n\n```javascript\nclass PlannerAgent {\n  // ... previous code ...\n  lifelines = 5;\n\n  async plan(problem) {\n    // ... planning logic ...\n\n    if (this.isStuck() && this.lifelines > 0) {\n      this.lifelines--;\n      const expertResponse = await callGPT4API(\"I'm stuck.  Here's the context: ...\");\n      // ... incorporate expert advice into the plan ...\n    }\n    // ... rest of planning logic ...\n  }\n}\n```\n\n**4. Retrieval of Past Observations:**\n\n* **Concept:** Use a logging mechanism to store previous actions and observations.  Provide access to this log to the agents to improve decision-making.  LangChain already handles this with callbacks.\n\n* **JavaScript Implementation:** Use a database (e.g., MongoDB, Pinecone if embeddings are needed for semantic search) or even a simple file to store the logs.\n\n**Key Libraries and Frameworks:**\n\n* **LangChain:** Excellent for building agent-based systems, managing prompts and chains, interacting with LLMs, and handling memory.\n* **Llama.cpp, Replicate, OpenAI Node.js library:** To interact with different LLMs via API calls.\n* **Node.js `fs` module:** For file system operations.\n* **Vector databases (e.g., Pinecone, Weaviate, etc.):** for storing, managing, and retrieving embeddings of past interactions.\n\n\nBy combining these techniques, JavaScript developers can build more sophisticated and cost-effective LLM-based multi-agent applications.  The paper provides a solid conceptual foundation, and the examples above illustrate how to translate those concepts into practical JavaScript code using popular frameworks and libraries.  This opens up many possibilities for innovative web applications with complex problem-solving capabilities.",
  "pseudocode": "No pseudocode block found.",
  "simpleQuestion": "Can cheaper LLMs automate ML tasks?",
  "timestamp": "2024-11-13T06:02:43.260Z"
}