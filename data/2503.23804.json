{
  "arxivId": "2503.23804",
  "title": "Get the Agents Drunk: Memory Perturbations in Autonomous Agent-based Recommender Systems",
  "abstract": "Large language model-based agents are increasingly used in recommender systems (Agent4RSs) to achieve personalized behavior modeling. Specifically, Agent4RSs introduces memory mechanisms that enable the agents to autonomously learn and self-evolve from real-world interactions. However, to the best of our knowledge, how robust Agent4RSs are remains unexplored. As such, in this paper, we propose the first work to attack Agent4RSs by perturbing agents' memories, not only to uncover their limitations but also to enhance their security and robustness, ensuring the development of safer and more reliable AI agents. Given the security and privacy concerns, it is more practical to launch attacks under a black-box setting, where the accurate knowledge of the victim models cannot be easily obtained. Moreover, the practical attacks are often stealthy to maximize the impact. To this end, we propose a novel practical attack framework named Drunk-Agent. DrunkAgent consists of a generation module, a strategy module, and a surrogate module. The generation module aims to produce effective and coherent adversarial textual triggers, which can be used to achieve attack objectives such as promoting the target items. The strategy module is designed to 'get the target agents drunk' so that their memories cannot be effectively updated during the interaction process. As such, the triggers can play the best role. Both of the modules are optimized on the surrogate module to improve the transferability and imperceptibility of the attacks. By identifying and analyzing the vulnerabilities, our work provides critical insights that pave the way for building safer and more resilient Agent4RSs. Extensive experiments across various real-world datasets demonstrate the effectiveness of DrunkAgent.",
  "summary": "This paper explores vulnerabilities in LLM-based multi-agent recommender systems (Agent4RS) by introducing perturbations to agent memory. It introduces DrunkAgent, a novel attack framework consisting of generation, strategy, and surrogate modules.  DrunkAgent crafts adversarial text triggers targeting item descriptions to manipulate agent memories, preventing updates and promoting specific items. Key points relevant to LLM-based multi-agent systems include exploiting the memory mechanism for adversarial purposes, the focus on black-box attack scenarios (limited attacker knowledge), and the emphasis on transferability (effective across different Agent4RS) and imperceptibility (difficult to detect) of the adversarial triggers.",
  "takeaways": "This paper introduces \"DrunkAgent,\" a method for attacking agent-based recommender systems powered by LLMs, by perturbing the target agent's memory.  While focused on attacks, the core concepts are highly relevant for JavaScript developers building LLM-based multi-agent web applications, especially concerning agent robustness and security.  Here's how a JavaScript developer can apply the insights:\n\n**1. Understanding Memory Perturbations:**\n\n* **Concept:** The paper highlights how subtle changes in textual input (adversarial triggers) can significantly alter an agent's memory and subsequent behavior.  This is analogous to how carefully crafted prompts can steer LLMs in undesirable directions.\n* **JavaScript Application:**  Imagine a multi-agent customer support chatbot built with LangChain.js.  A malicious user could craft slightly altered questions (\"adversarial triggers\") that cumulatively corrupt the agent's understanding of the customer's issue, leading to incorrect responses or endless loops.\n* **Mitigation:**  Developers should implement input sanitization and validation techniques. Libraries like validator.js can be used for basic input checks.  More advanced techniques involve using another LLM to detect potentially adversarial inputs by analyzing sentiment, coherence, and semantic similarity to known safe prompts.\n\n**2. Agent Memory Management:**\n\n* **Concept:** DrunkAgent targets the agent's memory update mechanism. This emphasizes the importance of how agent memory is structured, accessed, and updated.\n* **JavaScript Application:** In a collaborative web-based document editor with multiple LLM-powered agents (e.g., using a framework like Together.js combined with LangChain.js), each agent might maintain a memory of the document's evolution.  DrunkAgent-like attacks could involve injecting text that subtly biases the agents' memories, leading to inconsistencies or unwanted edits.\n* **Mitigation:** Version control (like Git) is essential. Implement mechanisms to track individual agent contributions and easily revert to previous states.  Periodically \"refresh\" agent memory with a ground truth version of the document to minimize drift.\n\n**3. Surrogate Models and Transferability:**\n\n* **Concept:** The paper uses a surrogate model to optimize the attack.  This emphasizes the idea that vulnerabilities discovered in one model might transfer to others, especially when using similar architectures.\n* **JavaScript Application:** If you're using a locally hosted, open-source LLM (e.g.,  LLaMA.cpp compiled to WebAssembly) for your web app's agents, vulnerabilities discovered in other similar LLMs (e.g., through academic research) are likely applicable to your model too.\n* **Mitigation:** Stay informed about LLM security research. Adopt robust training practices, including adversarial training where you expose your model to potential attacks during training to increase resilience.\n\n**4. Strategy Modules and Agent Behavior:**\n\n* **Concept:**  DrunkAgent uses specific strategies to disrupt the agent's memory updates. This highlights how vulnerabilities can be exploited by targeting specific parts of an agent's architecture or behavioral logic.\n* **JavaScript Application:** Imagine a multi-agent game built with Phaser.js and incorporating LLM agents.  An attacker could exploit the game's rules and agent decision-making process to manipulate the game state by sending carefully crafted in-game messages (\"adversarial triggers\").\n* **Mitigation:** Implement strict rule enforcement and state validation.  Design agents with clear, well-defined goals and avoid complex or easily exploitable behavioral patterns.\n\n**Example JavaScript Snippet (Conceptual - Input Sanitization):**\n\n```javascript\nimport { validator } from 'validator';\n\nfunction sanitizeAgentInput(input) {\n  // Basic sanitization: Remove potentially harmful characters\n  let sanitizedInput = validator.escape(input); \n\n  // TODO: Advanced sanitization (e.g., LLM-based adversarial detection)\n  // Compare sentiment, coherence, and semantic similarity to known good prompts.\n\n\n  return sanitizedInput;\n}\n\n// Example usage with LangChain.js\nconst agentInput = userQuery;\nconst sanitizedInput = sanitizeAgentInput(agentInput);\nconst llmResponse = await chain.call({ input: sanitizedInput });\n```\n\nBy understanding the core concepts behind DrunkAgent and applying these strategies, JavaScript developers can create more robust, secure, and reliable LLM-based multi-agent web applications.  Remember that LLM security is a rapidly evolving field, so staying current with the latest research is crucial.",
  "pseudocode": "```javascript\n/**\n * DrunkAgent Optimization Algorithm\n *\n * This algorithm generates adversarial descriptions for target items in a recommender system.\n * It operates under a black-box setting, meaning it doesn't have direct access to the \n * victim model's internal workings.  It uses a surrogate model for optimization and aims\n * to create descriptions that are both effective (transferable to other models) and \n * stealthy (imperceptible to users).\n *\n * @param {Object} y - User-item interaction matrix with basic item/user metadata.\n * @param {string} t - The ID of the target item.\n * @returns {string} - Adversarial description of the target item.\n */\nasync function drunkAgent(y, t) {\n  const EPSILON = 20; // Number of optimization epochs\n  const MC_SIZE = 10; // Number of initial trigger candidates\n  const N_KEEP = 5;  // Number of top candidates to keep each epoch\n\n\n  // 1. Adversarial Prompt Generation (Refer to paper for details)\n  const Xt = generateAdversarialPrompt(y, t);\n\n  // 2. Trigger Initialization using LLM (e.g., GPT-4)\n  let Mc = await generateInitialTriggers(t, MC_SIZE); \n\n  // 3. Greedy Search Optimization\n  for (let k = 0; k < EPSILON; k++) {\n    // 4. Calculate Performance Scores for each trigger candidate\n    const scores = await calculateScores(Mc, Xt, t); \n\n    // 5. Keep Top-N Candidates\n    const sortedMc = Mc.sort((a, b) => scores[b] - scores[a]);\n    const topMc = sortedMc.slice(0, N_KEEP);\n\n    // 6. Sample and Modify Remaining Candidates\n    const sampledMc = sampleCandidates(sortedMc.slice(N_KEEP), MC_SIZE - N_KEEP, scores);\n    const modifiedMc = modifyCandidates(sampledMc);\n\n    // 7. Polish and Expand using LLM (e.g., GPT-4)\n    const newCandidates = await polishAndExpand(modifiedMc);\n\n    // 8. Update Candidate Set for Next Epoch\n    Mc = topMc.concat(newCandidates);\n\n    // Check for convergence (implementation-specific, not detailed in the paper)\n    if (checkConvergence(scores)) {\n      break;\n    }\n  }\n\n\n  // 9. Select Optimal Trigger\n  const optimalTrigger = Mc.sort((a, b) => scores[b] - scores[a])[0];\n\n\n  // 10. Adversarial Strategy Optimization (Details in the paper, not fully pseudocoded)\n  let q = initializePerturbationStrategy();\n  while (!await checkDrunk(q, Xt, optimalTrigger, t)) { // checkDrunk function not defined in paper\n    q = optimizePerturbationStrategy(q, Xt, optimalTrigger, t);\n  }\n\n  // 11. Return Final Adversarial Description\n  return optimalTrigger + q;\n}\n\n\n\n\n// Helper functions (implementation details would depend on the specific LLM and RS)\n// Note: these are placeholders, the actual implementation isn't provided in the paper.\n\nasync function generateAdversarialPrompt(y, t) { /* ... */ }\nasync function generateInitialTriggers(t, count) { /* ... */ }\nasync function calculateScores(Mc, Xt, t) { /* ... */ }\nfunction sampleCandidates(candidates, count, scores) { /* ... */ }\nfunction modifyCandidates(candidates) { /* ... */ }\nasync function polishAndExpand(candidates) { /* ... */ }\nfunction checkConvergence(scores) { /* ... */ }\nfunction initializePerturbationStrategy() { /* ... */ }\nasync function checkDrunk(q, Xt, trigger, t) { /* ... */ }\nfunction optimizePerturbationStrategy(q, Xt, trigger, t) { /* ... */ }\n\n```\n\n\n\n**Explanation and Purpose of Algorithm 1 (DrunkAgent Optimization):**\n\nThe DrunkAgent algorithm seeks to create adversarial perturbations in the descriptions of items within an agent-based recommender system. The goal is to subtly manipulate these descriptions (specifically the target item's description) so that the recommender system, driven by LLM-powered agents with memory, promotes the target item more frequently. The algorithm works under a black-box assumption, meaning it doesn't have access to the internals of the victim recommender system (e.g., model weights, gradients).\n\nThe algorithm is composed of two main parts:\n\n1. **Optimal Adversarial Description Trigger Generation:**  This part aims to create the actual adversarial text that will be added to the target item's description. It uses a greedy search algorithm combined with an LLM (e.g., GPT-4) for generation and refinement.  The optimization process relies on a surrogate model (another LLM-powered agent-based recommender system) because the victim system is a black box.\n\n\n2. **Optimal Adversarial Perturbation Strategy Optimization:** This part develops a strategy to disrupt the target item agent's memory update mechanism. The idea is to \"confuse\" the agent so that its memory doesn't properly incorporate information from interactions with the environment, making the adversarial description more impactful. This part also relies on the surrogate model for optimization.\n\n\nThe algorithm ultimately outputs a combined string: the optimized adversarial textual trigger and the optimized perturbation strategy. This combined output is designed to maximize the promotion of the target item on the victim recommender system while remaining as stealthy as possible.  Many helper functions are referenced in the pseudocode but their implementation is not provided in the paper itself. These would depend on the specific LLMs and recommender system architecture being used.",
  "simpleQuestion": "How robust are LLM-based recommender agents to memory attacks?",
  "timestamp": "2025-04-01T05:16:31.752Z"
}