{
  "arxivId": "2412.03624",
  "title": "HOW TO CORRECTLY DO SEMANTIC BACKPROPAGATION ON LANGUAGE-BASED AGENTIC SYSTEMS",
  "abstract": "Language-based agentic systems have shown great promise in recent years, transitioning from solving small-scale research problems to being deployed in challenging real-world tasks. However, optimizing these systems often requires substantial manual labor. Recent studies have demonstrated that these systems can be represented as computational graphs, enabling automatic optimization. Despite these advancements, most current efforts in Graph-based Agentic System Optimization (GASO) fail to properly assign feedback to the system's components given feedback on the system's output. To address this challenge, we formalize the concept of semantic backpropagation with semantic gradients—a generalization that aligns several key optimization techniques, including reverse-mode automatic differentiation and the more recent TextGrad by exploiting the relationship among nodes with a common successor. This serves as a method for computing directional information about how changes to each component of an agentic system might improve the system's output. To use these gradients, we propose a method called semantic gradient descent which enables us to solve GASO effectively. Our results on both BIG-Bench Hard and GSM8K show that our approach outperforms existing state-of-the-art methods for solving GASO problems. A detailed ablation study on the LIAR dataset demonstrates the parsimonious nature of our method. A full copy of our implementation is publicly available at https://github.com/HishamAlyahya/semantic_backprop",
  "summary": "This paper introduces \"semantic backpropagation,\" a method for optimizing language-based multi-agent AI systems represented as computational graphs. It addresses the challenge of effectively assigning feedback to individual agent components based on overall system output.  Semantic backpropagation generalizes the concept of gradients from numerical optimization to semantic feedback, allowing for the use of LLMs in both forward execution of agents and optimization of the system. This allows for automatic optimization of the multi-agent system, reducing manual effort. Key to the method is incorporating \"neighborhood\" information during feedback propagation—considering the interplay between connected agents rather than treating them in isolation, a limitation of prior methods like TextGrad.  This is demonstrated to improve performance in question-answering tasks on various benchmarks, including BIG-Bench Hard and GSM8K. The method's minimalist nature and effectiveness in optimizing complex agentic systems are emphasized.",
  "takeaways": "Let's explore how JavaScript developers can apply the \"Semantic Backpropagation\" paper's insights to LLM-based multi-agent web applications.\n\n**Scenario:** Imagine building a collaborative writing web app using LLMs. Multiple agents contribute: a \"Grammar Agent,\" a \"Style Agent,\" a \"Fact-Checking Agent,\" and a \"Content Suggestion Agent.\" Each agent is an LLM with a specialized prompt. The user's input and agent outputs flow through a directed acyclic graph (DAG). The final output is the polished text.\n\n**Challenge:** How to optimize each agent's prompt based on user feedback (e.g., \"The style is too formal\") without manually tweaking prompts for every edge case.\n\n**Applying Semantic Backpropagation:**\n\n1. **Representing the System in JavaScript:**\n    - Use a library like `dagre-d3` or a custom class to represent the agent DAG. Each node can be an object with properties like `agentType`, `prompt`, and `output`. Edges define the data flow.\n\n    ```javascript\n    class AgentNode {\n      constructor(agentType, prompt) {\n        this.agentType = agentType;\n        this.prompt = prompt;\n        this.output = \"\"; // Initially empty\n      }\n      // ... methods to execute agent (LLM call), store output, etc.\n    }\n\n    // Example DAG setup (simplified):\n    const grammarAgent = new AgentNode(\"grammar\", \"Ensure perfect grammar.\");\n    const styleAgent = new AgentNode(\"style\", \"Adopt a casual tone.\");\n    // ... connect agents in DAG structure\n    ```\n\n2. **Implementing Semantic Gradients and Backpropagation:**\n    - When the user provides feedback, use another LLM (as described in the paper) to generate semantic gradients for each agent.  These gradients are instructions (in natural language) on how to improve each agent's prompt. LangChain is a convenient framework for this type of LLM management.\n\n    ```javascript\n    async function getSemanticGradient(agent, feedback, neighbors) {\n      const prompt = `Improve this agent's prompt based on this feedback: ${feedback}.\n      Current prompt: ${agent.prompt}\n      Neighbor outputs: ${neighbors.map(n => n.output).join('\\n')}`; // Crucial: neighbor context\n      const llmResponse = await makeLLMCall(prompt);  // Use LangChain or similar\n      return llmResponse; // Semantic gradient (prompt improvement instruction)\n    }\n    ```\n\n3. **Semantic Gradient Descent (Prompt Optimization):**\n    - Use the semantic gradients to update agent prompts. This, too, can involve an LLM call (as in the paper's Implementation 2) to refine the prompts iteratively.\n\n    ```javascript\n    async function updatePrompt(agent, gradient) {\n      const prompt = `Refine this prompt based on the following instruction: ${gradient}\n      Current Prompt: ${agent.prompt}`;\n      const llmResponse = await makeLLMCall(prompt);\n      agent.prompt = extractRefinedPrompt(llmResponse); // Extract from LLM output\n    }\n    ```\n\n4. **Client-Side Integration (using React, for example):**\n    - The DAG execution and prompt optimization can happen on the server. Update the client-side UI (e.g., the generated text) using websockets or a polling mechanism.\n\n**Key JavaScript Considerations:**\n\n* **Asynchronous operations:** LLM calls are asynchronous. Use `async/await` or Promises effectively.\n* **LLM interaction libraries:** Explore LangChain, Llama.cpp bindings, or similar for LLM management and prompt engineering.\n* **DAG visualization:** Use libraries like `dagre-d3` for visualizing the agent network, debugging, and understanding the data flow.\n* **State management:** Use React, Vue, or another framework to manage the application's state, especially as agent outputs and prompts change asynchronously.\n\n**Practical Implications for Web Development:**\n\n* **Automated agent improvement:** User feedback drives agent prompt optimization without manual intervention.\n* **Adaptive systems:** Agents can adapt to different writing styles and user preferences over time.\n* **Experimentation and A/B testing:** Test different DAG structures and prompt optimization strategies to find the most effective approach.\n\n\nThis approach empowers developers to build dynamic, user-centric multi-agent web applications where LLMs continuously learn and improve based on real-world interactions.  The concept of semantic gradients enables a much more nuanced and efficient way to optimize these complex systems than relying on simple prompt tweaking or brute-force methods.",
  "pseudocode": "The paper contains two algorithms described in pseudocode blocks. Here's the JavaScript translation and explanation for each:\n\n**Algorithm 1: Semantic Backpropagation**\n\n```javascript\n/**\n * Performs semantic backpropagation on a computational graph.\n *\n * @param {object} graph - The computational graph.  Assumed to have methods for topological sorting and accessing successors/predecessors.\n * @param {object} backwardFunctions - An object mapping (node, predecessor) pairs to backward functions. Key format: `nodeID,predecessorID`.\n * @param {object} aggregationFunctions - An object mapping node IDs to aggregation functions.\n * @param {string} outputGradient - The gradient of the loss with respect to the final answer.\n * @returns {object} An object containing the semantic gradient for each node. Key format: `nodeID`.\n */\nfunction semanticBackpropagation(graph, backwardFunctions, aggregationFunctions, outputGradient) {\n  const gradients = {};\n  gradients[graph.getOutputNodeID()] = outputGradient; // Initialize gradient for output node\n\n  const sortedNodes = graph.reverseTopologicalSort(graph.getNodesExceptOutput());\n\n  for (const node of sortedNodes) {\n    let nodeGradients = [];\n\n    for (const successor of graph.getSuccessors(node)) {\n      const predecessors = graph.getPredecessors(successor);\n      const predecessorValues = predecessors.map(p => graph.getNodeValue(p));\n      const successorValue = graph.getNodeValue(successor);\n\n      // Dynamically lookup the backward function from the provided object.\n      const backwardFunctionKey = `${successor.id},${node.id}`; // Create key\n      const backwardFunction = backwardFunctions[backwardFunctionKey];\n\n      // Call the backward function;  Assume it returns a string.\n      const gradient = backwardFunction(predecessorValues, successorValue, gradients[successor.id]);\n      nodeGradients.push(gradient);\n    }\n\n\n    gradients[node.id] = aggregationFunctions[node.id](nodeGradients);\n  }\n\n  return gradients;\n}\n\n```\n\n**Explanation:** This algorithm computes semantic gradients for each node in a computational graph, working backward from the output. It generalizes traditional backpropagation by using semantically interoperable representations (strings) for gradients and allowing arbitrary functions for computation and aggregation.  It uses a dynamically constructed key format for the `backwardFunctions` object.  This format makes calling the correct function trivial using bracket notation and helps keep the interface generic. The `graph` object needs to implement necessary methods and store relevant information to make the algorithm function correctly, such as methods `reverseTopologicalSort`, `getSuccessors`, `getPredecessors`, `getNodeValue` and a method `getOutputNodeID` to retrieve the ID of the output node of the provided graph.  Example implementations of this and the aforementioned methods are shown in the accompanying code snippet.\n\n\n\n**Algorithm 2: Semantic Gradient Descent**\n\n```javascript\n/**\n * Optimizes a parameterized graph using semantic gradient descent.\n *\n * @param {object} graph - The computational graph, including optimizable parameters.  Should implement methods to get parameters, validation set and assign new parameter values.\n * @param {function} lossFunction - The loss function.\n * @param {function} feedbackFunction - The output feedback function.\n * @param {function} parameterUpdateFunction - The parameter update function.\n * @param {number} lossThreshold - The threshold for computing gradients.\n * @param {number} batchSize - The batch size for gradient computation.\n * @param {function} validationFunction - The validation function.\n * @returns {object} The optimized parameters.\n */\nasync function semanticGradientDescent(graph, lossFunction, feedbackFunction, parameterUpdateFunction, lossThreshold, batchSize, validationFunction) {\n  let parameters = graph.getParameters(); // Get the parameters from the graph, assume an object mapping parameter IDs to string values.\n\n  while (!/* termination condition */) {\n    const gradients = {};\n    let batchCount = 0;\n\n    while (Object.keys(gradients).length < batchSize) { // Fill the batch\n      const query = graph.sampleQuery();  // Assume that the graph object handles query sampling.\n      const answer = await graph.execute(query, parameters); // Use graph to execute and compute answer\n      const loss = lossFunction(query, answer);\n\n      if (loss > lossThreshold) {\n        const feedback = feedbackFunction(query, answer);\n        const currentGradients = semanticBackpropagation(graph, /* ... (provide other arguments here as necessary) */ ,feedback);\n        // Accumulate gradients over the batch\n        for (const paramID in currentGradients) {\n          if (!gradients[paramID]) {\n             gradients[paramID] = [];\n          }\n           gradients[paramID].push(currentGradients[paramID]);\n        }\n         batchCount++;\n      }\n    }\n\n    const newParameters = {};\n    for (const paramID in parameters) {\n      newParameters[paramID] = await parameterUpdateFunction(parameters[paramID], gradients[paramID] || [] );\n    }\n    // Apply update gate using the validation set; assume validation function works on parameter objects.\n\n    if (validationFunction(newParameters) < validationFunction(parameters)) {\n      parameters = newParameters;\n      graph.setParameters(parameters); // Update graph with new parameters.\n    }\n  }\n\n  return parameters;\n}\n```\n\n**Explanation:** This algorithm implements semantic gradient descent to optimize the parameters of a computational graph. It iteratively samples queries, computes gradients if the loss exceeds a threshold, updates parameters based on aggregated gradients, and uses a validation gate to accept or reject proposed updates. Again, the `graph` object is assumed to implement several functions to make the algorithm function correctly. For example, it is expected to handle sampling queries with `sampleQuery`, executing queries with `execute`, retrieving the parameters with `getParameters` and assigning new values to parameters with `setParameters`.  These functions will likely depend on the underlying application and are best designed and implemented with the specific details of that application in mind.",
  "simpleQuestion": "How can I improve LLM agent feedback?",
  "timestamp": "2024-12-06T06:05:06.125Z"
}