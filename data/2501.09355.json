{
  "arxivId": "2501.09355",
  "title": "YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents in Augmented Reality Tasks",
  "abstract": "Multimodal AI Agents are AI models that have the capability of interactively and cooperatively assisting human users to solve day-to-day tasks. Augmented Reality (AR) head-worn devices can uniquely improve the user experience of solving procedural day-to-day tasks by providing egocentric multimodal (audio and video) observational capabilities to AI Agents. Such AR capabilities can help the AI Agents see and listen to actions that users take which can relate to multimodal capabilities of human users. Existing AI Agents, either Large Language Models (LLMs) or Multimodal Vision-Language Models (VLMs) are reactive in nature, which means that models cannot take an action without reading or listening to the human user's prompts. Proactivity of AI Agents on the other hand can help the human user detect and correct any mistakes in agent observed tasks, encourage users when they do tasks correctly or simply engage in conversation with the user - akin to a human teaching or assisting a user. Our proposed YET to Intervene (YETI) multimodal agent focuses on the research question of identifying circumstances that may require the agent to intervene proactively. This allows the agent to understand when it can intervene in a conversation with human users that can help the user correct mistakes on tasks, like cooking, using Augmented Reality. Our YETI Agent learns scene understanding signals based on interpretable notions of Structural Similarity (SSIM) on consecutive video frames. We also define the alignment signal which the AI Agent can learn to identify if the video frames corresponding to the user's actions on the task are consistent with expected actions. These signals are used by our AI Agent to determine when it should proactively intervene. We compare our results on the instances of proactive intervention in the HoloAssist multimodal benchmark for an expert agent guiding a user to complete procedural tasks.",
  "summary": "This research introduces YETI (YET to Intervene), a framework for creating proactive AI assistants in augmented reality.  Instead of only reacting to user requests, YETI anticipates user needs and intervenes proactively.  It uses lightweight signals like changes in object counts and visual similarity between video frames to trigger interventions, making it suitable for resource-constrained AR devices.\n\nKey points for LLM-based multi-agent systems:\n\n* **Proactivity:** YETI shifts the paradigm from reactive to proactive AI, crucial for effective assistance, especially in complex or safety-critical tasks.\n* **Lightweight Computation:**  YETI uses efficient algorithms to process video and detect intervention opportunities, enabling real-time operation on AR devices even with limited computational power.\n* **Multimodal Integration:**  While tested with a lightweight VLM (PaliGemma) for object counting, YETIâ€™s framework is designed to incorporate multiple modalities (e.g., hand pose, eye gaze) to enhance contextual understanding and intervention accuracy.\n* **Open-Source VLM Use:** The use of PaliGemma demonstrates the feasibility of leveraging open-source models for complex multi-agent applications.\n* **AR Focus:** YETI's design explicitly addresses the challenges and opportunities of AR interfaces, including egocentric perspectives and real-time interaction requirements.",
  "takeaways": "This paper introduces YETI, a proactive intervention system for Multimodal AI agents in AR, offering valuable insights for JavaScript developers building LLM-based multi-agent web apps. Here's how you can apply its core concepts:\n\n**1. Building Proactive Agents in Web Apps:**\n\n* **Scenario:** Imagine building a collaborative online code editor. Instead of passively waiting for user input, a YETI-inspired agent could proactively suggest code completions, identify potential bugs, or recommend relevant documentation.\n* **Implementation:**\n    * **Frontend (JavaScript):** Use libraries like `TensorFlow.js` or `WebDNN` to implement lightweight client-side object detection (using a pre-trained model or a simplified version like MobileNet) and SSIM calculations within the browser. This analyzes the user's coding activity (represented visually in the editor as changes in the code structure).\n    * **Backend (Node.js):**  Communicate these scene understanding signals to a backend service running an LLM like `LangChain.js` or a cloud-based LLM API. The LLM, primed with coding knowledge, uses these signals to decide if and when to proactively intervene (e.g., offer a code suggestion). \n    * **Framework Integration:** Integrate with popular frontend frameworks like React, Vue, or Angular to manage UI updates and user interactions seamlessly.\n\n**2. Real-Time Multi-Agent Collaboration:**\n\n* **Scenario:**  A multi-user collaborative whiteboard application where YETI-like agents facilitate interaction. Agents could anticipate user intentions, predict drawing actions, suggest collaborative features, or even prevent conflicting edits.\n* **Implementation:**\n    * **WebSockets (Socket.IO):** Use WebSockets to enable real-time communication between multiple agents and user clients.  Broadcast visual scene changes (strokes, object additions/deletions) from each user to all connected clients.\n    * **Client-Side Scene Analysis:**  Perform lightweight visual feature extraction (e.g., object counts, stroke patterns) and SSIM calculations on the canvas changes using JavaScript canvas APIs and image processing libraries.\n    * **Server-Side Coordination:**  A Node.js backend manages the different agents, using the scene data to determine the best course of action for each agent. This could involve using a rule-based system or a more complex LLM-based decision-making process.\n\n**3. Enhanced User Experiences:**\n\n* **Scenario:** An e-commerce website with AI-powered shopping assistants. Agents could proactively guide users based on their browsing history, offer personalized recommendations, predict needs based on viewed items, or suggest complementary products.\n* **Implementation:**\n    * **Browser-Side Tracking:** Monitor user interactions (clicks, scrolls, product views) using JavaScript event listeners.  Extract visual features from viewed products using image processing libraries.\n    * **Backend Agent Logic:**  Use LLMs to reason about the user's intentions based on the collected data.  Develop rules or train a smaller model to determine when an agent should proactively intervene (e.g., show a pop-up with recommendations, offer a discount code, or highlight related products).\n\n**4.  Experimentation and Prototyping:**\n\n* **JavaScript Libraries:** `LangChain.js`, `TensorFlow.js`, `WebDNN`, `Socket.IO`, various image processing libraries.\n* **Web Technologies:** WebSockets, Canvas API, Web Workers (for offloading computations).\n* **LLM APIs:** OpenAI, Google Cloud Vertex AI, Hugging Face Inference API.\n\nBy leveraging these tools and concepts, JavaScript developers can build intelligent, proactive multi-agent systems capable of enriching user experiences and enhancing the functionality of web applications. The YETI framework, although designed for AR, provides a conceptual foundation for bringing proactive AI to the web.  By adapting the core ideas of scene understanding and intervention logic, developers can create innovative web experiences powered by LLM-based multi-agent systems.",
  "pseudocode": "```javascript\nfunction yetiProactiveInterventionDetection(frames, alignmentScores) {\n  // Initialize output set of intervention frames and alignment scores per episode\n  const interventionFrames = new Set();\n  const episodeAlignmentScores = [];\n\n  // Hyperparameters (adjust as needed)\n  const episodeInterval = 5;  // k\n  const conversationInterval = 1; // m\n  const extremaRange = 1; // r\n\n  let episodeCount = 0;\n  let frameCount = 0;\n\n  for (let i = 0; i < frames.length; i++) {\n    if (episodeCount > 0 && i >= conversationInterval) {\n      frameCount++;\n      episodeAlignmentScores.push(alignmentScores[i]);\n\n      // Check if alignment score is within local extrema range\n      const minScore = Math.min(...episodeAlignmentScores);\n      const maxScore = Math.max(...episodeAlignmentScores);\n\n      if (alignmentScores[i] <= minScore + extremaRange || \n          alignmentScores[i] >= maxScore - extremaRange) {\n        interventionFrames.add(frames[i]); // Add frame to intervention set\n      }\n\n      if (frameCount === episodeInterval) {\n        // Reset for next episode\n        episodeAlignmentScores.length = 0; // Clear array \n        episodeCount++;\n        frameCount = 0;\n        i += conversationInterval; // Enforce minimum gap between interventions\n      }\n    } else if (episodeCount === 0) {\n      frameCount++;\n      episodeAlignmentScores.push(alignmentScores[i]);\n\n      if (frameCount === episodeInterval) {\n        const minScore = Math.min(...episodeAlignmentScores);\n        const maxScore = Math.max(...episodeAlignmentScores);\n\n        if (alignmentScores[i] <= minScore + extremaRange ||\n            alignmentScores[i] >= maxScore - extremaRange) {\n\n          interventionFrames.add(frames[i]); // Add frame for intervention\n        }\n        // Reset for next episode\n        episodeAlignmentScores.length = 0; // Clear the array contents\n        episodeCount++;\n        frameCount = 0;\n        i += conversationInterval; //  Enforce minimum gap\n      }\n    }\n  }\n  return Array.from(interventionFrames); // Convert set to array for output\n}\n\n\n// Example Usage (replace with actual frame and alignment score data)\nconst frames = [/*...frame data...*/];  // Array of frame objects or identifiers\nconst alignmentScores = [/*...alignment scores...*/]; // Array of numeric scores\nconst interventions = yetiProactiveInterventionDetection(frames, alignmentScores);\nconsole.log(interventions);\n\n\n```\n\n**Explanation of the YETI Proactive Intervention Detection Algorithm:**\n\nThis algorithm aims to identify suitable moments for a multimodal AI agent to proactively intervene in a user's task within an augmented reality environment.  It operates on a sequence of frames from a video, using alignment scores that reflect changes in the scene (e.g., change in object count).\n\n**Purpose:** The core idea is to detect when a scene changes significantly, indicating a potential point where the user may need assistance or correction. It avoids intervening too frequently by enforcing minimum intervals between interventions.\n\n**Key Parameters:**\n\n* `episodeInterval (k)`:  The number of consecutive frames that form an \"episode.\" Only one intervention is allowed per episode.\n* `conversationInterval (m)`: The minimum number of frames that must pass between consecutive interventions.  This prevents the AI from interrupting the user too often.\n* `extremaRange (r)`: This parameter controls the sensitivity to changes in alignment scores.  A larger range means the algorithm is more sensitive and will trigger interventions for smaller changes.\n\n**Logic:**\n\n1. **Initialization:** Sets up data structures and hyperparameters.\n2. **Frame Processing Loop:** Iterates through each frame in the video.\n3. **Episode Tracking:** Groups frames into episodes and calculates local extrema (minimum and maximum) of the alignment scores within each episode.\n4. **Intervention Trigger:** If the current frame's alignment score is close to a local extremum (within the `extremaRange`), and sufficient time has passed since the last intervention, the current frame is marked for intervention.\n5. **Episode Reset:**  After each episode, the counters are reset, and the `conversationInterval` is enforced to prevent excessive interventions.\n\n**JavaScript Implementation Notes:**\n\n* The code uses a `Set` to store intervention frames to ensure uniqueness.\n* The `Array.from()` method converts the `Set` back to an array for easier handling of the results.\n* The example usage section is a placeholder; you need to replace the example `frames` and `alignmentScores` data with your actual data.  The frames could be represented by image objects, timestamps, or any other suitable identifier.  The `alignmentScores` should be numeric values.\n\n\nThis JavaScript implementation provides a clear and concise way to realize the YETI algorithm for proactive intervention detection in augmented reality applications. It allows developers to experiment with different hyperparameters to fine-tune the system's behavior based on specific task requirements and user interaction patterns.",
  "simpleQuestion": "How can an AR agent proactively help users with tasks?",
  "timestamp": "2025-01-17T06:01:55.924Z"
}