{
  "arxivId": "2502.08056",
  "title": "Cognify: Supercharging Gen-AI Workflows With Hierarchical Autotuning",
  "abstract": "Today's gen-AI workflows that involve multiple ML model calls, tool/API calls, data retrieval, or generic code execution are often tuned manually in an ad-hoc way that is both time-consuming and error-prone. In this paper, we propose a systematic approach for automatically tuning gen-AI workflows. Our key insight is that gen-AI workflows can benefit from structure, operator, and prompt changes, but unique properties of gen-AI workflows require new optimization techniques. We propose AdaSeek, an adaptive hierarchical search algorithm for autotuning gen-AI workflows. AdaSeek organizes workflow tuning methods into different layers based on the user-specified total search budget and distributes the budget across different layers based on the complexity of each layer. During its hierarchical search, AdaSeek redistributes the search budget from less useful to more promising tuning configurations based on workflow-level evaluation results. We implement AdaSeek in a workflow autotuning framework called Cognify and evaluate Cognify using six types of workflows such as RAG-based QA and text-to-SQL transformation. Overall, Cognify improves these workflows' generation quality by up to 2.8×, reduces execution monetary cost by up to 10x, and reduces end-to-end latency by 2.7x.",
  "summary": "This paper introduces Cognify, a framework for automatically optimizing complex workflows involving multiple Generative AI models (Gen-AI workflows), tools, and data retrieval.  Manual tuning of such workflows is time-consuming and inefficient.  Cognify uses a novel hierarchical search algorithm, AdaSeek, to efficiently explore the space of possible workflow improvements, considering changes to workflow structure, individual steps (like model selection or code changes), and edge weights (like prompt engineering).  AdaSeek adapts its search based on the available budget and observed results, making it particularly relevant to LLM-based multi-agent systems where evaluation can be costly.  Key points relevant to LLM-based multi-agent systems include the use of LLMs for proposing and evaluating workflow changes, the focus on non-differentiable components and discrete metrics common in these systems, and the ability to optimize under budget constraints.",
  "takeaways": "This paper introduces Cognify, a framework for optimizing LLM-based workflows, and its underlying algorithm AdaSeek. Here's how a JavaScript developer can apply these insights to multi-agent AI projects for web development:\n\n**1. Hierarchical Optimization with Limited Budget:**\n\n* **Scenario:** A multi-agent chatbot for e-commerce, where agents handle product discovery, order placement, and customer support.  The budget for LLM calls is limited.\n* **Application:** Instead of tuning all agent prompts and interaction logic simultaneously, adopt a hierarchical approach.  Prioritize optimizing the product discovery agent first (most crucial for user engagement), then order placement, and finally customer support.  This allows allocating the limited budget to the most impactful areas.  Use a JavaScript library like `hyperopt.js` (a port of Hyperopt, which includes TPE) for Bayesian Optimization within each layer.\n\n```javascript\n// Example using hyperopt.js (simplified)\nconst hyperopt = require('hyperopt.js');\n\nasync function optimizeAgent(agent, budget) {\n  const space = { // Define search space for agent parameters\n    promptTemplate: hyperopt.choice(['template1', 'template2']),\n    // ... other parameters\n  };\n\n  const trials = new hyperopt.Trials();\n  const best = await hyperopt.fmin(\n    async (params) => await evaluateAgent(agent, params), // Evaluation function\n    space,\n    hyperopt.tpe.suggest, // Use TPE\n    budget,\n    trials\n  );\n  return best;\n}\n\n// Optimize agents hierarchically\nasync function optimizeWorkflow() {\n  const productAgentParams = await optimizeAgent(productAgent, 10);\n  const orderAgentParams = await optimizeAgent(orderAgent, 5);\n  // ...\n}\n```\n\n**2. Adaptive Search and Budget Redistribution:**\n\n* **Scenario:** Building a multi-agent system for content creation, where agents generate text, images, and optimize for SEO.  Initial experiments show that image generation parameters are less impactful than text generation.\n* **Application:**  Implement a system that tracks the effectiveness of different tuning configurations.  If tuning the image generation agent yields minimal improvement, redistribute the remaining budget to further optimize the text generation agent or explore architecture changes (e.g., adding a fact-checking agent).  Monitor evaluation results during the search process and dynamically adjust the budget allocation in JavaScript.\n\n**3. Exploring Different Workflow Structures:**\n\n* **Scenario:** Developing a collaborative code editor with agents for code completion, debugging, and testing.\n* **Application:** Experiment with different workflow structures.  Start with a simple sequential workflow, then explore parallel execution of agents (e.g., code completion and debugging running concurrently). Use A/B testing within the web app to compare the effectiveness of different structures and dynamically adjust the workflow based on user feedback. Cognify's task decomposition and ensembling ideas can be implemented as functions within JavaScript that restructure the agent interaction graph.\n\n**4. Prompt Engineering as \"Weight\" Tuning:**\n\n* **Scenario:** Creating a multi-agent news summarization app.\n* **Application:** Focus on optimizing agent prompts as the primary \"weight\" tuning mechanism.  Experiment with different prompt templates, few-shot examples, and instructions (e.g., \"Summarize this article in 5 bullet points\").  Use a JavaScript library like `LangChain.js` to manage and manipulate prompts dynamically.\n\n\n**5. Combining with Existing Frameworks:**\n\n* **Scenario:**  Any web-based multi-agent application using LangChain.js or similar.\n* **Application:** Integrate the principles of AdaSeek and Cognify into your LangChain workflow. You'll still define your agents and chains, but you'll overlay the optimization layer on top, using a library like `hyperopt.js` for the Bayesian Optimization and a custom implementation of AdaSeek's hierarchical and adaptive logic in JavaScript.\n\n\nBy applying these principles, JavaScript developers can build more efficient and effective LLM-based multi-agent systems for various web development scenarios, even with limited resources.  The key is to move away from ad-hoc tuning and towards a more systematic and data-driven optimization process.",
  "pseudocode": "The paper contains two algorithms described in pseudocode: AdaSeek (Algorithm 1) and LayerSearch (Algorithm 2). Here are their JavaScript implementations with explanations:\n\n```javascript\n// Algorithm 1: AdaSeek\nasync function adaSeek(totalBudget, cogs, evaluator) {\n  let results = []; // Global result set\n  let usedBudget = 0;\n\n  // Perform search with 1 to 3 layers\n  for (let numLayers = 1; numLayers <= 3; numLayers++) {\n    let layeredCogs = JSON.parse(JSON.stringify(cogs)); // Deep copy to avoid modification\n\n    if (numLayers === 1) {\n      layeredCogs = [layeredCogs.flat()]; // Merge all cogs into a single layer\n    } else if (numLayers === 2) {\n      layeredCogs = [layeredCogs[0].concat(layeredCogs[1]), layeredCogs[2]]; // Merge step & weight, keep architecture separate\n    }\n\n    let expectedSearchSize = layeredCogs.map(layer => Math.pow(layer.length, 1.1));\n    let totalExpectedSize = expectedSearchSize.reduce((a, b) => a * b, 1);\n    let budgetForRound = Math.min(totalBudget - usedBudget, totalExpectedSize);\n\n    if (numLayers === 3 && totalBudget - usedBudget > totalExpectedSize) {\n      budgetForRound = totalBudget - usedBudget; // Use all remaining budget for 3 layers\n    }\n\n    let layerBudgets = expectedSearchSize.map(size => (size / totalExpectedSize) * budgetForRound);\n\n    let layerResults = await layerSearch([], budgetForRound, numLayers, layerBudgets, layeredCogs, evaluator);\n    results = results.concat(layerResults);\n    usedBudget += budgetForRound;\n\n    if (usedBudget >= totalBudget) {\n      break; // Stop when budget is exhausted\n    }\n  }\n\n  return selectBestConfigs(results); // Return best configurations based on evaluation results\n}\n\n// Helper function (not defined in the paper, but necessary for implementation)\nfunction selectBestConfigs(results) {\n  // Implementation depends on the specific evaluation metric and desired optimization criteria.\n  // Example: Return the configuration with the highest score\n  return results.reduce((best, current) => (current.score > best.score ? current : best), { score: -Infinity });\n}\n\n\n// Algorithm 2: LayerSearch (recursive)\nasync function layerSearch(chosenConfigs, budget, currentLayer, layerBudgets, layeredCogs, evaluator) {\n  let feedback = [];\n  const W = 10; // Chunk size (not specified in the paper, requires tuning)\n  const R = 1;  // Initial budget for successive halving (not specified, requires tuning)\n  const η = 2;  // successive halving rate (not specified, requires tuning)\n\n  if (currentLayer === 1) { // Innermost layer\n    for (let i = 0; i < budget; i++) {\n\n      let config = sampleConfig(layeredCogs[currentLayer - 1]); // sample innermost layer config (paper uses TPE, we simplify it here for demonstration)\n      let allConfigs = chosenConfigs.concat(config);\n      let evaluation = await evaluator(allConfigs); // Asynchronously evaluate the workflow\n\n      if (earlyStop(evaluation)) {\n        break;\n      }\n\n      feedback.push(evaluation);\n    }\n\n    return feedback;\n  } else { // Non-innermost layers\n    let usedBudget = 0;\n    let layerFeedback = [];\n\n    while (usedBudget < budget) {\n      let numConfigs = Math.min(W, budget - usedBudget);\n      usedBudget += numConfigs;\n\n\n      let configs = sampleConfigs(layeredCogs[currentLayer-1], numConfigs);\n\n      let chunkFeedback = []\n      for (let s = 0 ; s < numConfigs; s++) {\n        let rs = R * (η**s); // Budget for lower layers\n\n        for(const config of configs){\n\n          let innerFeedback = await layerSearch(chosenConfigs.concat(config), budget, currentLayer - 1, layerBudgets, layeredCogs, evaluator);\n          chunkFeedback = chunkFeedback.concat(innerFeedback);\n\n\n\n        }\n        configs = selectTopConfigs(configs, chunkFeedback, Math.floor(configs.length / η)); //Keep best half for next round of SH.\n\n      }\n\n      layerFeedback = layerFeedback.concat(chunkFeedback);\n\n\n      if (earlyStop(layerFeedback)) {\n        break;\n      }\n\n    }\n    return layerFeedback;\n  }\n}\n\n\n\n// Helper function implementations.\nfunction sampleConfig(cogLayer){\n  // Implement a sampling strategy for a single configuration here.\n  // Example return a random selection from the options in cogs\n  return [cogLayer[Math.floor(Math.random() * cogLayer.length)]]; // Sample one cog randomly\n\n}\n\nfunction sampleConfigs(cogLayer, numConfigs){\n  // Implement a sampling strategy here.\n  // Example: return random selection from cog layer\n  return Array.from({ length: numConfigs }, () => sampleConfig(cogLayer));\n\n}\n\nfunction selectTopConfigs(configs, feedback, k) {\n  // Implement logic to select top k configs based on feedback\n  //  Implementation will depend on how you define k (paper suggests using successive halving)\n  // Example return the k configs with the highest score\n  return configs.slice(0,k);\n\n}\n\n\nfunction earlyStop(feedback) {\n  // Implement early stopping logic here.\n  // Paper suggests stopping if the latest few searches don't improve significantly\n  return false; // Placeholder - replace with actual logic\n}\n\n\n\n\n```\n\n**Algorithm 1: AdaSeek**\n\n* **Purpose:** To efficiently explore the search space of workflow configurations, considering architecture, step, and weight changes (cogs), with a limited budget.\n* **Explanation:**  AdaSeek uses an adaptive hierarchical search. It starts with a single layer search, then gradually increases the number of layers (up to 3) if the budget allows.  The budget is distributed across layers proportionally to their estimated complexity. The search within each layer utilizes the LayerSearch algorithm.  AdaSeek combines the results from all layers and selects the best-performing workflow configurations.\n* **Key Improvements:**  The adaptive layering and budget allocation allow it to balance exploration and exploitation efficiently, especially under limited budgets, which is a common constraint in real-world scenarios.\n\n**Algorithm 2: LayerSearch (Recursive)**\n\n* **Purpose:**  To explore the configurations within a specific layer of the AdaSeek hierarchy.\n* **Explanation:**  This function recursively explores the cogs layer by layer. For the innermost layer, it samples configurations and evaluates the workflow. For other layers, it uses a chunk-based successive halving approach combined with TPE (Tree-structured Parzen Estimator) sampling to focus on more promising areas of the search space. It returns the feedback (evaluation results) to the upper layer.\n* **Key Improvements:**  The successive halving approach and chunking helps to utilize the budget effectively by allocating more resources to more promising configurations.  The TPE helps to handle different types of cogs (continuous and discrete) within the Bayesian Optimization framework.\n\n\n\nThese JavaScript adaptations provide a starting point for implementing these algorithms. Remember that some parts (like `selectBestConfigs`, `sampleConfigs`, `selectTopConfigs`, and `earlyStop`) are simplified for demonstration and would require more sophisticated logic based on the specific application and evaluation metric in a real-world setting. Also, integrating with a Bayesian Optimization library for TPE sampling would be necessary. The paper uses Python and their open-sourced Cognify utilizes the `hyperopt` library.  Similar JavaScript libraries for Bayesian Optimization can be used.",
  "simpleQuestion": "How can I automate complex LLM workflow tuning?",
  "timestamp": "2025-02-13T06:03:07.198Z"
}