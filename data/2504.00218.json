{
  "arxivId": "2504.00218",
  "title": "Agents Under Siege: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks",
  "abstract": "Most discussions about Large Language Model (LLM) safety have focused on single-agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking pragmatic systems that have constrains such as limited token bandwidth, latency between message delivery, and defense mechanisms. We design a permutation-invariant adversarial attack that optimizes prompt distribution across latency and bandwidth-constraint network topologies to bypass distributed safety mechanisms within the system. Formulating the attack path as a problem of maximum-flow minimum-cost, coupled with the novel Permutation-Invariant Evasion Loss (PIEL), we leverage graph-based optimization to maximize attack success rate while minimizing detection risk. Evaluating across models including Llama, Mistral, Gemma, DeepSeek and other variants on various datasets like JailBreakBench and AdversarialBench, our method outperforms conventional attacks by up to 7x, exposing critical vulnerabilities in multi-agent systems. Moreover, we demonstrate that existing defenses, including variants of Llama-Guard and PromptGuard, fail to prohibit our attack, emphasizing the urgent need for multi-agent specific safety mechanisms.",
  "summary": "This paper explores vulnerabilities in multi-agent LLM systems, specifically how carefully crafted prompts (\"attacks\") can be spread through a network of LLMs to bypass safety measures and cause an LLM to generate harmful content (a \"jailbreak\").  It introduces a method to optimize these attacks, considering network limitations like bandwidth and latency, and making them resistant to the order the prompt pieces arrive. Experiments show this method is significantly more effective than simpler attacks, highlighting a critical weakness in current multi-agent LLM safety mechanisms.  The research primarily focuses on text-based agents and assumes partial knowledge of the network structure.",
  "takeaways": "This research paper presents valuable insights for JavaScript developers working with LLM-based multi-agent systems, particularly in web development. Here are practical examples of how these insights can be applied:\n\n**1. Secure Message Passing with LangChain and Socket.IO:**\n\nImagine building a collaborative writing application where multiple LLM agents work together, powered by LangChain for LLM interaction and Socket.IO for real-time communication. This paper highlights the risk of adversarial prompt injection via inter-agent messaging. To mitigate this, you can implement the following:\n\n```javascript\n// Server-side (Node.js with Socket.IO & LangChain)\nio.on('connection', (socket) => {\n  socket.on('agentMessage', (msg) => {\n    // Implement a \"safety mechanism\" edge before broadcasting\n    const isSafe = safetyFilter(msg.prompt); // Use PromptGuard or similar\n    if (isSafe) {\n      // Use LangChain to process the prompt and distribute to other agents\n      const chain = new LLMChain({ llm: llm, prompt: promptTemplate });\n      const res = await chain.call({ input: msg.prompt });\n      socket.broadcast.emit('agentMessage', { ...msg, response: res });\n    } else {\n      console.warn(\"Potentially harmful prompt detected:\", msg.prompt);\n      socket.emit('error', 'Harmful prompt detected');\n    }\n  });\n});\n\n\n// Client-side (JavaScript in browser)\nsocket.emit('agentMessage', { prompt: 'Write a story about...', agentId: 1});\n\nsocket.on('agentMessage', (msg) => {\n  // Update UI with the LLM agent's response\n});\n```\n\nThis example shows a basic safety filter implementation before message broadcasting. You can further improve this by adapting the paperâ€™s proposed optimization strategies, like breaking down larger prompts into chunks and routing them through \"safer\" agent paths.  You can use graph libraries like `vis-network` or `sigma.js` on the server-side to model your agent network and implement optimized routing based on bandwidth and security profiles.\n\n\n**2. Asynchronous Message Handling and Permutation Invariance with Redux-Saga:**\n\nIn a complex multi-agent web application, message arrival order isn't guaranteed. Redux-Saga, a JavaScript library for managing side effects in Redux applications, can be leveraged to handle the asynchronous nature and ensure permutation invariance, as suggested in the paper.\n\n```javascript\n// Redux-Saga\nfunction* handleAgentMessages() {\n  const messages = yield takeEvery('AGENT_MESSAGE', processMessage);\n\n  // Reconstruct the final prompt regardless of chunk arrival order\n  const fullPrompt = reconstructPrompt(messages); // Implements logic from paper\n\n  // Call LLM with the reconstructed prompt\n  const llmResponse = yield call(llm, fullPrompt);\n\n  yield put({ type: 'LLM_RESPONSE', payload: llmResponse });\n}\n\n\nfunction reconstructPrompt(messages) {\n  // Sort or assemble messages based on identifiers or sequence numbers\n  // Implement the \"concatenation\" logic from the paper's PIEL section\n  // ...\n}\n```\n\nThis code snippet demonstrates how Redux-Saga can intercept incoming agent messages, store them, and reconstruct the complete prompt before submitting it to the LLM, mitigating the risks associated with asynchronous message arrival.\n\n\n**3. Distributed Safety Mechanisms with Web Workers:**\n\nImplementing safety mechanisms on every edge can be computationally intensive. To address this, leverage Web Workers to offload safety checks to separate threads, preventing blocking the main thread and improving UI responsiveness.\n\n```javascript\n// Main thread\nconst worker = new Worker('safetyWorker.js');\n\nworker.postMessage({ prompt: '...' });\n\nworker.onmessage = (event) => {\n  if (event.data.isSafe) {\n    // Proceed with prompt\n  } else {\n    // Handle unsafe prompt\n  }\n};\n\n\n// safetyWorker.js (Web Worker)\nonmessage = (event) => {\n  const isSafe = safetyFilter(event.data.prompt); // Run safety checks\n  postMessage({ isSafe });\n};\n```\n\n\n\nBy implementing these techniques, JavaScript developers can build more secure and robust multi-agent LLM applications for the web. Remember that this field is rapidly evolving; continuous exploration and adaptation of latest research findings are crucial.",
  "pseudocode": "```javascript\nfunction permutationInvariantEvasionOptimization(targetModelType, initialChunks, iterations) {\n  // Initialize chunks with random token sequences.  In a real application, this\n  // would involve more sophisticated prompt engineering techniques.\n  let optimizedChunks = initialChunks.map(chunk => ({\n    tokens: Array.from({ length: chunk.length }, () => Math.floor(Math.random() * targetModelType.vocabSize))\n  }));\n\n\n  for (let t = 0; t < iterations; t++) {\n    let totalLoss = 0;\n\n    // Iterate through all possible chunk permutations (simplified for demonstration).\n    // In reality, you'd likely sample permutations as K! can be large.\n    const permutations = permute(optimizedChunks);\n\n\n    for (const permutation of permutations) {\n      const concatenatedPrompt = permutation.flatMap(chunk => chunk.tokens);\n\n      // Calculate the negative log-likelihood of generating the target harmful output\n      // given the current prompt (using the language model).\n      const loss = -Math.log(targetModelType.probability(concatenatedPrompt, harmfulOutput));\n      totalLoss += loss;\n\n    }\n\n\n    totalLoss /= permutations.length;\n\n\n    // Update chunks using Greedy Coordinate Gradient. This involves calculating the\n    // gradient of the loss with respect to each token and making replacements\n    // to maximize the likelihood of generating the harmful output.\n    optimizedChunks = optimizedChunks.map(chunk =>\n      greedyCoordinateGradient(chunk, totalLoss, targetModelType)\n    );\n  }\n\n  return optimizedChunks;\n}\n\n\n\n// Helper function to generate all permutations of an array (simplified).\n// In practice, you'd likely want a more efficient permutation sampling method.\nfunction permute(arr) {\n  if (arr.length <= 1) return [arr];\n\n  const permutations = [];\n  for (let i = 0; i < arr.length; i++) {\n    const rest = [...arr.slice(0, i), ...arr.slice(i + 1)];\n    const restPermutations = permute(rest);\n    for (const rp of restPermutations) {\n      permutations.push([arr[i], ...rp]);\n    }\n  }\n\n  return permutations;\n\n}\n\n\n\n\n// Helper function to implement GCG update (simplified).\nfunction greedyCoordinateGradient(chunk, loss, model) {\n\n  // In a real application, you'd compute gradients here.\n  // This simplified version simply returns the original chunk\n\n  return chunk;\n}\n\n\n\n// Example usage:\n// Assume targetModelType has methods like probability and vocabSize\n// Also assume harmfulOutput is defined elsewhere\n\nconst initialChunks = [{ length: 5 }, { length: 3 }, { length: 7 }]; // Example chunk lengths\nconst iterations = 10;\n\nconst optimizedAdversarialChunks = permutationInvariantEvasionOptimization(\n  targetModelType,\n  initialChunks,\n  iterations\n);\n\nconsole.log(optimizedAdversarialChunks);\n\n```\n\n\n**Explanation of Algorithm 1: Permutation-Invariant Evasion Optimization**\n\n**Purpose:** This algorithm aims to generate adversarial prompts for multi-agent LLM systems that remain effective regardless of the order in which prompt chunks arrive at the target agent. It does so by optimizing the prompt chunks using a permutation-invariant loss function, considering all possible orderings of these chunks.\n\n**Key Steps:**\n\n1. **Initialization:** Start with randomly initialized token sequences for each chunk.\n2. **Iterative Optimization:** Repeat the following steps for a predefined number of iterations:\n    * **Permutation Loop:** Iterate through all possible orderings (or sample a subset) of the chunks. For each ordering:\n        * **Concatenate:** Concatenate the chunks according to the current ordering.\n        * **Calculate Loss:** Compute the negative log-likelihood (NLL) of the target harmful output given the concatenated prompt using the target LLM. This measures how likely the LLM is to generate the harmful output.\n        * **Accumulate Loss:** Add the NLL to the total loss.\n    * **Average Loss:** Divide the total loss by the number of permutations to get the average loss for this iteration.\n    * **Greedy Coordinate Gradient (GCG):** Update each chunk's token sequence using GCG, aiming to minimize the average loss. GCG iteratively refines token choices by calculating gradients and making replacements to increase the likelihood of generating the harmful output.\n\n**Key Improvements and Simplifications in the JavaScript Code:**\n\n* **Permutation Generation:** The `permute` function is a simplified example and, in a real-world scenario, you would likely use a more efficient method, potentially one that samples permutations rather than generating all of them.\n* **GCG Implementation:**  The `greedyCoordinateGradient` function is a placeholder. A real implementation would involve calculating gradients of the loss with respect to each token in the chunk and using this information to make token replacements.\n* **Language Model Integration:** The code assumes the existence of a `targetModelType` object with methods like `probability` (to calculate the probability of generating a sequence) and `vocabSize`.  You would need to integrate your actual LLM and its API here.\n* **Harmful Output:** The code assumes `harmfulOutput` is defined.  This would be the specific sequence of tokens that represents the harmful output you are trying to elicit from the LLM.\n\n\n\nThis improved version provides a clearer and more functional structure, enabling JavaScript developers to grasp the core concepts and experiment with the algorithm.  Remember to replace the placeholder functions and integrate your LLM to get a fully working implementation.",
  "simpleQuestion": "Can prompt attacks break multi-agent LLMs?",
  "timestamp": "2025-04-02T05:07:20.993Z"
}