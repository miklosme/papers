{
  "arxivId": "2503.15547",
  "title": "Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents",
  "abstract": "Large Language Models (LLMs) are combined with plugins to create powerful LLM agents that provide a wide range of services. Unlike traditional software, LLM agent's behavior is determined at runtime by natural language prompts from either user or plugin's data. This flexibility enables a new computing paradigm with unlimited capabilities and programmability, but also introduces new security risks, vulnerable to privilege escalation attacks. Moreover, user prompt is prone to be interpreted in an insecure way by LLM agents, creating non-deterministic behaviors that can be exploited by attackers. To address these security risks, we propose Prompt Flow Integrity (PFI), a system security-oriented solution to prevent privilege escalation in LLM agents. Analyzing the architectural characteristics of LLM agents, PFI features three mitigation techniques-i.e., untrusted data identification, enforcing least privilege on LLM agents, and validating unsafe data flows. Our evaluation result shows that PFI effectively mitigates privilege escalation attacks while successfully preserving the utility of LLM agents.",
  "summary": "This paper introduces Prompt Flow Integrity (PFI), a security framework for LLM agents designed to prevent privilege escalation attacks.  PFI identifies and isolates untrusted data, enforces least privilege by using separate agents for trusted and untrusted data, and validates data flow to prevent unintended actions.  Key points for LLM-based multi-agent systems include: untrusted data from plugins poses a significant risk; current LLM agents often lack the principle of least privilege, making them vulnerable; data flow validation is crucial but challenging due to the probabilistic nature of LLMs; and PFI's approach of isolating untrusted data and validating data flow offers a more deterministic security guarantee.",
  "takeaways": "This paper introduces Prompt Flow Integrity (PFI), a security framework for LLM agents, focusing on preventing privilege escalation.  Let's translate these concepts into practical JavaScript examples for web developers building LLM-based multi-agent applications.\n\n**1. Untrusted Data Identification:**\n\n* **Concept:** PFI identifies untrusted data based on its source (e.g., user input, third-party API, web scraping).  In a JavaScript context, this means categorizing data received from various sources.\n* **JavaScript Example:**\n\n```javascript\n// Example using LangchainJS and a hypothetical plugin\nconst { LLMChain } = require(\"langchain\");\n\n// ... other Langchain setup ...\n\nconst executePlugin = async (pluginName, input) => {\n  const result = await plugins[pluginName].execute(input); // Execute plugin\n  result.source = pluginName; // Add source information\n  return result;\n};\n\n\nconst processData = async (data) => {\n  if (data.source === 'UserInput') {\n      // Trusted data - proceed with LLMChain directly\n      const response = await llmChain.call({ input: data.content });\n  } else if (data.source === 'ThirdPartyAPI') {\n      // Untrusted Data - handle with caution (e.g., sanitize, validate)\n      const sanitizedData = sanitizeInput(data.content);\n      // ... proceed with untrusted agent or verification ...\n  }\n  // ...\n};\n\n// ...\n\n\n```\n\n**2. Enforcing Least Privilege:**\n\n* **Concept:** PFI isolates agents into trusted and untrusted components.  This can be mimicked in JavaScript by creating separate LLM chains or agent instances with different access levels.\n* **JavaScript Example:**\n\n```javascript\n// Example using hypothetical trusted and untrusted agents\nconst trustedAgent = new LLMChain({ /*... access to all plugins ...*/ });\nconst untrustedAgent = new LLMChain({ /*... limited access to plugins ...*/ });\n\n\nconst processData = async (data) => {\n    if (isTrusted(data)) {\n        return trustedAgent.call({ /* ... */ });\n    } else {\n        // Route untrusted data through untrusted agent with restricted actions\n        const queryResult = await untrustedAgent.call({ /* ... data, restricted query */});\n        // Sanitize or validate untrusted agent's output before using it in trusted context\n        const safeResult = sanitizeOutput(queryResult);\n        return trustedAgent.call({ /* ... use safeResult */ });\n    }\n};\n```\n\n**3. Data Flow Validation:**\n\n* **Concept:** PFI tracks and validates data flow to prevent unintended use of untrusted data.  In JavaScript, this involves logging and inspecting the flow of data through your application, potentially with custom middleware.\n* **JavaScript Example:**\n\n```javascript\n// Example using a simplified logging approach\nconst dataFlowLog = [];\n\nconst logDataFlow = (data, source, destination) => {\n  dataFlowLog.push({ data, source, destination, timestamp: Date.now() });\n};\n\n\nconst processData = async (data) => {\n\n    logDataFlow(data, data.source, 'TrustedAgent');\n\n    // ... processing ...\n\n    if (!isSafeDataFlow(dataFlowLog)) {  // Implement isSafeDataFlow to check your security policy\n        console.warn(\"Potential unsafe data flow detected!\");\n        // ... alert user or take other actions ...\n    }\n\n    // ... \n};\n```\n\n**JavaScript Frameworks and Libraries:**\n\n* **LangchainJS:**  A powerful framework for building LLM-powered applications. Use it to create chains, manage prompts, and interact with various LLMs.\n* **LlamaIndex:** Another framework for building LLM apps, helpful for data augmentation and querying.\n* **Custom Middleware/Logging:** Implement middleware within your Node.js/Express.js (or other backend framework) app to track and validate data flow as it moves through the application.  Libraries like `winston` or `pino` can be useful for logging.\n\n\n**Web Development Scenarios:**\n\n* **Chatbots:** Implement PFI principles in your chatbot to securely handle user input, especially when integrated with external services or APIs.\n* **Content Creation Tools:** Ensure that user-generated content or content fetched from external sources is handled safely, especially if it's used to generate other content or interact with privileged functions.\n* **Automated Assistants:**  When automating tasks with LLM agents, prioritize security to prevent malicious instructions or unintended actions.\n\n\nBy implementing these PFI-inspired strategies, JavaScript developers can build more robust and secure LLM-based multi-agent applications. Remember that security is an ongoing process, and these examples are just a starting point.  Continuously monitor and adapt your security measures as the LLM landscape evolves.",
  "pseudocode": "```javascript\n// TrustCheck function (Section 4.1)\nfunction TrustCheck(data, Pdata) {\n  const dsrc = GetDataAttribute(data); // Get data source attribute\n\n  // Check trustworthiness based on Pdata\n  // Assumes Pdata is a nested structure (e.g., object)\n  // allowing for wildcard matching (*). \n  // More sophisticated implementations may be needed.\n\n  let isTrusted = false;\n  const dsrcParts = dsrc.split(\":\"); // Handle composite dsrc\n\n  function checkAttribute(attribute, policy) {\n    for (const key in policy) {\n      if (key === \"*\" || key.startsWith(attribute + \":\") || (key.includes(\"*\") && minimatch(attribute, key))) {\n        if (typeof policy[key] === 'boolean') {\n          return policy[key]; // Return boolean value\n        } else {\n          return checkAttribute(dsrcParts.shift(), policy[key]); // Recursively check nested attributes\n        }\n      }\n    }\n    return false; // Untrusted by default if not found\n  }\n\n  isTrusted = checkAttribute(dsrcParts.shift(), Pdata);\n\n  return isTrusted ? \"Trusted\" : \"Untrusted\";\n}\n\n\n// Example usage (mimics an apparmor-like Pdata policy)\nconst PdataExample = {\n  \"email\": {\n    \"user@example.com\": true, // Trusted\n    \"*@example.com\": true, // Trusted\n    \"*\": false // Default untrusted\n  },\n  \"cloud\": {\n    \"private\": true, // Trusted\n    \"*\": false // Default untrusted\n  },\n  \"plugin\": {\n    \"*\": true // All plugin messages trusted\n  }\n};\n\nconst data1 = { content: \"...\", dsrc: \"email:user@example.com\" };\nconst data2 = { content: \"...\", dsrc: \"cloud:public\" };\nconst data3 = { content: \"...\", dsrc: \"plugin:WebSearch\" };\n\nconsole.log(TrustCheck(data1, PdataExample)); // Output: Trusted\nconsole.log(TrustCheck(data2, PdataExample)); // Output: Untrusted\nconsole.log(TrustCheck(data3, PdataExample)); // Output: Trusted\n\n\n// GetDataAttribute is a placeholder function \n// and its implementation depends on how dsrc \n// is associated with data.  \nfunction GetDataAttribute(data) {\n  return data.dsrc;\n}\n\n\n// Placeholder minimatch function (requires npm install minimatch)\nfunction minimatch(str, pattern) {\n  const mm = require('minimatch'); // Make sure to install minimatch\n  return mm(str, pattern);\n}\n\n\n\n// GenerateQuery function (Section 4.2.2)\nasync function GenerateQuery(ctxAt, pluginCall, llm) {\n  const prompt = `System: Generate a JSON format query to extract necessary information from ${pluginCall}'s result`;\n  const query = await llm.call(prompt, ctxAt);  // Call the LLM to generate the query\n  return JSON.parse(query); // Assuming LLM returns a JSON string\n}\n\n\n\n\n// FlowCheck function (Section 4.3) (Simplified example â€“ PFI uses different checks for different flow types)\nfunction FlowCheck(dataProxy, sink, sinkType) {\n  if (sinkType === \"privileged\" && dataProxy.startsWith(\"#DATA\")) {\n    // Unsafe flow detected! Alert user.\n    const confirmation = confirm(`Do you approve using unsafe data ${dataProxy} in ${sink}?`);\n    if (confirmation) {\n      // User approved - replace proxy with actual data.\n      return ResolveDataProxy(dataProxy); \n    } else {\n      // User denied - block the flow.\n      return \"PERMISSION_DENIED\"; // Or throw an error, etc.\n    }\n  }\n  return dataProxy; // No unsafe flow detected, return as is\n}\n\n\n\n// Placeholder function to resolve a data proxy\nfunction ResolveDataProxy(proxy) {\n    // Logic to lookup actual data using the proxy\n    // (e.g., from proxy table in the paper).\n    // Replace with your actual implementation.\n    return \"Resolved data for \" + proxy;\n}\n\n```\n\n**TrustCheck():**\n\n* **Purpose:** Determines whether a given piece of `data` is trusted or untrusted based on its `dsrc` attribute and the `Pdata` policy.\n* **Algorithm:** The function extracts the `dsrc` from the data. It then checks this `dsrc` against the provided `Pdata` policy.  It utilizes a recursive approach and wildcard matching to handle complex, nested policies and composite `dsrc` values.  It defaults to \"Untrusted\" if no matching rule is found.\n* **JavaScript implementation:** The provided JavaScript utilizes a simplified pattern matching approach. A production-ready implementation would need a more robust mechanism (e.g., using a library like `minimatch` for wildcard support, handling regular expressions in PData rules, and implementing more sophisticated recursive or iterative policy checking). The `GetDataAttribute` function is a placeholder and would need to be implemented based on how dsrc is stored.\n\n**GenerateQuery():**\n\n* **Purpose:**  Generates a query in JSON format to instruct `Au` (untrusted agent) on what information to extract from untrusted data.\n* **Algorithm:**  It constructs a system prompt instructing the LLM to generate the query based on the current agent context (`ctxAt`) and the `pluginCall` that returned the untrusted data. It calls the LLM with the prompt and context, and parses the returned JSON query.\n* **JavaScript implementation:** This uses a placeholder `llm.call()` function. You would replace this with the actual method to call your LLM, providing it with the prompt and context. The implementation assumes the LLM returns a JSON string, which is then parsed.\n\n**FlowCheck():**\n\n* **Purpose:** Detects unsafe data flows from untrusted data proxies within the trusted agent.\n* **Algorithm:** Checks if the `sink` (where data is being used) is `privileged`, and if the data being used is an untrusted `dataProxy`. If both are true, an unsafe flow is detected, and the user is alerted to authorize it. If authorized, the proxy is replaced with the actual data; otherwise, the flow is blocked.\n* **JavaScript implementation:**  This JavaScript implementation demonstrates a *simplified* version of `FlowCheck`, focused on explicit data flows. PFI uses different checks for explicit control and implicit flows, which are not included here. The `ResolveDataProxy` function is a placeholder and needs a proper implementation to fetch the original untrusted data given the proxy.\n\n\nThese JavaScript snippets provide a basic starting point for experimenting with PFI's core concepts in your projects.  Remember to consider the comments about required additions for production-level deployments. Using an existing LLM agent framework would simplify integrating these core elements.",
  "simpleQuestion": "How can I secure LLM agent prompts against privilege escalation?",
  "timestamp": "2025-03-21T06:03:46.452Z"
}