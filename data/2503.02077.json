{
  "arxivId": "2503.02077",
  "title": "M³HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality",
  "abstract": "Designing effective reward functions in multi-agent reinforcement learning (MARL) is a significant challenge, often leading to suboptimal or misaligned behaviors in complex, coordinated environments. We introduce Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality (M³HF), a novel framework that integrates multi-phase human feedback of mixed quality into the MARL training process. By involving humans with diverse expertise levels to provide iterative guidance, M³HF leverages both expert and non-expert feedback to continuously refine agents' policies. During training, we strategically pause agent learning for human evaluation, parse feedback using large language models to assign it appropriately and update reward functions through predefined templates and adaptive weight by using weight decay and performance-based adjustments. Our approach enables the integration of nuanced human insights across various levels of quality, enhancing the interpretability and robustness of multi-agent cooperation. Empirical results in challenging environments demonstrate that M³HF significantly outperforms state-of-the-art methods, effectively addressing the complexities of reward design in MARL and enabling broader human participation in the training process.",
  "summary": "This paper introduces M³HF (Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality), a method for training AI agents in collaborative tasks where defining clear rewards is difficult.  It addresses this by incorporating human feedback at different stages of the training process to refine the agents' behavior.\n\nKey points for LLM-based multi-agent systems:\n\n* **Human feedback parsing:** LLMs interpret human feedback (which can be varied in quality and detail) and translate it into structured rewards for the individual agents.\n* **Iterative refinement:** Feedback is incorporated across multiple training \"generations,\" allowing the system to progressively improve agent coordination based on human guidance.\n* **Reward function templates:** LLMs use predefined templates to create new reward functions based on the parsed feedback, adding structure and efficiency to the process.\n* **Adaptive weighting:** The system dynamically adjusts the importance of different reward components based on the observed improvement in agent performance, mitigating the impact of noisy or inaccurate feedback.\n* **VLM exploration:**  The paper explores using Vision-Language Models (VLMs) as an alternative feedback source, but finds current VLMs lacking in providing actionable suggestions.",
  "takeaways": "Let's translate the M³HF paper's insights into practical JavaScript examples for LLM-based multi-agent web applications.\n\n**Scenario:** Imagine building a collaborative web application for online event planning, where multiple AI agents assist users with tasks like scheduling, resource allocation, and vendor communication. These agents are powered by LLMs and interact with each other and the user through a web interface.\n\n**Applying M³HF principles:**\n\n1. **Multi-Phase Human Feedback Integration:**\n\n* **JavaScript Implementation:**\n```javascript\n// Store reward functions in a pool for each agent\nconst agentRewardPools = {};\n\n// Function to update reward function based on feedback\nasync function updateRewardFunction(agentId, feedback) {\n    const parsedFeedback = await parseFeedbackWithLLM(feedback); // Call to LLM parsing API\n    const newRewardFunction = generateRewardFunction(parsedFeedback); // Uses templates (see below)\n    if (!agentRewardPools[agentId]) {\n      agentRewardPools[agentId] = [];\n    }\n    agentRewardPools[agentId].push(newRewardFunction);\n    adjustRewardWeights(agentId);\n}\n\n\n// Function to gather feedback after agent actions\nfunction gatherFeedback(agentActions) {\n    // Display agent actions to the user (e.g., schedule proposal, vendor contact)\n    // Provide interface for user feedback (e.g., ratings, free-form text)\n    const userFeedback = getUserFeedbackFromUI(); \n    updateRewardFunction(agentId, userFeedback); // Update the reward pool\n}\n\n// Example usage:\ngatherFeedback(agentsActions);\n```\n\n2. **LLM-powered Feedback Parsing and Reward Function Generation:**\n\n* **JavaScript Implementation (using a hypothetical LLM API):**\n\n```javascript\nasync function parseFeedbackWithLLM(feedback) {\n    const response = await fetch('https://your-llm-api/parse', {\n        method: 'POST',\n        body: JSON.stringify({ feedback, numAgents: 3 }), // Pass feedback and numAgents\n        headers: { 'Content-Type': 'application/json' }\n    });\n    return await response.json(); // Returns parsed feedback per agent and general feedback\n}\n\nfunction generateRewardFunction(parsedFeedback) {\n  // Example using distance-based reward for scheduling agent\n  if (parsedFeedback.agent_0.includes(\"schedule earlier\")) {\n      return (state) => {\n          const scheduledTime = state.scheduledTime;\n          const targetTime = state.idealTime;\n          return -Math.abs(scheduledTime - targetTime); // Negative distance as reward\n      };\n  } // Add more conditions and templates for other agents and feedback types\n  return ()=>{};\n\n}\n\nfunction adjustRewardWeights(agentId){\n\n  // Get the reward function pool for the agent\n  const rewardFunctions = agentRewardPools[agentId];\n\n  // Normalize weights\n\n  let sum = 0;\n  for (let j = 0; j < rewardFunctions.length; j++) {\n    sum = sum + rewardFunctions[j].weight;\n  }\n  for (let j = 0; j < rewardFunctions.length; j++) {\n    rewardFunctions[j].weight = rewardFunctions[j].weight/sum;\n  }\n\n  //Performance based adjustment (Simplification)\n\n  if(agentPerformanceImproved(agentId)){\n    rewardFunctions[rewardFunctions.length -1].weight = rewardFunctions[rewardFunctions.length -1].weight + beta;\n  }\n  else{\n    rewardFunctions[rewardFunctions.length -1].weight = max(0, rewardFunctions[rewardFunctions.length -1].weight - beta)\n  }\n\n}\n```\n\n3. **Reward Templates:**\n\n* **Distance-based:** Reward agents for scheduling events closer to user-preferred times.\n* **Action-based:** Reward agents for successfully contacting vendors or booking resources.\n* **Status-based:** Reward agents for reaching agreement on event details.\n* **Combined:** Combine different templates to create more nuanced rewards.\n\n\n**JavaScript Frameworks/Libraries:**\n\n* **Frontend:** React, Vue, or Angular to create interactive UI for feedback collection.\n* **Backend:** Node.js with Express to handle feedback processing and agent communication.\n* **LLM Integration:** LangChain, LlamaIndex for interacting with LLM APIs and managing prompts.\n\n\n**Key Considerations:**\n\n* **Feedback Frequency:** Gather feedback at strategic points during the event planning process.\n* **Feedback Quality Handling:** Implement mechanisms to filter or downweight low-quality or inconsistent feedback.\n* **Experimentation:** Start with simple reward templates and gradually increase complexity.\n* **User Experience:** Design a user-friendly interface for feedback submission and agent interaction.\n\n\nBy incorporating these M³HF principles, you can create more adaptable and effective multi-agent systems for web applications.  Remember that this is a simplified example. Real-world applications may require more sophisticated reward functions and feedback handling strategies. The key is to iterate and refine based on user feedback and agent performance.",
  "pseudocode": "```javascript\n// Algorithm 1: M³HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality\n\nasync function m3hf(N, initialRewardFunctions, rewardTemplates, environment, initialPolicies, totalGenerations) {\n  // Initialize reward function pools for each agent\n  const rewardPools = initialRewardFunctions.map(reward => [reward]);\n\n  // Iterate through generations\n  for (let k = 0; k < totalGenerations; k++) {\n    // Multi-agent Training Phase\n    for (let i = 0; i < N; i++) {\n      // Get current reward function (Eq. 13 - explained below)\n      const currentRewardFunction = getCombinedRewardFunction(rewardPools[i]);\n\n      // Train policy using current reward function (Eq. 4 conceptual - implementation would be environment-specific)\n      initialPolicies[i] = await trainPolicy(initialPolicies[i], environment, currentRewardFunction) ; // Replace with actual training logic\n    }\n\n    // Rollout Generation (Sec. 4.1)\n    let rolloutTrajectories = null;\n    if (shouldGenerateRollout(k)) { // Replace with actual criteria for rollout generation\n      rolloutTrajectories = await generateRollouts(environment, initialPolicies);\n    }\n\n    // Human Feedback Phase (Sec. 4.2)\n    if (rolloutTrajectories) {\n      const humanFeedback = await getHumanFeedback(rolloutTrajectories);  // Replace with feedback acquisition logic\n\n      // Feedback Parsing using LLM (Sec. 4.2 - implementation details in prompts)\n      const parsedFeedback = await parseFeedback(humanFeedback, N);\n\n\n      // Reward Function Update (Sec. 4.3)\n      for (let i = 0; i < N; i++) {\n        //Generate new reward function from feedback (Eq. 11 and prompts)\n        const newRewardFunction = await generateRewardFunction(rewardTemplates, parsedFeedback[i], environment);\n\n        rewardPools[i].push(newRewardFunction);\n\n\n         // Weight update logic (Eq. 14-16)\n        updateWeights(rewardPools[i], initialPolicies[i], environment, initialRewardFunctions[i]) \n\n      }\n\n    }\n  }\n\n\n    //get combined Reward Function. (Eq. 13)\n   function getCombinedRewardFunction(agentRewardPool) {\n     let M = agentRewardPool.length;\n     let combinedReward = (s, a) => 0;\n\n\n     for (let m=0; m < M; m++){\n        combinedReward = (s, a) => combinedReward(s, a) + agentRewardPool[m].weight * agentRewardPool[m].rewardFunction(s, a);\n      }\n\n     return combinedReward;\n\n\n    }\n\n   function updateWeights(agentRewardPool, agentPolicy, environment, originalRewardFunction){\n\n      let M = agentRewardPool.length;\n      let decayFactor = 0.9;\n      let adjustmentFactor = 0.1;\n      // Initialize weight for new reward function (Eq. 15)\n        agentRewardPool[M - 1].weight = 1 / M;\n\n\n\n      // Apply weight decay to existing weights (Eq. 14)\n      for (let m = 0; m < M - 1; m++) {\n        agentRewardPool[m].weight *= Math.pow(decayFactor, (M - 1) - m); \n      }\n\n        // Normalize weights (Eq. 15)\n       let totalWeight = agentRewardPool.reduce((sum, reward) => sum + reward.weight, 0);\n\n        for (let m = 0; m < M; m++){\n          agentRewardPool[m].weight /= totalWeight;\n         }\n\n       // Compute performance difference (Ar)\n       let r_prev = evaluatePolicy(agentPolicy, environment, originalRewardFunction); // Replace evaluatePolicy\n       let r_current = evaluatePolicy(agentPolicy, environment, getCombinedRewardFunction(agentRewardPool));\n       let perfDiff = r_current - r_prev;\n\n\n      // Adjust weight of newest reward function (Eq. 16)\n     if (perfDiff > 0) {\n          agentRewardPool[M - 1].weight += adjustmentFactor;\n     } else {\n\n         agentRewardPool[M - 1].weight = Math.max(0, agentRewardPool[M - 1].weight - adjustmentFactor);\n      }\n\n\n     //Now set the updated reward function\n      agentRewardPool[M-1].rewardFunction = (s,a) => agentRewardPool[M-1].rewardFunction(s,a) / agentRewardPool[M-1].weight;\n       \n    }\n\n\n\n\n  return initialPolicies;\n}\n\n\n\n// Placeholder functions – replace with your actual implementations\n\nasync function trainPolicy(policy, environment, rewardFunction) {\n    // Your policy training logic here – this is highly environment and algorithm specific.\n    // It should use the provided rewardFunction during training.\n   return  //updated policy;\n\n}\n\nasync function generateRollouts(environment, policies) {\n  // Your rollout generation logic here\n  return; // rollout data\n}\n\nfunction shouldGenerateRollout(k) {\n  // Your logic to determine if a rollout should be generated at generation k (periodic evaluation or performance stagnation).\n  return true; // Or false based on your criteria.\n}\n\nasync function getHumanFeedback(rolloutTrajectories) {\n    // Your logic to acquire human feedback for the given rollouts.\n    return // human feedback;\n}\n\n\nasync function parseFeedback(humanFeedback, numAgents){\n  //Parse feedback based on the prompt in Section F.4.\n  // This function uses LLM as described in the paper Sec 4.2 \n\n  return // agentSpecificFeedback, generalFeedback;\n}\n\n\nasync function generateRewardFunction(rewardTemplates, parsedFeedback, environment){\n  //Generate new reward function as described in  Sec 4.2 and prompts provided in Section F.4\n\n   return // newRewardFunction;\n}\n\n\nfunction evaluatePolicy(policy, environment, rewardFunction){\n  //Evaluate policy given the environment and the reward function.\n   return //performance metrics\n}\n\n\n```\n\n**Explanation of Algorithm 1 and its Purpose:**\n\nThe M³HF (Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality) algorithm aims to improve multi-agent coordination in complex tasks by incorporating human feedback into the reward function.  This is especially useful when the original reward function is sparse or difficult to learn from. The algorithm operates in generations. Within each generation:\n\n1. **Multi-agent Training Phase:** Agents train their policies based on a weighted combination of reward functions accumulated so far including the initial reward function, and any reward functions generated from previous human feedback.\n\n2. **Rollout Generation:**  At specific intervals (periodically or when performance stagnates), the agents' current policies are used to generate rollout trajectories, which are essentially recordings of the agents playing the game.\n\n3. **Human Feedback Phase:**  A human observes these rollout trajectories and provides feedback on the agents' performance. This feedback can be free-form text, suggesting improvements in coordination, strategy, or task allocation.\n\n4. **Feedback Parsing:** An LLM parses this human feedback and assigns it to specific agents or to all agents generally.\n\n5. **Reward Function Update:**  Based on the parsed feedback, the system uses predefined reward function templates (like distance-based rewards, action-based rewards, status-based rewards, etc.) to create new reward functions that incentivize behaviors aligned with the human's suggestions. These new reward functions are added to the agent's reward pool.\n\n6. **Weight Update:** The weights of the reward functions in each agent's pool are adjusted using a decay mechanism (giving more weight to recent feedback) and a performance-based adjustment (increasing the weight of reward functions that demonstrably improve performance on the original task).  This ensures that helpful feedback gets more weight, while unhelpful feedback gets less weight.\n\nBy iterating through these phases, the agents progressively refine their policies, incorporating human insights to achieve better coordination and overall performance.  The algorithm allows for \"mixed quality\" feedback, meaning the human doesn't have to be an expert, and their feedback can be noisy or even partially incorrect. The weight adjustment mechanism helps mitigate the impact of poor quality feedback.",
  "simpleQuestion": "How can mixed-quality human feedback improve MARL reward functions?",
  "timestamp": "2025-03-05T06:03:16.267Z"
}