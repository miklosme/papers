{
  "arxivId": "2502.06060",
  "title": "Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning",
  "abstract": "Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent's goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model's listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model's speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on AMONG US, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at https://socialdeductionllm.github.io/.",
  "summary": "This research explores how to train language models (LLMs) for effective communication and strategy in multi-agent social deduction games, specifically using a simplified version of Among Us.\n\nKey points for LLM-based multi-agent systems:\n\n* **Emergent Communication:** LLMs can learn complex communication strategies without human examples by using a combination of reinforcement learning (RL) and tailored reward signals.\n* **Listening and Speaking:** The system separates training into \"listening\" (understanding messages and game state) and \"speaking\" (generating influential messages).  Listening is trained via a supervised imposter prediction task, while speaking is trained through RL by rewarding messages that shift other agents' beliefs towards the correct imposter.\n* **Robustness:** The trained LLM agents are robust against a variety of imposters and environment configurations.\n* **Self-Improvement:** The framework allows LLMs to self-critique and improve their communication abilities over time without relying on human feedback.\n* **Natural Language Grounding:** The game environment is designed to interact with agents using natural language, making it directly compatible with LLMs as agents.\n* **Challenges:**  Addressing issues like diverging from natural language, degenerate solutions, and the use of action tokens in discussions required specific training techniques.\n* **Ethical Considerations:** Agents sometimes generate false statements for strategic advantage, raising potential ethical concerns for applications beyond the game environment.",
  "takeaways": "This paper offers valuable insights for JavaScript developers working with LLM-based multi-agent applications, particularly in web development. Here are some practical examples:\n\n**1. Building a Collaborative Writing Tool:**\n\n* **Concept:** Multiple LLMs collaborate to write an article, story, or code. Each agent could specialize in a different writing style (e.g., creative, technical, persuasive), tone, or content area.\n* **Implementation:**\n    * **Frontend:** Use a framework like React or Vue.js to create a collaborative text editor where each agent's contributions are visually distinct (e.g., different colors, annotations).\n    * **Backend:** Node.js with a library like LangChain can manage interactions with the LLMs. Implement the \"listening\" and \"speaking\" training loops described in the paper.  The \"listening\" loop trains each agent to predict the overall writing goal given previous contributions. The \"speaking\" loop uses a reward function based on how much other agents' confidence in the final product increases after each contribution.\n    * **Inter-agent Communication:**  Use a message queue (e.g., Redis, Kafka) or a real-time framework (e.g., Socket.IO) to enable LLMs to communicate intermediate text, edits, and feedback.\n* **Evaluation:** Measure improvements by assessing the coherence, style consistency, and overall quality of the co-authored text compared to text generated by a single LLM.\n\n\n**2. Creating an Interactive Storytelling Experience:**\n\n* **Concept:** LLMs play different characters in a story that unfolds based on user interactions on a webpage.\n* **Implementation:**\n    * **Frontend:** A JavaScript framework like React or Svelte can render the narrative, character dialogues, and user interaction options.\n    * **Backend:** Node.js can manage the story state and LLM interactions. The \"listening\" loop helps agents understand the current narrative and user choices, while the \"speaking\" loop rewards contributions that move the narrative forward in a compelling way (e.g., increased user engagement, positive feedback).  Consider LangChain or similar libraries.\n    * **Memory Management:** Implement a memory mechanism (e.g., a database, vector database with embeddings) to store character backstories, past interactions, and the evolving story state, allowing the LLMs to maintain context.\n* **Evaluation:** Track user engagement metrics (e.g., time spent reading, number of interactions) to assess the quality and immersiveness of the storytelling experience.\n\n\n**3. Developing a Multi-Agent Chatbot for Customer Service:**\n\n* **Concept:** Multiple LLMs with different specializations (e.g., product knowledge, technical support, billing) collaborate to answer complex customer queries.\n* **Implementation:**\n    * **Frontend:** Implement a chat interface on a webpage using a framework like React.\n    * **Backend:** Node.js handles communication between the LLMs and the frontend. The \"listening\" loop helps agents identify which agent is best suited to handle a specific aspect of the query. The \"speaking\" loop rewards contributions that improve customer satisfaction (e.g., positive feedback, issue resolution).\n    * **Coordination Mechanism:** Implement a \"discussion phase\" as described in the paper where agents exchange internal messages to decide who takes the lead on answering different parts of the query.\n* **Evaluation:** Measure customer satisfaction scores and the efficiency of issue resolution to assess the effectiveness of the multi-agent chatbot system.\n\n\n**Key JavaScript Considerations:**\n\n* **Asynchronous Programming:** Use `async`/`await` or Promises to manage concurrent LLM interactions.\n* **Modular Design:** Break down the system into reusable modules (e.g., for communication, reward calculation, memory management).\n* **Observability:** Implement logging and monitoring to track agent interactions, performance, and potential issues.\n\n\nBy applying the concepts from the research paper and using appropriate JavaScript technologies, developers can create innovative and effective multi-agent AI web applications. The focus should be on designing appropriate reward functions and communication protocols for the specific application. Remember to prioritize ethical considerations and mitigation of potential risks associated with LLMs, such as generating false information or harmful content.",
  "pseudocode": "The paper doesn't contain pseudocode blocks in the traditional sense. However, it presents several loss functions and update rules in mathematical notation, which can be interpreted and implemented as JavaScript functions.  Let's translate these into JavaScript:\n\n**1. Reinforcement Learning Loss (Equation 1):**\n\n```javascript\nfunction calculateRLLoss(trajectory, policy, basePolicy, lambdaKL) {\n  let loss = 0;\n  for (const timestep of trajectory) {\n    const action = policy(timestep.state); // Get action from current policy\n    loss -= Math.log(policy(timestep.state, action)) * timestep.reward; // Negative log-likelihood of action times reward\n    loss += lambdaKL * klDivergence(policy(timestep.state), basePolicy(timestep.state)); // KL divergence regularization\n  }\n  return loss;\n}\n\n// Helper function to calculate KL divergence (example, needs to be adapted to specific policy representation)\nfunction klDivergence(p, q) {\n  let divergence = 0;\n  for (let i = 0; i < p.length; i++) {\n    if (p[i] > 0 && q[i] > 0) {\n      divergence += p[i] * Math.log(p[i] / q[i]);\n    }\n  }\n  return divergence;\n}\n```\n\n* **Explanation:** This function calculates the reinforcement learning loss for a given trajectory. It encourages the policy to choose actions that maximize cumulative rewards, while also staying close to the base language model using KL divergence regularization.\n\n**2. Listening Loss (Equation 2):**\n\n```javascript\nfunction calculateListeningLoss(actionObservationHistory, policy, trueImposter) {\n  return -Math.log(policy(actionObservationHistory, trueImposter)); // Negative log-likelihood of correctly identifying the imposter\n}\n```\n\n* **Explanation:**  This function calculates the loss for the imposter prediction task. It encourages the agent to correctly identify the imposter given its action-observation history.\n\n**3. Speaking Reward (Equation 6):**\n\n```javascript\nfunction calculateSpeakingReward(currentBeliefs, previousBeliefs) {\n  return currentBeliefs - previousBeliefs; // Difference in crewmates' beliefs on the true imposter\n}\n```\n\n* **Explanation:** This function calculates the reward for a spoken message based on how it influences other crewmates' beliefs about the imposter. A positive reward means the message increased the overall certainty about the imposter's identity.\n\n\n**4. Combined Loss functions (Equations 3, 4, 7, 8):** These equations combine the above loss functions and reward.  Their implementation would involve calling the individual loss/reward functions within a larger training loop, weighing them with the corresponding lambda hyperparameters.  For brevity, a full JavaScript implementation isn't shown, but the core logic would be to sum the weighted individual losses/rewards.\n\n**Key Improvements and Purpose of the Algorithms:**\n\n* **Dense Rewards for Communication:** The listening loss and speaking reward provide denser feedback compared to the sparse win/lose signal of the game, enabling more effective learning of communication strategies.\n* **Self-Improvement:** By leveraging the agent's own predictions and the reactions of other agents, the system enables self-improvement of communication skills without relying on human data.\n* **Robustness:**  The training for dynamic settings and against adaptive imposters aims to develop robust communication strategies that work in various scenarios and against different opponents.\n\n\nThese JavaScript implementations are conceptual and would require adaptation based on the specific data structures and policy representation used in the application.  The core logic, however, reflects the mathematical formulas presented in the paper. This provides a starting point for JavaScript developers to implement these multi-agent AI concepts in their projects.",
  "simpleQuestion": "How can LLMs learn social deduction via multi-agent RL?",
  "timestamp": "2025-02-11T06:11:18.337Z"
}