{
  "arxivId": "2411.15998",
  "title": "PIANIST: Learning Partially Observable World Models with LLMs for Multi-Agent Decision Making",
  "abstract": "Effective extraction of the world knowledge in LLMs for complex decision-making tasks remains a challenge. We propose a framework PIANIST for decomposing the world model into seven intuitive components conducive to zero-shot LLM generation. Given only the natural language description of the game and how input observations are formatted, our method can generate a working world model for fast and efficient MCTS simulation. We show that our method works well on two different games that challenge the planning and decision-making skills of the agent for both language and non-language-based action taking, without any training on domain-specific training data or explicitly defined world model.",
  "summary": "This paper introduces PIANIST, a framework for using LLMs as world models in multi-agent, partially observable settings.  PIANIST decomposes the world model into seven components (Partitioned functions, Information sets, Action functions, Number of actors, Information realization functions, State spaces, and Transition-reward functions), which are generated by prompting an LLM with the game rules and a Python template. This allows for efficient planning using methods like Monte Carlo Tree Search (MCTS).  Key to LLM-based multi-agent systems is PIANIST's ability to handle partial observability and complex action spaces (especially in language-based games) by using the LLM to suggest plausible actions, improving search efficiency and mitigating LLM bias.  The framework shows promising zero-shot performance in games like GOPS and Taboo, demonstrating the potential of LLMs for generating effective world models in multi-agent scenarios.",
  "takeaways": "This paper introduces PIANIST, a framework for using LLMs as world models in multi-agent, partially observable scenarios, which opens exciting possibilities for JavaScript developers building interactive, AI-driven web applications. Here's how a JavaScript developer can apply the insights from the PIANIST paper:\n\n**1. Building Interactive Narrative Games:**\n\n* **Scenario:** Imagine creating a text-based adventure game where multiple users interact with each other and an LLM-powered game master within a shared virtual world.\n* **PIANIST Application:** Use an LLM to generate the PIANIST components (transition, action, reward, information partition functions) based on a description of the game world and rules. The LLM can also propose dialogue options for players, streamlining the action space and making the game more manageable. The client-side JavaScript code could handle user input, display the narrative, and communicate with a server that houses the LLM and MCTS logic.\n* **JavaScript Implementation:** Frameworks like React, Vue, or Svelte could be used for the front-end, while Node.js and libraries like Langchain or Transformers.js could handle server-side LLM interactions.  Use a library like TensorFlow.js to implement the MCTS algorithm in the browser, allowing for a more responsive experience.\n\n**2. Collaborative Design Tools:**\n\n* **Scenario:** Develop a web app where multiple users collaboratively design a product, assisted by an LLM agent suggesting design options and evaluating design choices based on pre-defined criteria.\n* **PIANIST Application:** The LLM could define the world model (design space, possible actions, design criteria), generate potential design modifications (actions), and predict the impact of these modifications (transitions and rewards). The client-side JavaScript code could visualize the design, enable user interactions, and communicate with the LLM.\n* **JavaScript Implementation:**  Use a canvas-based library like Fabric.js or p5.js for visualization and user interaction. Server-side logic could be handled by Node.js and a suitable LLM library.\n\n**3. Simulating Complex Systems:**\n\n* **Scenario:** Create a web application that simulates a complex system like a supply chain, a traffic network, or an ecosystem, with multiple LLM-powered agents representing different entities within the system.\n* **PIANIST Application:** Define the system's rules and parameters. The LLM can model the behavior of each agent, predict their actions, and simulate the evolution of the system. Client-side JavaScript could visualize the simulation and allow users to interact with it by changing parameters or introducing external events.\n* **JavaScript Implementation:** D3.js or Three.js are excellent for visualizations.  Server-side, Node.js and an LLM library can handle the simulation logic.\n\n**4. Personalized Learning Environments:**\n\n* **Scenario:** Build an adaptive learning platform where each student interacts with an LLM-powered tutor.\n* **PIANIST Application:** The LLM could model the student's learning progress, generate personalized learning materials, and adapt the difficulty level based on student performance. The PIANIST framework could be used to model the interaction between the student and the tutor, considering the student's knowledge level (hidden state) and providing personalized feedback (rewards).\n* **JavaScript Implementation:** Client-side, use a framework like React to create interactive learning modules. Implement server-side logic with Node.js and an LLM library.\n\n**Key JavaScript Concepts and Libraries:**\n\n* **Asynchronous Programming (async/await):** Essential for managing LLM interactions, which can be time-consuming.\n* **WebSockets:**  Enable real-time communication between clients and the server, essential for multi-agent interactions.\n* **LLM Libraries:**  Langchain, Transformers.js.\n* **Visualization Libraries:** D3.js, Three.js, Fabric.js, p5.js.\n* **Front-end Frameworks:** React, Vue, Svelte, Angular.\n* **Back-end Framework:** Node.js.\n\nBy leveraging the PIANIST framework and these JavaScript tools, developers can create engaging and intelligent web applications that push the boundaries of interactive, multi-agent AI experiences.  Remember to consider ethical implications and potential biases when designing LLM-powered applications.  Focus on human-centered design, with the LLM acting as a powerful tool to enhance the user experience.",
  "pseudocode": "The paper contains one pseudocode block describing the Monte Carlo Tree Search (MCTS) algorithm for partial information games using information sets. Here's the JavaScript conversion along with explanations:\n\n```javascript\nclass Node {\n  constructor(state, parent, action, actor, h, values) {\n    this.state = state; // Hidden state\n    this.parent = parent;\n    this.action = action; // Action taken to reach this node\n    this.actor = actor;  // Actor who took the action\n    this.h = h;      // Information set\n    this.values = values; // Value estimates for each player\n    this.children = [];\n    this.actionsTried = new Set(); // Keep track of tried actions\n    this.visitCount = 0;\n  }\n}\n\nfunction mcts(initialState, initialInfoSet, numIterations) {\n  const root = new Node(initialState, null, null, null, initialInfoSet, estimateValues(initialState));\n  const graph = [root]; // maintain all nodes\n\n  for (let i = 0; i < numIterations; i++) {\n    let node = realize(graph, root.h);\n    let action;\n    [node, action] = select(node);\n\n    if (node) { // Not a terminal node\n      const child = expand(node, action);\n      graph.push(child); \n      backpropagate(child);\n    }\n  }\n\n  return bestChild(root).action;\n}\n\nfunction select(node) {\n  while (!isTerminal(node)) {\n    if (node.actionsTried.size < node.children.length) {\n      for(const child of node.children){\n          if(!node.actionsTried.has(child.action)){\n            return [node, child.action]; // untried action from expansion\n          }\n      } \n    } else {\n      node = bestChild(node);\n    }\n  }\n  return [null, null]; // Terminal node\n}\n\n\nfunction expand(parent, action) {\n  const [nextState, rewards] = transition(parent.state, action, parent.actor);\n  const nextActions = enumerateActions(nextState, parent.actor);\n  const nextInfoSet = informationSet(parent, nextActions); \n  const childValues = estimateValues(nextState, rewards);\n  const child = new Node(nextState, parent, action, parent.actor, nextInfoSet, childValues);\n  parent.children.push(child);\n  parent.actionsTried.add(action);\n  return child;\n}\n\n\nfunction estimateValues(state, rewards) {\n  // Placeholder: Replace with your value heuristic or rollout\n  return rewards; // Example: Use immediate rewards \n}\n\n\nfunction backpropagate(node) {\n  let nextValues = node.values;\n  while (node) {\n    node.visitCount++;\n    for(let player in nextValues){\n       node.values[player] = node.values[player] + (nextValues[player] - node.values[player]) / node.visitCount;\n    } \n\n    node = node.parent;\n  }\n}\n\n\nfunction bestChild(node) {\n  // Implement UCT or other selection strategy here\n   // Placeholder:  Select child with most visits\n   return node.children.reduce((best, child) => (child.visitCount > best.visitCount ? child : best), node.children[0]);\n}\n\n\n\nfunction realize(graph, h) {\n  const nodes = graph.filter(node => node.h === h);\n  if (nodes.length === 0) {\n      // Create a new node node with node.h = h and node.s = I(h) and add to graph\n      const newState = informationRealization(h);\n      const newNode = new Node(newState, null, null, null, h, estimateValues(newState));\n      graph.push(newNode);\n      return newNode;\n    } else {\n      const randomIndex = Math.floor(Math.random() * nodes.length);\n       return nodes[randomIndex];\n    }\n}\n\n// Placeholder functions - replace with game-specific logic\nfunction isTerminal(node){ return false;} // Implement Terminal conditions\nfunction transition(state, action, actor){ return [null, {}];} // Implement state transitions and reward function\nfunction enumerateActions(state, actor){ return [];} // Implement action enumeration based on state\nfunction informationSet(parent, action){ return parent.h;} // Implement information set update\nfunction informationRealization(h){ return null;}\n\n// Example usage:\nconst initialState = {}; // Replace with your initial game state\nconst initialInfoSet = {}; // Replace with your initial info set\nconst numIterations = 1000;\nconst bestAction = mcts(initialState, initialInfoSet, numIterations);\nconsole.log(\"Best action:\", bestAction);\n\n```\n\n\n**Explanation of the MCTS Algorithm and its Purpose:**\n\nMCTS is a tree search algorithm commonly used for decision-making in games and other sequential decision problems, especially where the search space is large or unknown.  Its purpose is to find the best action to take from a given state by simulating many possible game outcomes.  It's particularly useful in games with partial information, like the Taboo game mentioned in the paper, where the agent doesn't have complete knowledge of the game state.\n\nThe key idea of MCTS is to build a search tree where each node represents a game state and each edge represents an action.  The algorithm iteratively expands the tree by simulating game play from the current state. These simulations use a combination of exploration (trying out less-visited actions) and exploitation (favoring actions that have led to good outcomes in the past).\n\n**Key components and their roles in the JavaScript code:**\n\n* **`Node` class:** Represents a node in the search tree, storing the game state, parent node, action taken to reach this node, information set, value estimates, and children.\n* **`mcts(initialState, initialInfoSet, numIterations)`:** The main MCTS function. It takes the initial game state and information set, and the number of iterations (simulations) to run.\n* **`select(node)`:** Traverses the tree from the root to a leaf node using the UCT (Upper Confidence bounds for Trees) formula to balance exploration and exploitation.\n* **`expand(node, action)`:** Creates a new child node representing the game state after taking a given action.\n* **`estimateValues(state)`:** Estimates the value of a given state. This is often done using a heuristic function or by running random simulations (rollouts).\n* **`backpropagate(node)`:** Updates the visit counts and value estimates of all nodes along the path from the newly expanded node back to the root.\n* **`bestChild(node)`:** Selects the best child node according to a specific strategy (e.g., highest average reward, UCT).\n* **`realize(graph, h)`:** This function maps an information set `h` to a concrete hidden state from the graph, or generates a new state if needed. This represents the agent dealing with the partial observability.\n\n\n\nThe placeholder functions (`isTerminal`, `transition`, `enumerateActions`, `informationSet`, `informationRealization`) need to be replaced with game-specific logic.  The provided example uses immediate rewards as a simple value heuristic. In a real application, you would likely use a more sophisticated method, such as a learned value function or a deeper rollout.  Similarly, the `bestChild` function currently selects the child with the most visits; in practice, you would implement a UCT selection strategy.",
  "simpleQuestion": "Can LLMs build multi-agent game world models without training?",
  "timestamp": "2024-11-26T06:05:47.076Z"
}