{
  "arxivId": "2503.21807",
  "title": "LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning",
  "abstract": "Abstract. Multi-agent reinforcement learning (MARL) faces two critical bottlenecks distinct from single-agent RL: credit assignment in cooperative tasks and partial observability of environmental states. We propose LERO, a framework integrating Large language models (LLMs) with evolutionary optimization to address these MARL-specific challenges. The solution centers on two LLM-generated components: a hybrid reward function that dynamically allocates individual credit through reward decomposition, and an observation enhancement function that augments partial observations with inferred environmental context. An evolutionary algorithm optimizes these components through iterative MARL training cycles, where top-performing candidates guide subsequent LLM generations. Evaluations in Multi-Agent Particle Environments (MPE) demonstrate LERO's superiority over baseline methods, with improved task performance and training efficiency.",
  "summary": "This paper introduces LERO, a framework that uses LLMs and evolutionary algorithms to improve multi-agent reinforcement learning (MARL).  It tackles the challenges of credit assignment (figuring out which agent deserves credit for success) and partial observability (agents only seeing part of the environment) in MARL.\n\nLERO uses LLMs to generate two key components: hybrid reward functions (HRFs) that combine team and individual rewards, and observation enhancement functions (OEFs) to add context to what agents see.  An evolutionary algorithm then refines these LLM-generated components over multiple training cycles, improving agent performance and cooperation.  The key innovation is combining LLM generation with evolutionary refinement to create better reward and observation functions for multi-agent systems.  This is demonstrated with improved performance on cooperative navigation tasks.",
  "takeaways": "This paper presents LERO, a framework for enhancing multi-agent reinforcement learning (MARL) using LLMs and evolutionary optimization.  Here's how a JavaScript developer can apply its insights to LLM-based multi-agent projects, focusing on web development:\n\n**1. Hybrid Reward Functions with LLMs:**\n\n* **Scenario:** Imagine building a multi-agent web app where AI agents collaborate to personalize a user's webpage layout.  The goal is to maximize user engagement (clicks, time spent) while ensuring aesthetic appeal and accessibility.\n* **Application:** Instead of manually crafting a complex reward function, use an LLM (e.g., accessed via OpenAI's API) to generate a hybrid reward function based on a prompt. This prompt should describe both global rewards (overall user engagement) and individual agent rewards (contribution to specific aspects like layout, color scheme, content placement).\n* **JavaScript Implementation:**\n    ```javascript\n    async function generateRewardFunction(taskDescription, scenarioCode) {\n      const prompt = `\n        // System prompt (as in the paper's Appendix B)\n        // User prompt with taskDescription and scenarioCode\n      `;\n      const response = await openai.createCompletion({\n        // ... API parameters\n        prompt,\n      });\n      const rewardFunctionCode = response.data.choices[0].text;\n      // Execute the code string using eval() or Function constructor (carefully!)\n      return new Function('agent', 'world', rewardFunctionCode);\n    }\n\n    // Example usage\n    const rewardFn = await generateRewardFunction(\n      \"Maximize user engagement and aesthetics\",\n      `// Code describing webpage elements, user interactions, etc.`\n    );\n    const reward = rewardFn(agent1, webpageWorld);\n    ```\n* **Benefits:**  LLMs can create nuanced reward functions capturing complex trade-offs, potentially leading to more effective learning and better agent collaboration.\n\n**2. Enhanced Observation with LLMs:**\n\n* **Scenario:** Consider a multi-agent web game where AI agents control characters with limited visibility. Each agent only sees its immediate surroundings.\n* **Application:** Use an LLM to generate an observation enhancement function that infers global information from partial observations.  The prompt should describe what global information is relevant (e.g., locations of other players, important resources) and how it can be inferred from local observations.\n* **JavaScript Implementation:**  Similar to reward function generation, use the OpenAI API and prompts (refer to Appendix B of the paper) to generate JavaScript code for the observation enhancement function. This function would take an agent's local observation (e.g., a section of the game grid represented as a 2D array) as input and return a richer representation incorporating inferred global information.  Consider using libraries like TensorFlow.js for efficient tensor operations within the generated function.\n* **Benefits:** This allows agents to make better decisions based on a more complete understanding of the game state, promoting more strategic gameplay and cooperation.\n\n**3. Evolutionary Optimization:**\n\n* **Scenario:**  In the webpage layout personalization example, the LLM-generated reward function might not be perfect initially.\n* **Application:** Implement a simple evolutionary algorithm in JavaScript to iteratively refine the LLM-generated components.  Maintain a population of reward functions and observation enhancement functions.  Evaluate their performance by training agents with them. Select the best-performing ones, apply \"crossover\" (combining parts of different functions), and \"mutation\" (introducing small random changes) using the LLM as a code generation tool within the evolutionary loop.\n* **Libraries:**  Consider using a genetic algorithm library for JavaScript like `geneticalgorithm`.\n* **Benefits:** Evolutionary optimization can further improve the effectiveness of the LLM-generated functions by adapting them to the specific MARL task and environment.\n\n\n**Key Considerations for JavaScript Developers:**\n\n* **Security:** Be extremely cautious using `eval()` or the `Function` constructor with LLM-generated code. Sanitize the code rigorously to prevent malicious code execution.\n* **Performance:** LLMs are computationally expensive. Optimize your prompts and API usage to minimize inference costs. Consider using smaller, specialized LLMs or fine-tuning existing models for specific reward/observation tasks.\n* **Framework Integration:** Integrate the LLM-generated functions seamlessly with your chosen JavaScript MARL framework. This might involve adapting the function's input/output formats to match the framework's data structures.  Explore existing JavaScript reinforcement learning libraries.\n* **Experimentation:**  The paper provides a starting point.  Experiment with different prompt designs, LLM models, and evolutionary algorithms to discover what works best for your specific web development scenarios.\n\n\n\nBy carefully considering these elements, JavaScript developers can leverage the power of LLMs and evolutionary optimization to build more sophisticated and effective multi-agent AI systems for web applications. Remember to focus on clearly defining the tasks, environment, and desired agent behaviors in your prompts to guide the LLM's code generation process.",
  "pseudocode": "```javascript\nfunction agent_reward(agent, world) {\n  // Calculates the individual reward for an agent based on how well it guides its\n  // partner (goal_a) to reach the assigned landmark (goal_b).\n\n  if (agent.goal_a === null || agent.goal_b === null) {\n    return 0.0;\n  }\n\n  // Compute the Euclidean distance between the partner agent and the target landmark\n  const dist = Math.sqrt(\n    Math.pow(agent.goal_a.state.p_pos[0] - agent.goal_b.state.p_pos[0], 2) +\n    Math.pow(agent.goal_a.state.p_pos[1] - agent.goal_b.state.p_pos[1], 2)\n  );\n\n  let reward = -dist;\n\n  // Provide a bonus if the partner reaches the target landmark\n  if (dist < 0.1) {\n    reward += 10.0;\n  }\n\n  return reward;\n}\n\n\nfunction global_reward(world) {\n    // Calculates the global reward for the environment by aggregating the\n    // performance of all agents guiding their respective partners towards their\n    // targets.\n\n    let total_reward = 0.0;\n    const bonus = 10.0;\n\n    for (const agent of world.agents) {\n        if (agent.goal_a === null || agent.goal_b === null) {\n            continue;\n        }\n\n        // Compute the Euclidean distance for the current guidance pair\n        const dist = Math.sqrt(\n            Math.pow(agent.goal_a.state.p_pos[0] - agent.goal_b.state.p_pos[0], 2) +\n            Math.pow(agent.goal_a.state.p_pos[1] - agent.goal_b.state.p_pos[1], 2)\n        );\n\n\n        total_reward += -dist;\n\n        // Add bonus if the target landmark is reached\n        if (dist < 0.1) {\n            total_reward += bonus;\n        }\n    }\n    return total_reward;\n}\n\n\n\n\n\nfunction stateEnhance(obs) {\n    // Enhances agent observations by extracting supplementary global information.\n    const device = obs.device;  // Assuming a tensor library like TensorFlow.js is used\n    const [batch_size, num_agent, obs_length] = obs.shape;\n    const landmark_rel = obs.slice([0, 0, 2], [batch_size, num_agent, 6]).reshape([batch_size, num_agent, 3, 2]);\n    const landmark_dists = tf.norm(landmark_rel, 'euclidean', -1);\n\n\n    let partner_dist;\n    if (num_agent === 2) {\n\n        const diff = tf.sub(landmark_rel.slice([0, 0, 0, 0], [batch_size, 1, 3, 2]), landmark_rel.slice([0, 1, 0, 0], [batch_size, 1, 3, 2]));\n        const avg_diff = tf.mean(diff, 1);\n        const pdist = tf.norm(avg_diff, 'euclidean', -1, true);\n        partner_dist = pdist.reshape([batch_size, 1, 1]).tile([1, num_agent, 1]);\n\n    } else {\n        partner_dist = tf.zeros([batch_size, num_agent, 1], obs.dtype);\n    }\n\n\n    const global_info = tf.concat([landmark_dists, partner_dist], -1);\n\n    return global_info;\n\n\n}\n```\n\n**Explanation of the algorithms and their purposes:**\n\n1. **`agent_reward(agent, world)`:** This function calculates an individual reward for an agent in a multi-agent environment. The reward is based on how effectively the agent guides its partner towards a target landmark.  A higher reward is given when the partner is closer to the target, with a bonus awarded if the partner reaches the target within a certain proximity. This function encourages individual agents to perform their assigned roles effectively.\n\n\n\n2. **`global_reward(world)`:** This calculates a global reward based on the collective performance of all agents in the environment.  It aggregates the individual guidance outcomes by summing the negative distances between each partner agent and its target landmark. A bonus is added if all agent pairs successfully guide their partners to the respective target landmarks.  This incentivizes cooperation among agents to achieve the overall task objective.\n\n\n\n3. **`stateEnhance(obs)`:**  This function enhances the observation space by adding global information about the environment. It extracts two key features: (1) the distances between each agent and the landmarks, and (2) an estimated distance between the partner agents.  This enriched observation provides agents with a more comprehensive understanding of the environment, facilitating improved coordination. It utilizes efficient tensor operations to avoid costly `for` loops.  It is designed for a 2-agent scenario in the simple reference task, but provides a placeholder for other configurations.\n\n\n\nThese functions are crucial components of the LERO framework presented in the paper.  They address the challenges of credit assignment (through individual and global rewards) and partial observability (through observation enhancement) in multi-agent reinforcement learning. The JavaScript adaptations utilize TensorFlow.js for tensor operations to mimic the original Python/PyTorch implementations.  Note that the `obs.device` attribute and the `tf.*` functions are TensorFlow.js conventions. Ensure that your environment includes TensorFlow.js or a similar library. Also, vectorized calculations using libraries like NumJs may further optimize the code if performance becomes a bottleneck.",
  "simpleQuestion": "How can LLMs improve MARL credit assignment and observation?",
  "timestamp": "2025-03-31T05:03:14.330Z"
}