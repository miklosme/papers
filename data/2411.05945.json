{
  "arxivId": "2411.05945",
  "title": "NEKO: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts",
  "abstract": "Construction of a general-purpose post-recognition error corrector poses a crucial question: how can we most effectively train a model on a large mixture of domain datasets? The answer would lie in learning dataset-specific features and digesting their knowledge in a single model. Previous methods achieve this by having separate correction language models, resulting in a significant increase in parameters. In this work, we present Mixture-of-Experts as a solution, highlighting that MoEs are much more than a scalability tool. We propose a Multi-Task Correction MoE, where we train the experts to become an “expert” of speech-to-text, language-to-text and vision-to-text datasets by learning to route each dataset's tokens to its mapped expert. Experiments on the Open ASR Leaderboard show that we explore a new state-of-the-art performance by achieving an average relative 5.0% WER reduction and substantial improvements in BLEU scores for speech and translation tasks. On zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with 15.5% to 27.6% relative WER reduction in the Hyporadise benchmark. NeKo performs competitively on grammar and post-OCR correction as a multi-task model.",
  "summary": "This paper introduces NEKO, a multi-task generative error correction model for post-processing outputs from ASR, MT, ST, OCR, and TEC systems. It leverages a Mixture-of-Experts (MoE) architecture, where each expert specializes in a specific task during training.\n\nFor LLM-based multi-agent systems, the key takeaway is the use of MoE to manage distinct expert agents, each trained on a different modality or task. This specialization allows knowledge sharing between agents through the router while maintaining task-specific expertise, potentially creating a more robust and generalizable system compared to training a single massive model for all tasks.  This work demonstrates the viability of using task-oriented MoE training with LLMs in a multi-agent context for specialized output correction across several application areas, and it achieves state-of-the-art results on standard benchmarks such as ASR and speech translation tasks.",
  "takeaways": "This paper introduces NEKO, a multi-task generative error correction model using a Mixture of Experts (MoE) approach. Here are practical examples of how a JavaScript developer could apply these insights to LLM-based multi-agent AI projects, focusing on web development scenarios:\n\n**1. Collaborative Writing/Editing Web App:**\n\n* **Scenario:** Imagine a Google Docs-like application where multiple users collaborate on a document. Each user acts as an \"agent\" with an LLM assisting their writing.  Errors in grammar, style, or factual accuracy can arise.\n* **NEKO Application:** Integrate a NEKO-inspired MoE system.  Each expert within the MoE focuses on a specific type of correction (e.g., grammar, style, fact-checking). As users type, the router directs segments of text to the appropriate expert for real-time correction suggestions.\n* **JavaScript Implementation:**\n    * Frontend: Use a rich text editor library like ProseMirror, CKEditor, or Quill.  As the user types, send text segments to a backend server.\n    * Backend: Implement the MoE routing logic using Node.js and TensorFlow.js or a similar library for interacting with pre-trained LLMs. Return correction suggestions to the frontend.\n    * Example:\n        ```javascript\n        // Frontend (using a hypothetical library for interacting with the backend)\n        editor.on('textChange', (textSegment) => {\n          nekoBackend.getCorrections(textSegment, 'grammar').then(suggestions => {\n            displaySuggestions(suggestions);\n          });\n        });\n        ```\n\n**2. Multi-Agent Chat Application:**\n\n* **Scenario:** A customer service chatbot interacts with multiple specialized LLMs (agents) – one for product information, another for order status, and a third for technical support. The chatbot needs to route user queries effectively and correct any misinterpretations.\n* **NEKO Application:** The chatbot acts as the router in a MoE system. It analyzes the user's input and routes it to the appropriate expert LLM. NEKO's generative correction capability helps refine user queries or clarify ambiguous requests before sending them to the expert.\n* **JavaScript Implementation:**\n    * Frontend: Use a chatbot UI library.\n    * Backend:  Use Node.js to handle incoming messages. Implement the routing and correction logic using a library like Langchain.js to manage interactions with multiple LLMs.\n    * Example (simplified Langchain.js integration):\n        ```javascript\n        const chain = new LLMChain({ llm: routerLlm }); // Router LLM\n        const expertChains = {\n          product: new LLMChain({ llm: productLlm }),\n          order: new LLMChain({ llm: orderLlm }),\n          support: new LLMChain({ llm: supportLlm })\n        };\n\n        chain.call({ input: userMessage }).then(result => {\n          const expert = result.text; // Router's decision\n          expertChains[expert].call({ input: correctedUserMessage});\n        });\n        ```\n\n**3. Real-time Multilingual Translation App:**\n\n* **Scenario:** A web app provides real-time translation of user input across multiple languages.\n* **NEKO Application:**  A MoE system with experts specializing in different language pairs handles the translation. NEKO's error correction capabilities ensure accurate translations even with noisy input (like spoken language transcriptions).\n* **JavaScript Implementation:** Similar to the chat application example, but the expert LLMs would be language translation models.\n\n**Key Considerations for JavaScript Developers:**\n\n* **LLM Integration:** Use libraries like Langchain.js or Hugging Face's `transformers.js` to interact with LLMs from your JavaScript code.\n* **MoE Implementation:** While dedicated MoE libraries in JavaScript are still emerging, you can adapt existing deep learning frameworks like TensorFlow.js to create custom MoE architectures.\n* **Routing Logic:** Carefully design the routing algorithm in your MoE. This is crucial for directing inputs to the correct expert. Consider using a separate LLM for routing, as suggested by the NEKO paper.\n* **Performance:**  MoE models can be computationally expensive. Consider optimization techniques and backend infrastructure to ensure responsive performance in web applications.\n\n\nBy adapting NEKO's core concepts, JavaScript developers can build more robust and intelligent multi-agent applications that effectively handle errors and improve user experience. These examples demonstrate the practical value of multi-agent AI research in advancing web technologies.",
  "pseudocode": "There are two pseudocode blocks described as mathematical formulas in the paper: equation (1) and equation (2). Here are their JavaScript implementations and explanation:\n\n**Equation (1):  MoE Layer Output**\n\n```javascript\nfunction moeLayerOutput(inputToken, experts, gatingNetwork) {\n  let output = 0;\n  for (let i = 0; i < experts.length; i++) {\n    const expertWeight = gatingNetwork(inputToken)[i];\n    const expertOutput = experts[i](inputToken);\n    output += expertWeight * expertOutput; \n  }\n  return output;\n}\n```\n\n* **Explanation:** This function calculates the output of a Mixture-of-Experts (MoE) layer. It takes the `inputToken`, an array of `experts` (functions that process the token), and a `gatingNetwork` (a function that assigns weights to experts based on the input token).  It iterates through the experts, weighting each expert's output by the value assigned by the gating network, and sums the results. This weighted sum represents the combined output of the MoE layer.\n\n\n**Equation (2): Gating Network (Router)**\n\n```javascript\nfunction gatingNetwork(inputToken, numExperts, topK) {\n\n  function softmax(logits) {\n    const maxLogit = Math.max(...logits);\n    const expLogits = logits.map(l => Math.exp(l - maxLogit));\n    const sumExp = expLogits.reduce((a, b) => a + b);\n    return expLogits.map(el => el / sumExp);\n\n  }\n\n  function topKlogits(logits, k) {\n      const sortedIndices = Array.from(logits.keys()).sort((a, b) => logits[b] - logits[a]);\n      const topKIndices = sortedIndices.slice(0, k);\n\n\n      const topKLogits = new Array(logits.length).fill(-8);\n      topKIndices.forEach(i => {\n        topKLogits[i] = logits[i];\n      })\n      return topKLogits;\n\n  }\n\n\n  // Simplified dot product (assuming inputToken and Wg are compatible) - replace with appropriate logic\n  const logits = inputToken.map((value, index) => value * wg[index]).reduce((a, b) => a + b);;\n\n  const topKResults = topKlogits(logits, topK)\n\n  return softmax(topKResults);\n\n}\n\n\n\n\n//Example wg and input (replace with your actual implementation)\n\nconst wg = [0.1,0.2,0.3,0.4] // Example wg matrix\nconst inputToken = [1,2,3,4]\n\n\nconst numExperts = 4; // Example\nconst topK = 2;// Example\nconst gatingWeights = gatingNetwork(inputToken, numExperts, topK);\n\nconsole.log(gatingWeights)\n\n```\n\n* **Explanation:** This function represents the gating network or router. It receives `inputToken`, `numExperts`, and `topK` as arguments.  It calculates `logits` using a simplified dot product (in a real application, this would be a matrix multiplication and more sophisticated logic).  Then it extracts the `topK` logits via `topKlogits`.  The `softmax` function normalizes these top logits into probabilities.  This output array of probabilities, returned by the function, decides which experts are selected for processing the `inputToken`.\n\n\n\n**Note:**  These JavaScript snippets illustrate the core logic.  In a real LLM implementation using a framework like TensorFlow.js or PyTorch.js, you would replace the simplified math operations with appropriate tensor operations provided by the framework for efficiency. Additionally,  the example `wg` and `inputToken` have been simplified for demonstration. In reality, these would be multi-dimensional arrays representing the input data and the gating network weights.",
  "simpleQuestion": "How to train an LLM for multi-task correction?",
  "timestamp": "2024-11-12T06:05:51.829Z"
}