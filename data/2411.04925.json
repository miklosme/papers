{
  "arxivId": "2411.04925",
  "title": "STORYAGENT: Customized STORYTELLING VIDEO GENERATION VIA MULTI-AGENT COLLABORATION",
  "abstract": "The advent of AI-Generated Content (AIGC) has spurred research into automated video generation to streamline conventional processes. However, automating storytelling video production, particularly for customized narratives, remains challenging due to the complexity of maintaining subject consistency across shots. While existing approaches like Mora and AesopAgent integrate multiple agents for Story-to-Video (S2V) generation, they fall short in preserving protagonist consistency and supporting Customized Storytelling Video Generation (CSVG). To address these limitations, we propose StoryAgent, a multi-agent framework designed for CSVG. StoryAgent decomposes CSVG into distinct subtasks assigned to specialized agents, mirroring the professional production process. Notably, our framework includes agents for story design, storyboard generation, video creation, agent coordination, and result evaluation. Leveraging the strengths of different models, StoryAgent enhances control over the generation process, significantly improving character consistency. Specifically, we introduce a customized Image-to-Video (I2V) method, LoRA-BE, to enhance intra-shot temporal consistency, while a novel storyboard generation pipeline is proposed to maintain subject consistency across shots. Extensive experiments demonstrate the effectiveness of our approach in synthesizing highly consistent storytelling videos, outperforming state-of-the-art methods. Our contributions include the introduction of StoryAgent, a versatile framework for video generation tasks, and novel techniques for preserving protagonist consistency.",
  "summary": "This paper introduces StoryAgent, a multi-agent system for creating customized storytelling videos.  It uses several specialized AI agents working together, including a story designer, storyboard generator, video creator, agent manager, and an observer.  The system takes a text prompt and reference videos of a subject as input and generates a video featuring that subject acting out the story. Key to LLM-based multi-agent systems is the use of LLMs like GPT-4 for tasks such as story design, agent coordination (managing which agent acts when), and result evaluation.  Novel techniques are introduced to maintain subject consistency across video shots, addressing limitations of current methods.  One such technique, LoRA-BE, customizes an existing image-to-video model for improved subject fidelity.  The storyboard generation uses a \"remove and redraw\" method to ensure subject consistency across storyboard frames.",
  "takeaways": "This paper presents exciting possibilities for JavaScript developers working with LLM-based multi-agent systems. Here's how a JavaScript developer can apply these insights, focusing on web development scenarios:\n\n**1. Building a Multi-Agent Storyboard Generator:**\n\n* **Concept:**  The paper's StoryAgent architecture and storyboard generation pipeline (generation, removal, redrawing) are directly applicable.  Imagine a web app where users provide a story prompt and reference images of a character. JavaScript agents can collaborate to generate a storyboard.\n* **Implementation:**\n    * **LLM Integration:** Use a JavaScript library like `langchain.js` to interface with LLMs like GPT-4 for story design (generating shot descriptions) and initial storyboard generation via text-to-image.\n    * **Image Manipulation:** Libraries like `fabric.js` or `konva.js` can handle the \"removal\" step (masking the subject) and redrawing (integrating the user's character into the generated images).  Consider exploring open-source segmentation models convertible to JavaScript using tools like `ONNX.js` or `TensorFlow.js`.\n    * **Agent Coordination:** Implement the Agent Manager using a message queue system (like Redis, RabbitMQ, accessed via respective JS libraries) or a peer-to-peer framework like WebRTC for direct agent communication. This manages the workflow (story design -> initial storyboard -> segmentation -> redrawing).\n\n**2. Creating a Customized Video Animation Tool:**\n\n* **Concept:** The LoRA-BE method for customized image animation can be implemented in a JavaScript environment for character animation based on user-provided storyboards.\n* **Implementation:**\n    * **LoRA-BE in JavaScript:** While directly porting LoRA-BE might be challenging, similar concepts can be adapted.  Explore TensorFlow.js or WebGPU for implementing attention mechanisms and LoRA within a browser environment. Focus on fine-tuning smaller, more manageable models suited for web execution.\n    * **I2V with Diffusion Models:** While computationally intensive, simplified diffusion models can be implemented (or pre-trained models accessed via API) for video generation from storyboard frames in JavaScript.  Again, `TensorFlow.js` or `WebGPU` will be crucial.\n    * **Frontend Integration:**  The animation tool can be integrated into a web frontend using frameworks like React, Vue, or Svelte.  Users upload their storyboards, and the JavaScript agents process them to generate the animation.\n\n**3. Interactive Storytelling Web Application:**\n\n* **Concept:** Combine the storyboard generator and animation tool to build a complete interactive storytelling application.\n* **Implementation:**\n    * **User Interface:** Design a user-friendly interface allowing users to input story ideas, upload character references, and control the video generation process.\n    * **Backend Agents:** The agents for story design, storyboard creation, and video generation can run on a server (Node.js).  Client-side JavaScript manages user interaction and communicates with the backend agents.\n    * **Real-Time Feedback:**  Consider integrating real-time feedback mechanisms where users can refine the generated storyboards or animation parameters, engaging in a collaborative storytelling process.\n\n**4. Experimental Projects:**\n\n* **Simplified Agent System:**  Start with a simplified version of StoryAgent using a few JavaScript agents communicating via simple message passing. Focus on prototyping the core multi-agent interaction flow.\n* **Customized Image Generation:** Experiment with image customization techniques inspired by AnyDoor using existing JavaScript image manipulation libraries.\n* **Web-Based Animation:** Explore existing JavaScript animation libraries and frameworks to prototype simple character animation based on generated storyboards.\n\n**Key Libraries and Frameworks:**\n\n* **LLM Interaction:** `langchain.js`\n* **Image Manipulation:** `fabric.js`, `konva.js`, `OpenCV.js`\n* **Machine Learning:** `TensorFlow.js`, `ONNX.js`, `WebGPU`\n* **Frontend Frameworks:** React, Vue, Svelte\n* **Backend:** Node.js\n* **Message Queues:** Libraries for Redis, RabbitMQ, etc.\n* **Peer-to-Peer Communication:** WebRTC\n\n\nBy breaking down the research concepts into these practical steps and leveraging existing JavaScript tools, developers can begin experimenting with and implementing LLM-based multi-agent systems for innovative web applications. Remember to focus on adapting the core concepts to web-friendly implementations, given current browser and hardware limitations. This area is rapidly evolving, offering exciting opportunities for JS developers.",
  "pseudocode": "```javascript\n// Equation (1): Denoising process in the Video Creator (LoRA-BE)\nfunction denoiseVideo(zt, encodedInputImage, encodedTextPrompt, encodedCondImage, timestep) {\n  // Concatenate the noisy video frame latent codes and the encoded input image\n  const concatenatedInput = tf.concat([zt, encodedInputImage], /*axis=*/1); // Assuming tf.js is used for tensor operations.\n\n  // Pass the concatenated input, encoded text prompt, and encoded conditional image through the U-Net\n  const unetOutput = unet.predict([concatenatedInput, encodedTextPrompt, encodedCondImage]);\n\n  // Apply the backward process (LDM's reverse diffusion) to denoise the noisy video frame\n  const ztMinus1 = backwardProcess(unetOutput, zt, timestep);\n\n  return ztMinus1;\n}\n\n\n\n// Equation (2): Loss function for training the Video Creator (LoRA-BE)\nfunction calculateLoss(epsilon, unetOutput, similarityMap, subjectMask) {\n  // Calculate LDM loss (standard diffusion loss)\n  const ldmLoss = tf.losses.meanSquaredError(epsilon, unetOutput);\n\n  // Calculate Localization Loss\n  const maskedSimilarity = similarityMap.mul(subjectMask); // Element-wise multiplication.\n  const localizationLoss = tf.mean(maskedSimilarity);\n\n  // Combine the losses. Hyperparameters for weighting the localization loss might need adjustment\n  const totalLoss = ldmLoss.sub(localizationLoss);\n\n  return totalLoss;\n}\n\n```\n\n**Explanation of the Algorithms and their Purpose:**\n\nThe provided JavaScript code implements the core logic described in equations (1) and (2) of the research paper, specifically focusing on the Video Creator agent and its LoRA-BE customized image animation method.\n\n**1. `denoiseVideo` Function:**\n\n* **Purpose:** This function implements the denoising process within the I2V generation using the Latent Diffusion Model (LDM). It takes a noisy video frame's latent representation (`zt`), encoded input image, text prompt, and conditional image, along with the current timestep, and returns a denoised version of the latent video frame (`ztMinus1`).\n* **Algorithm:** The function concatenates the noisy video frame and the encoded input image.  It then passes this concatenated input, along with the encoded text prompt and conditional image, through the U-Net model. Finally, it utilizes the `backwardProcess` function (which represents LDM's reverse diffusion process) to obtain the denoised latent video frame.  This process refines the video frame towards a cleaner representation guided by the input image and text prompt.\n\n**2. `calculateLoss` Function:**\n\n* **Purpose:** This function calculates the loss used to train the Video Creator agent with the LoRA-BE customization. It combines the standard LDM loss with a localization loss that encourages the model to focus on the subject of interest.\n* **Algorithm:** The function calculates the standard LDM loss using mean squared error between the predicted noise (`unetOutput`) and the actual noise (`epsilon`). It then calculates a localization loss based on the similarity map between the subject's token embeddings and the latent video, weighted by the subject mask. This localization loss encourages the network to focus attention on the relevant areas of the video corresponding to the subject. The total loss is a combination of the LDM loss and the localization loss.\n\n\nThese two functions represent the crucial components of the LoRA-BE customized image animation method within the StoryAgent framework, implementing the denoising process and the training objective.  TensorFlow.js (`tf`) is assumed to be used for tensor operations, which is common in JavaScript-based machine learning.  The provided code offers a more concrete implementation of the abstract concepts presented in the research paper, making it directly applicable for JavaScript developers working on similar projects.  Note that placeholder functions like `unet.predict` and `backwardProcess` are used, assuming the developer would integrate their specific U-Net model and backward process implementation.",
  "simpleQuestion": "How to make consistent story videos with AI agents?",
  "timestamp": "2024-11-08T06:04:19.489Z"
}