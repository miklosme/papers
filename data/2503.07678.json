{
  "arxivId": "2503.07678",
  "title": "Using a single actor to output personalized policy for different intersections",
  "abstract": "Abstract\n\nRecently, with the development of Multi-agent reinforcement learning (MARL), adaptive traffic signal control (ATSC) has achieved satisfactory results. In traffic scenarios with multiple intersections, MARL treats each intersection as an agent and optimizes traffic signal control strategies through learning and real-time decision-making. To enhance the efficiency of training and deployment in large-scale intersection scenarios, existing work predominantly employs shared parameter methods. Considering that observation distributions of intersections might be different in real-world scenarios, shared parameter methods might lack diversity and thus lead to high generalization requirements in the shared-policy network. A typical solution is to increase the size of network parameters. However, simply increasing the scale of the network does not necessarily improve policy generalization, which is validated in our experiments. Moreover, practical traffic signal control systems must consider the deployment cost of decision devices. Accordingly, an approach that considers both the personalization of intersections and the efficiency of parameter sharing is required. To this end, we propose Hyper-Action Multi-Head Proximal Policy Optimization (HAMH-PPO), a Centralized Training with Decentralized Execution (CTDE) MARL method that utilizes a shared PPO policy network to deliver personalized policies for intersections with non-iid observation distributions. The centralized critic in HAMH-PPO uses graph attention units to calculate the graph representations of all intersections and outputs a set of value estimates with multiple output heads for each intersection. The decentralized execution actor takes the local observation history as input and output distributions of action as well as a so-called hyper-action to balance the multiple values estimated from the centralized critic to further guide the updating of TSC policies. The combination of hyper-action and multi-head values enables multiple agents to share a single actor-critic while achieving personalized policies. The effectiveness of HAMH-PPO is validated through extensive experiments on real-world and synthetic road network traffic.",
  "summary": "This paper addresses the challenge of efficiently controlling traffic signals in large-scale road networks using multi-agent reinforcement learning (MARL). Existing methods often rely on shared parameters across all intersections (agents), which can hinder performance when traffic patterns vary significantly between intersections. The proposed HAMH-PPO (Hyper-Action Multi-Head Proximal Policy Optimization) model aims to personalize policies for each intersection while maintaining efficient parameter sharing.  It employs a centralized critic that estimates multiple value functions for each intersection and a hyper-action mechanism in the actor network to combine these values based on intersection-specific preferences.\n\nKey points for LLM-based multi-agent systems:\n\n* **Personalized policies with shared parameters:** HAMH-PPO addresses the trade-off between personalization and efficiency in multi-agent systems, which is relevant for LLM agents that need to adapt to individual tasks while sharing core knowledge.\n* **Multi-head value estimation:** The use of multiple value functions provides a richer representation of potential outcomes for each agent, analogous to LLMs considering multiple perspectives or generating diverse responses.\n* **Hyper-action mechanism:** This mechanism allows agents to dynamically weight different value functions, similar to how LLMs can attend to different parts of their input or knowledge base.\n* **Centralized training with decentralized execution (CTDE):** This paradigm, common in MARL, offers potential benefits for LLM-based agents, enabling efficient learning through shared experience while allowing for independent action.\n* **Scalability:** HAMH-PPO demonstrates improved performance in large-scale scenarios, highlighting its potential applicability to complex multi-agent systems with numerous LLM agents.",
  "takeaways": "This paper presents HAMH-PPO, a novel approach for coordinating multiple reinforcement learning agents, specifically addressing the challenge of personalized policies within a shared model. This is highly relevant to JavaScript developers building LLM-based multi-agent applications for the web, where efficiency and personalized user experiences are paramount.  Here are practical examples illustrating how a JavaScript developer can leverage these insights:\n\n**1. Personalized Chatbot Interactions in a Customer Service Application:**\n\nImagine building a customer service web app with multiple LLM-powered chatbots, each handling different product categories or customer segments. Using a naive shared-parameter approach, all chatbots would provide similar responses, ignoring individual customer needs and product nuances.  HAMH-PPO offers a solution:\n\n* **JavaScript Implementation:** Utilize a JavaScript framework like TensorFlow.js or WebDNN to implement the core HAMH-PPO logic.\n* **Multi-Head Critic:** Design a multi-head critic network within your JavaScript model. Each head specializes in evaluating chatbot performance for a specific product category or customer segment (e.g., \"technical support,\" \"billing inquiries,\" \"product feedback\").\n* **Hyper-Action Network:**  Implement a hyper-action network that, given the conversation history and user context (e.g., past purchases, browsing history), dynamically weights the outputs of different critic heads. This allows the shared actor network (the LLM) to generate personalized responses tailored to the specific context.\n* **Example:** If a user with a history of technical issues initiates a chat, the hyper-action network would prioritize the \"technical support\" critic head, guiding the LLM to provide more technically-focused assistance.\n\n**2. Collaborative Content Creation with Multiple LLMs:**\n\nConsider a web app for collaborative story writing, where multiple LLMs contribute to a narrative based on user input. A shared model could lead to repetitive or inconsistent storylines. Applying HAMH-PPO:\n\n* **JavaScript Framework:** Leverage a framework like Node.js with a library for interacting with LLMs (e.g., LangchainJS).\n* **Personalized Story Arcs:** Train each LLM to specialize in a particular character arc, writing style, or genre. The multi-head critic evaluates the contribution of each LLM based on coherence, style, and user preferences.\n* **Hyper-Action for Narrative Control:** The hyper-action network dynamically balances the contributions of different LLMs based on the evolving narrative and user feedback. This ensures diverse and coherent storylines while maintaining personalization.\n* **Example:** If a user steers the story towards a mystery, the hyper-action network prioritizes the LLM specialized in mystery writing, shaping the narrative accordingly.\n\n**3. Multi-Agent Game Development:**\n\nDevelop a real-time strategy game where multiple LLM-controlled units collaborate under player command.  A basic shared model could lead to predictable unit behavior. With HAMH-PPO:\n\n* **Game Engine Integration:** Integrate HAMH-PPO within a JavaScript game engine like Phaser or Babylon.js.\n* **Specialized Unit Roles:** Train each LLM to control a specific unit type (e.g., scout, warrior, healer). The multi-head critic evaluates unit performance based on metrics like combat effectiveness, resource gathering, or support provided.\n* **Hyper-Action for Dynamic Strategy:**  The hyper-action network dynamically adjusts the influence of each critic head based on the game state (e.g., enemy positions, resource availability, player commands). This enables adaptive unit behavior, personalized tactics, and more engaging gameplay.\n* **Example:**  If the player orders an attack, the hyper-action network prioritizes the combat effectiveness critic heads, leading to more aggressive unit behavior.\n\n\n**Key Considerations for JavaScript Developers:**\n\n* **Scalability:** Consider optimizations for running HAMH-PPO efficiently in a browser environment.\n* **Library Integration:** Explore existing JavaScript libraries for reinforcement learning and LLM interaction (e.g., TensorFlow.js, WebDNN, LangchainJS).\n* **User Interface:** Design intuitive interfaces for users to provide feedback and influence the behavior of multi-agent systems.\n\nBy adopting the core concepts of HAMH-PPO, JavaScript developers can unlock the potential of LLM-based multi-agent systems to create innovative and personalized web experiences.  This opens up exciting possibilities for dynamic user interfaces, collaborative content creation, and sophisticated game development.",
  "pseudocode": "```javascript\n// JavaScript implementation of the HAMH-PPO algorithm (Algorithm 1)\n\nasync function hamhPPO(K, N, T, E) {\n  // Input:\n  //   K: Episodes\n  //   N: Number of agents (intersections)\n  //   T: Time steps per episode\n  //   E: Epochs\n  // Output:\n  //   Trained critic and actor networks (theta and phi, respectively)\n\n  // 1. Initialize networks and replay buffer\n  let phi = initializeCriticNetwork(); // Initialize critic network\n  let theta = initializeActorNetwork(); // Initialize actor network\n  let D = []; // Replay buffer\n\n  // 2. Main training loop over episodes\n  for (let k = 0; k < K; k++) {\n    // 3. Reset environment\n    resetSimulatorEnvironment();\n\n    // 4. Set data buffer\n    D = [];\n\n    // 5. Collect trajectories\n    let trajectories = [];\n    for (let t = 0; t < T; t++) {\n      let observations = [];\n      let actions = [];\n      let nextObservations = [];\n      for (let i = 0; i < N; i++) {\n        let observation = getObservation(i);\n        let action = selectAction(theta, observation, i); // Use actor network to select action\n        applyAction(action, i); // Apply action to the environment\n        let nextObservation = getObservation(i);\n\n        observations.push(observation);\n        actions.push(action);\n        nextObservations.push(nextObservation);\n      }\n      trajectories.push({ observations, actions, nextObservations });\n    }\n\n    // 6. Store trajectories in replay buffer\n    D = trajectories;\n\n\n    // 7. Training loop over epochs\n    for (let e = 0; e < E; e++) {\n      // 8. Update actor network\n      theta = updateActorNetwork(theta, D, phi);  // Eq. 6\n\n      // 9. Update critic network\n      phi = updateCriticNetwork(phi, D);  // Eq. 8\n    }\n  }\n\n  return { theta, phi };\n}\n\n\n\n// Helper functions (placeholders, need actual implementations based on the paper)\n\nfunction initializeCriticNetwork() {\n  // Initialize the critic network (Multi-head structure with GAT layers)\n  // ...\n  return criticNetwork;\n}\n\nfunction initializeActorNetwork() {\n  // Initialize the actor network (GRU-based with hyperaction output)\n  // ...\n  return actorNetwork;\n}\n\nfunction resetSimulatorEnvironment() {\n  // Reset the CityFlow simulator environment\n  // ...\n}\n\n\nfunction getObservation(agentIndex) {\n  // Get observation from the environment for the given agent (intersection)\n  // ...\n  return observation;\n}\n\n\nfunction selectAction(theta, observation, agentIndex) {\n  // Use the actor network (theta) to select an action based on the observation \n  // and agent index (for hyperaction calculation)\n  // ...\n  return action;\n}\n\n\nfunction applyAction(action, agentIndex) {\n  // Apply the chosen action to the environment for the given agent (intersection)\n  // ...\n}\n\nfunction updateActorNetwork(theta, D, phi) {\n  // Update the actor network parameters (theta) using the PPO loss and entropy regularization (Eq. 6)\n  // ...\n  return updatedTheta;\n}\n\nfunction updateCriticNetwork(phi, D) {\n  // Update the critic network parameters (phi) using the TD squared error loss (Eq. 8)\n  // ...\n  return updatedPhi;\n}\n\n\n\n// Example usage (async/await is used because hamhPPO is async)\nasync function runExample() {\n  const K = 1000; // Number of episodes\n  const N = 12;   // Number of intersections (agents)\n  const T = 360;  // Time steps per episode\n  const E = 15;  // Epochs\n\n  const { theta, phi } = await hamhPPO(K, N, T, E); \n  console.log(\"Training complete. Trained actor and critic:\", theta, phi);\n}\n\nrunExample();\n\n```\n\n**Explanation of the HAMH-PPO Algorithm and its JavaScript Implementation:**\n\nThe Hyper-Action Multi-Head Proximal Policy Optimization (HAMH-PPO) algorithm is designed for large-scale traffic signal control. It addresses the challenge of personalized control for different intersections while maintaining efficient training through parameter sharing.\n\n**Purpose:** To optimize traffic flow by adaptively controlling traffic signals at multiple intersections, considering the individual characteristics of each intersection.\n\n**Key Concepts:**\n1. **Centralized Training with Decentralized Execution (CTDE):**  All agents (intersections) share the same actor and critic networks, but each agent acts independently based on its local observations during execution.\n\n2. **Multi-Head Critic:** The critic network outputs multiple value estimations for each intersection, capturing the diverse needs of different traffic situations.\n\n3. **Hyper-action:**  The actor network outputs a hyper-action, a probability distribution, that weighs the importance of the multiple value estimations from the critic. This allows personalized policy learning for each intersection.\n\n4. **Proximal Policy Optimization (PPO):** This method stabilizes the policy learning by limiting policy updates, preventing drastic changes that could destabilize training.\n\n5. **Graph Attention Network (GAT):**  The critic uses a GAT to process the graph structure of the road network, capturing the spatial relationships between intersections.\n\n\nThe provided JavaScript code implements the core logic of the HAMH-PPO algorithm described in Algorithm 1 of the paper. The helper functions are placeholders, and you will need to implement them based on the details provided in the paper, including the network architectures (GRU for the actor, GAT and MLPs for the critic), PPO loss calculations, entropy regularization, and interaction with the CityFlow traffic simulator. The `async/await` syntax is used because the training process involves interactions with the environment (simulator), which are usually asynchronous operations.  It is crucial to replace the placeholder comments with the appropriate implementations from the paper to make the code fully functional.",
  "simpleQuestion": "Can one actor personalize policies for diverse intersections?",
  "timestamp": "2025-03-12T06:03:11.394Z"
}