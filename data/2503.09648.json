{
  "arxivId": "2503.09648",
  "title": "A Survey on Trustworthy LLM Agents: Threats and Countermeasures",
  "abstract": "With the rapid evolution of Large Language Models (LLMs), LLM-based agents and Multi-agent Systems (MAS) have significantly expanded the capabilities of LLM ecosystems. This evolution stems from empowering LLMs with additional modules such as memory, tools, environment, and even other agents. However, this advancement has also introduced more complex issues of trustworthiness, which previous research focusing solely on LLMs could not cover. In this survey, we propose the TrustAgent framework, a comprehensive study on the trustworthiness of agents, characterized by modular taxonomy, multi-dimensional connotations, and technical implementation. By thoroughly investigating and summarizing newly emerged attacks, defenses, and evaluation methods for agents and MAS, we extend the concept of Trustworthy LLM to the emerging paradigm of Trustworthy Agent. In TrustAgent, we begin by deconstructing and introducing various components of the Agent and MAS. Then, we categorize their trustworthiness into intrinsic (brain, memory, and tool) and extrinsic (user, agent, and environment) aspects. Subsequently, we delineate the multifaceted meanings of trustworthiness and elaborate on the implementation techniques of existing research related to these internal and external modules. Finally, we present our insights and outlook on this domain, aiming to provide guidance for future endeavors. For easy reference, we categorize all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/Ymm-cll/TrustAgent.",
  "summary": "This paper surveys trustworthiness issues in Large Language Model (LLM)-based agents and Multi-Agent Systems (MAS). It introduces the TrustAgent framework, categorizing trustworthiness issues by agent module (brain, memory, tool) and interaction type (agent-to-agent, agent-to-environment, agent-to-user), covering attacks, defenses, and evaluations. Key points for LLM-based multi-agent systems include: (1) Novel attack vectors like infectious attacks spreading through MAS, prompt injection in multi-turn dialogues exploiting memory, backdoors triggered by multi-agent interactions, and tool manipulation for external attacks. (2) Defense strategies leveraging multi-agent collaboration for alignment, filtering, and topological defenses against attack propagation. (3)  The need for dynamic evaluations reflecting complex agent-environment interactions and the importance of considering multi-agent trust dynamics beyond individual agent behavior.",
  "takeaways": "This paper provides a comprehensive overview of trustworthiness issues in LLM-based multi-agent systems, offering several practical insights for JavaScript developers. Here's how a JavaScript developer can apply these insights to their projects:\n\n**1. Intrinsic Trustworthiness (Brain, Memory, Tool):**\n\n* **Brain (LLM):**\n    * **Jailbreak Defense:** Implement input validation using JavaScript and regex to filter potentially harmful prompts.  Consider using libraries like `validator.js` for complex validation rules. Example: prevent prompts containing specific keywords known for triggering jailbreaks.  Additionally, explore using a smaller LLM on the client-side as a filter (Single-model Filter) to pre-process user input before sending it to the main LLM.\n    * **Prompt Injection Defense:** Sanitize user inputs thoroughly before including them in prompts. Use DOMPurify or similar libraries to prevent script injection attacks in web interfaces.  For example, if user input is used to construct a database query within a prompt, sanitize it to avoid malicious SQL injections.\n    * **Backdoor Detection:** Regularly analyze LLM outputs for unexpected behavior or responses (Focused Assessment). Implement logging mechanisms in JavaScript to track interactions and analyze them for suspicious patterns using libraries like `ml5.js` for basic anomaly detection or connecting to server-side analytics pipelines.\n\n* **Memory:**\n    * **Memory Poisoning Defense:**  Implement checksums or digital signatures for data stored in your vector database (Detection). Verify these signatures in your JavaScript code before including data in prompts to ensure integrity.  Consider using libraries like `js-sha256` for hashing.\n    * **Privacy Leakage Defense:** Encrypt sensitive data before storing it in your vector database. Use client-side encryption libraries like `crypto-js` or `forge` to protect user data. Implement access control mechanisms in JavaScript to restrict access to memory functions based on user roles.\n    * **Memory Misuse Defense:** Limit the number of turns in multi-turn conversations (Output Intervention) to mitigate risks associated with malicious exploitation of short-term memory.  Implement this as a counter in your JavaScript application logic.\n\n* **Tool:**\n    * **Tool Manipulation Defense:** Create a whitelist of approved tools and APIs that the agent can access (Based on Tool Description).  Enforce this restriction using JavaScript logic. Validate all tool parameters before execution using `joi` or similar schema validation libraries.\n    * **Tool Abuse Defense:**  Use sandboxing techniques (Sandbox Simulation) to test the agent's interaction with external tools in a controlled environment. Browser-based JavaScript sandboxes or containerization technologies (Docker, if using Node.js on the server-side) can help simulate real-world interactions without impacting production systems.\n\n**2. Extrinsic Trustworthiness (Agent-to-Agent, Agent-to-Environment, Agent-to-User):**\n\n* **Agent-to-Agent:**\n    * **Infectious Attack Defense:**  Isolate agents within separate containers or processes (Topological Defense) to limit the spread of malicious behavior.  This can be implemented using iframes in the browser or separate Node.js processes on the server-side.\n    * **Cooperative Attack Defense:** Implement consensus mechanisms (Collaborative Defense) where multiple agents vote or debate on an action before execution.  This could involve a voting system implemented in JavaScript or more advanced techniques involving blockchain.\n\n* **Agent-to-Environment:**\n    * **Physical/Digital Environment Defense:**  Implement robust error handling and fallback mechanisms in your JavaScript code to address unexpected environmental feedback.  Monitor API calls and environment interactions using logging and analytics tools.  For web agents, use browser APIs to monitor network connectivity and handle interruptions gracefully.\n\n* **Agent-to-User:**\n    * **Trust Mechanisms:** Provide users with transparency into the agent's reasoning and actions. Use JavaScript to display explanations or visualizations of the agent's thought process.  Implement feedback mechanisms so users can flag potential issues.\n\n**JavaScript Frameworks and Libraries:**\n\n* **Node.js:** For server-side logic, including agent communication and interaction with external APIs and databases.\n* **React, Vue, Angular:** For building interactive web interfaces for multi-agent applications.\n* **Socket.IO:** For real-time communication between agents and user interfaces.\n* **LangChain.js:** For LLM integration and chain-of-thought reasoning.\n* **LlamaIndex:** For data augmentation, indexing, and querying.\n\n\nBy focusing on these defenses and applying relevant JavaScript techniques, developers can significantly improve the trustworthiness of their LLM-based multi-agent systems, enabling safer and more reliable web applications.  Remember that this is a rapidly evolving field, so staying up-to-date with the latest research and best practices is crucial.",
  "pseudocode": "No pseudocode block found.",
  "simpleQuestion": "How can I build trustworthy LLM agents?",
  "timestamp": "2025-03-14T06:01:59.223Z"
}