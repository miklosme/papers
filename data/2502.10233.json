{
  "arxivId": "2502.10233",
  "title": "Learning to Solve the Min-Max Mixed-Shelves Picker-Routing Problem via Hierarchical and Parallel Decoding",
  "abstract": "Abstract. The Mixed-Shelves Picker Routing Problem (MSPRP) is a fundamental challenge in warehouse logistics, where pickers must navigate a mixed-shelves environment to retrieve SKUs efficiently. Traditional heuristics and optimization-based approaches struggle with scalability, while recent machine learning methods often rely on sequential decision-making, leading to high solution latency and suboptimal agent coordination. In this work, we propose a novel hierarchical and parallel decoding approach for solving the min-max variant of the MSPRP via multi-agent reinforcement learning. While our approach generates a joint distribution over agent actions, allowing for fast decoding and effective picker coordination, our method introduces a sequential action selection to avoid conflicts in the multi-dimensional action space. Experiments show state-of-the-art performance in both solution quality and inference speed, particularly for large-scale and out-of-distribution instances. Our code is publicly available at http://github.com/LTluttmann/marl4msprp",
  "summary": "This paper introduces MAHAM (Multi-Agent Hierarchical Attention Model), a new algorithm for optimizing picker routes in mixed-shelves warehouses.  The goal is to minimize the longest route taken by any picker (min-max objective) which improves overall order fulfillment time.\n\nKey points for LLM-based multi-agent systems:\n\n* **Hierarchical and parallel decoding:** MAHAM decides on locations and items to pick simultaneously for all agents, enabling coordination. This is unlike typical autoregressive models that plan sequentially for each agent.\n* **Sequential action selection within parallel decoding:**  While decisions are made in parallel, actions are executed sequentially to avoid conflicts (e.g., two pickers trying to grab the same item). This is managed via a learned ranking of which agents should act first.\n* **Agent context encoder:** Encodes picker information like location, remaining capacity, and current tour length, enhancing decision-making. Includes a ranking-based positional encoding and self-attention for inter-agent communication.\n* **Parameter sharing:**  Increases efficiency and improves generalization by reusing parameters in the encoder's cross-attention mechanism.\n* **Self-supervised learning:**  The model trains on pseudo-optimal solutions generated by its own best performance, progressively improving over time.",
  "takeaways": "This research paper presents a novel approach to solving multi-agent routing problems, specifically the Min-Max Mixed-Shelves Picker Routing Problem (MSPRP), using hierarchical and parallel decoding. While the paper's context is warehouse logistics, its core concepts are highly relevant to JavaScript developers building LLM-based multi-agent applications, especially in web development.\n\nHere are some practical examples illustrating how JavaScript developers can apply the insights from this paper:\n\n**1. Collaborative Web Editing:**\n\nImagine a collaborative document editing platform similar to Google Docs. Multiple users (agents) can edit the same document concurrently.  The challenge is to manage conflicts and ensure a consistent final document. This paper's sequential action selection algorithm can be applied here.\n\n* **JavaScript Implementation:** Developers can use a framework like Yjs or ShareDB to manage shared data. When multiple users attempt to edit the same section, a priority system based on the paper's sequential action selection can determine which edit takes precedence. This priority could be determined by factors like user roles, timestamps, or even LLM-based predictions of edit quality.\n\n```javascript\n// Simplified example using Yjs and a priority function\nimport * as Y from 'yjs'\n\nconst ydoc = new Y.Doc()\nconst ytext = ydoc.getText('myText')\n\nfunction priority(userId) { \n  // Implement priority logic based on user roles, timestamps etc.\n  // Higher value = higher priority\n  return userRoles[userId] * 10 + Date.now();\n}\n\nydoc.on('update', (update, origin) => {\n  // Apply updates based on priority\n  if (origin !== 'local') {  // Ignore local updates\n      const userId = update.origin // Assume update contains userId\n      // ... conflict resolution logic using 'priority(userId)' ...\n  }\n})\n```\n\n**2. Multi-Agent Chatbots for Customer Service:**\n\nConsider a customer service scenario where multiple specialized chatbots (agents) collaborate to answer complex user queries. One chatbot might handle product information, another shipping, and a third returns. The paper's hierarchical decoding approach is relevant here.\n\n* **JavaScript Implementation:**  Node.js with a message broker (e.g., Redis, RabbitMQ) can be used. Each chatbot acts as an agent listening for specific message types. An orchestrator bot, inspired by the hierarchical decoder, can route incoming user queries to the appropriate chatbot(s) based on the query's content, using LLMs to analyze and classify the message.\n\n```javascript\n// Simplified example using Redis pub/sub\nconst redis = require('redis');\n\n// Chatbots subscribe to relevant channels\nconst productBot = redis.createClient();\nproductBot.subscribe('product_info');\n\n// Orchestrator bot receives all messages and routes\nconst orchestrator = redis.createClient();\norchestrator.subscribe('user_queries');\n\norchestrator.on('message', (channel, message) => {\n  // Use LLM to analyze message and route\n  const topic = analyzeQuery(message); // LLM-powered analysis\n  orchestrator.publish(topic, message);\n}); \n```\n\n\n**3. Distributed Task Management in Web Apps:**\n\nImagine a complex web application that distributes tasks among multiple serverless functions (agents). The challenge is to optimize task allocation and minimize overall execution time. The min-max objective of the MSPRP is directly applicable.\n\n* **JavaScript Implementation:** Frameworks like Serverless or AWS Lambda can be used. A central scheduler, similar to the paper's decoding mechanism, can analyze incoming tasks using LLMs to estimate their complexity and resource requirements. Tasks can then be allocated to serverless functions, aiming to minimize the maximum execution time among all functions, similar to the min-max objective.\n\n\n**Key JavaScript Libraries & Frameworks:**\n\n* **Yjs/ShareDB:** For real-time collaborative data management.\n* **Node.js with message brokers (Redis, RabbitMQ, Kafka):**  For multi-agent communication and coordination.\n* **Serverless/AWS Lambda/Azure Functions:** For distributed task management.\n* **Langchain/LlamaIndex:** For integrating and leveraging LLMs in the multi-agent system.\n\n\n**Summary for JavaScript Developers:**\n\nThis paper provides valuable insights for building efficient LLM-based multi-agent systems in web development:\n\n* **Parallel Decoding:** Process agent actions concurrently to improve responsiveness and reduce latency.\n* **Hierarchical Decoding:** Decompose complex actions into smaller sub-actions for better organization and scalability.\n* **Sequential Action Selection:** Manage conflicts effectively and ensure coherent agent behavior.\n* **Min-Max Objective:** Optimize for overall system performance by minimizing the maximum task completion time or resource usage among agents.\n\n\nBy understanding and applying these concepts, JavaScript developers can build more robust, efficient, and scalable multi-agent web applications leveraging the power of LLMs.  The paper's core ideas are transferable to various web development scenarios beyond the specific examples mentioned above.  Experimenting with these concepts using JavaScript and relevant web technologies is a great starting point for developers looking to advance the capabilities of multi-agent systems on the web.",
  "pseudocode": "```javascript\n// Algorithm 1: Sequential Action Selection from Joint Logit-Space\n\nfunction sequentialActionSelection(logits, mask, maskUpdateFunc, beta, defaultAction) {\n  let actions = Array(logits.length).fill(defaultAction); // Initialize agent actions\n  let M = mask.map(row => [...row]); // Create a copy of the mask\n\n  while (M.flat().some(element => element === 0)) { // While not all mask elements are 1\n    const L_prime = logits.map((row, i) => \n      row.map((value, j) => M[i][j] === 1 ? value : -Infinity)\n    ); // Mask infeasible actions\n\n\n    const probabilities = L_prime.map((row, agentIndex) =>\n      row.map((logit) => Math.exp(logit / beta)) // Normalize logits with temperature\n    );\n\n    const agentToAct = probabilities.findIndex((row, agentIndex) => M[agentIndex].includes(0)); // Find an agent with unmasked actions\n\n    if (agentToAct < 0) {\n      break; // All actions masked, terminate\n    }\n    \n    // Sample an action from available choices for selected agent\n    let availableActions = probabilities[agentToAct].map((_, index) => index).filter((actionIndex) => M[agentToAct][actionIndex] === 0);\n    let actionProbs = availableActions.map(actionIndex => probabilities[agentToAct][actionIndex]).map(v => Math.exp(v));\n    \n    let totalProb = actionProbs.reduce((a, b) => a + b, 0);\n    actionProbs = actionProbs.map(p => p / totalProb);\n    let cumulativeProb = 0;\n    let randomNum = Math.random();\n    let chosenActionIndex;\n\n    for(let i = 0; i < availableActions.length; i++){\n        cumulativeProb += actionProbs[i];\n        if (randomNum <= cumulativeProb){\n            chosenActionIndex = availableActions[i];\n            break;\n        }\n    }\n    actions[agentToAct] = chosenActionIndex; \n\n    M[agentToAct].fill(1);  // Mark all actions of the chosen agent as taken\n    M = maskUpdateFunc(M, actions); // Update the mask based on the selected action\n\n  }\n\n  return actions;\n}\n\n\n\n\n\n// Algorithm 2: Self-improvement training for neural CO\n\nasync function selfImprovementTraining(X, objectiveFunc, N, a, VALIDATION) {\n  let pi_theta = initializePolicy(); // Randomly initialize policy\n  let best = pi_theta;\n  let DATASET = [];\n\n\n  for (let epoch = 0; epoch < 50; epoch++) { // Limiting to 50 epochs for demonstration\n    const INSTANCES = sampleInstances(X, N);\n    \n    for (const x of INSTANCES) {\n        let A = [];\n        for (let i = 0; i < a; i++) {\n            A.push(sampleFeasibleSolutions(x, best)); // Assuming sampleFeasibleSolutions samples with best policy\n        }\n\n      const bestSolution = findBestSolution(A, objectiveFunc, x);\n      DATASET.push({ x, solution: bestSolution });\n    }\n\n\n    for (let batch = 0; batch * 2000 < DATASET.length; batch++) {\n      const batchData = DATASET.slice(batch * 2000, (batch+1) * 2000); // Adjust batch size as needed\n      let loss = 0;\n\n      for (const { x, solution } of batchData) {\n        const d_j = Array(solution.length).fill(0).map(() => Math.floor(Math.random() * (solution.length))); // Random d_j between 1 and T-1\n\n        for (let t = 0; t < solution.length; t++) {\n          loss -= Math.log(pi_theta(solution[t], solution.slice(0, t), x)); // Cross-entropy loss (assuming pi_theta can handle partial solutions)\n        }\n      }\n      loss /= batchData.length;\n      pi_theta = updatePolicy(pi_theta, loss); // Update pi_theta with gradient descent \n    }\n\n    if (await evaluatePolicy(pi_theta, VALIDATION, objectiveFunc) > await evaluatePolicy(best, VALIDATION, objectiveFunc)) {\n      best = pi_theta;\n      DATASET = [];\n    }\n  }\n\n  return best;\n}\n\n\n// Helper functions (placeholders - replace with actual implementations)\n\nfunction initializePolicy() { /* ... */ }\nfunction sampleInstances(X, N) { /* ... */ }\nfunction sampleFeasibleSolutions(x, policy) { /* ... */ }\nfunction findBestSolution(A, objectiveFunc, x) { /* ... */ }\nfunction updatePolicy(policy, loss) { /* ... */ }\nasync function evaluatePolicy(policy, validationSet, objectiveFunc) { /* ... */ }\n\n```\n\n\n**Algorithm 1 Explanation:**\n\nThis algorithm performs sequential action selection from a joint action space, ensuring feasibility. It's designed for multi-agent scenarios where agents' actions might conflict.  It starts with default actions, iteratively refines them based on logits (representing action preferences), and masks infeasible actions. The key idea is to sample actions one agent at a time, updating the feasibility mask after each selection to prevent conflicts.  The temperature parameter `beta` controls the randomness of the action sampling.\n\n**Algorithm 2 Explanation:**\n\nThis algorithm implements the self-improvement training method for combinatorial optimization. It iteratively improves a policy by sampling solutions, selecting the best ones, and training the policy to mimic these pseudo-optimal solutions using cross-entropy loss. A key feature is the use of a reference policy (best) that's updated when the trained policy outperforms it on a validation set. This creates progressively better training data. The parameters `N`, `a`, and `B` control the number of instances, candidate solutions, and batch size, respectively.",
  "simpleQuestion": "How can parallel decoding improve multi-agent warehouse routing?",
  "timestamp": "2025-02-17T06:03:12.987Z"
}