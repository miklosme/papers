{
  "arxivId": "2502.04180",
  "title": "Multi-agent Architecture Search via Agentic Supernet",
  "abstract": "Large Language Model (LLM)-empowered multi-agent systems extend the cognitive boundaries of individual agents through disciplined collaboration and interaction, while constructing these systems often requires labor-intensive manual designs. Despite the availability of methods to automate the design of agentic workflows, they typically seek to identify a static, complex, one-size-fits-all system, which, however, fails to dynamically allocate inference resources based on the difficulty and domain of each query. To address this challenge, we shift away from the pursuit of a monolithic agentic system, instead optimizing the agentic supernet, a probabilistic and continuous distribution of agentic architectures. We introduce MaAS, an automated framework that samples query-dependent agentic systems from the supernet, delivering high-quality solutions and tailored resource allocation (e.g., LLM calls, tool calls, token cost). Comprehensive evaluation across six benchmarks demonstrates that MaAS (I) requires only 6~45% of the inference costs of existing handcrafted or automated multi-agent systems, (II) surpasses them by 0.54% ~ 11.82%, and (III) enjoys superior cross-dataset and cross-LLM-backbone transferability. The code will be available at https://github.com/bingreeky/MaAS.",
  "summary": "This paper introduces MaAS, a new method for designing LLM-based multi-agent systems. Instead of building a single, fixed system, MaAS creates a \"supernet,\" a distribution of possible multi-agent architectures.  This allows it to dynamically sample a tailored multi-agent system for each individual task, optimizing for both performance and resource efficiency (like the number of LLM calls and token cost).  Key points for LLM-based multi-agent systems are its dynamic architecture sampling based on task complexity, improved performance and resource efficiency compared to fixed systems, transferability across LLMs and datasets, and the ability to generalize to unseen agent operators.",
  "takeaways": "This paper introduces MaAS, a novel approach to designing multi-agent systems for LLMs, shifting from a \"one-size-fits-all\" architecture to a dynamic, query-dependent system. Here are some practical examples of how JavaScript developers can apply these insights to LLM-based multi-agent AI projects in web development:\n\n**1. Dynamic Resource Allocation for a Chatbot:**\n\nImagine building a customer support chatbot.  Instead of a fixed chain of LLMs and tools, you could use the MaAS concept to dynamically allocate resources.  \n\n* **Scenario:** A simple question like \"What are your store hours?\" could be handled by a single zero-shot LLM call. A more complex query like \"I want to return a defective product I bought last month. What's the process?\" might require multiple agents: one for retrieving purchase history, one for analyzing return policies, and another for generating a personalized response.\n* **Implementation:**\n    * **Langchain.js:**  Use Langchain's chain and agent functionalities as building blocks for your multi-agent system.  Implement a controller (as described in the paper) using a smaller, more efficient LLM to analyze incoming queries and decide which agents to activate.\n    * **Custom Controller:**  Alternatively, create a custom controller in JavaScript. This controller can be rule-based (e.g., keyword detection) or use a smaller classification model to categorize queries based on complexity.\n* **Benefit:** Reduces LLM API costs and latency by avoiding unnecessary computations for simple queries.\n\n\n**2. Personalized Content Generation:**\n\nConsider a website that generates personalized stories or articles.  A MaAS-inspired approach can dynamically combine different LLM agents.\n\n* **Scenario:** User input like \"Write a short story about a dragon\" could trigger a basic story-generating agent. Input like \"Write a sci-fi short story about a dragon who is also a spaceship mechanic, with illustrations\" might involve an agent for sci-fi world-building, a character design agent, and an image generation agent.\n* **Implementation:**\n    * **Langchain.js + Transformers.js:** Combine Langchain for agent orchestration and Transformers.js for running smaller LLMs client-side for faster processing of simpler requests.\n    * **Serverless Functions:**  Use serverless functions (e.g., AWS Lambda, Google Cloud Functions) to deploy individual agents, triggered dynamically by the controller based on user input.\n* **Benefit:** Creates a more engaging user experience by tailoring the content generation process to the userâ€™s specific request, leveraging the power of multiple specialized LLMs without excessive overhead.\n\n**3. Interactive Game Development:**\n\nMaAS can be applied to game development to create more dynamic and responsive NPCs (Non-Player Characters).\n\n* **Scenario:** A simple interaction, like asking an NPC for directions, could be handled by a single LLM call. A more complex interaction, like engaging in a quest with the NPC, could trigger multiple agents: one for dialogue management, another for quest logic, and possibly one for simulating emotions or personality.\n* **Implementation:**\n    * **Node.js + WebSockets:** Use Node.js to host the multi-agent system and WebSockets for real-time communication between the client (player) and the server (game world).\n    * **TensorFlow.js/ONNX.js:** Explore integrating smaller LLMs client-side (using TensorFlow.js or ONNX.js) for handling simple NPC interactions while deferring to the server-side multi-agent system for more complex scenarios.\n* **Benefit:** Creates more believable and engaging NPCs that adapt their behavior based on the player's actions and choices.\n\n\n**Key Considerations for JavaScript Developers:**\n\n* **Controller Design:** The controller logic is critical.  Consider rule-based systems, lightweight classification models, or even smaller LLMs for the controller.\n* **Agent Communication:** Clearly define the communication protocols between agents. JSON objects are a good choice for exchanging structured data.\n* **Asynchronous Programming:**  Use promises and async/await in JavaScript to manage the asynchronous nature of LLM interactions efficiently.\n* **State Management:** Implement robust state management to track the conversation history and the current state of each agent.\n\nBy adopting the principles of MaAS, JavaScript developers can unlock new possibilities in LLM-based web development, creating more dynamic, efficient, and personalized experiences. Remember to prioritize experimentation and iterative development to find the best approach for your specific application.",
  "pseudocode": "```javascript\n// Algorithm 1: Multi-agent Architecture Search (MaAS)\n\nasync function maas(D, O, pi, Q) {\n  // D: Dataset (Dtrain, Dtest)\n  // O: Set of agentic operators\n  // pi: Initial distribution of agentic architectures (probabilities)\n  // Q: Controller network (parameterized by phi)\n\n  for (const [q, a] of D.Dtrain) { // Iterate through training data\n    let G; // Multi-agent system (DAG)\n\n    // Sample query-dependent MAS from agentic supernet\n    const V = []; // List to hold selected operators at each layer\n    for (let l = 1; l <= L; l++) { // L: Max number of layers\n      const Ve = sampleOperators(q, V, pi, Q); // Eq. 9\n      V.push(Ve);\n\n      if (l === L || Ve.includes('Oexit')) { \n        break; // Exit when reaching max depth or early exit\n      }\n    }\n    G = buildMAS(V); // Construct MAS from selected operators (Eq. 8)\n\n\n    // Execute sampled MAS\n    const generatedAnswer = await executeMAS(G); // Eq. 6\n\n\n    // Self-evolve agentic supernet (update pi and O)\n    const lossWRTpi = calculateLossWRTpi(a, generatedAnswer, pi, O, q); // Eq. 11\n    const lossWRTO = await calculateLossWRTO(a, generatedAnswer, O, q); // Eq. 12 (Textual Gradient)\n\n    pi = updateDistribution(pi, lossWRTpi); // Eq. 10\n    O = updateOperators(O, lossWRTO); // Eq. 10\n\n  }\n\n  return { pi, O }; // Optimized agentic supernet\n}\n\n\n// Helper functions (simplified for illustration)\n\nfunction sampleOperators(q, V, pi, Q) {\n  // Sample operators based on the query and previous layers using the controller network Q (Eq. 9)\n  // ... (Implementation details involving Mixture-of-Experts, thresholds, etc.)\n}\n\nfunction buildMAS(V) {\n // Construct the multi-agent system (DAG) from the selected operators at each layer (Eq. 8)\n // ... (Connect operators based on their dependencies, forming a DAG)\n}\n\nasync function executeMAS(G) {\n  // Execute the multi-agent system G to generate an answer (Eq. 6)\n  // ... (Involves calls to LLMs, tools, etc.)\n}\n\nfunction calculateLossWRTpi(a, generatedAnswer, pi, O, q) {\n  // Calculate loss with respect to the distribution pi (Eq. 11)\n  // ... (Monte Carlo estimation)\n}\n\n\nasync function calculateLossWRTO(a, generatedAnswer, O, q) {\n  // Estimate loss with respect to the operators O using textual gradients (Eq. 12)\n  // ... (Agent-based textual feedback mechanism) \n}\n\nfunction updateDistribution(pi, lossWRTpi) {\n  // Update the distribution pi based on the calculated loss\n  // ... (Gradient-based update)\n}\n\nfunction updateOperators(O, lossWRTO) {\n  // Update operators O based on textual gradient feedback\n  // ... (Modify prompts, temperatures, etc.)\n}\n\n```\n\n**Explanation of the MaAS Algorithm:**\n\nThe MaAS algorithm aims to automate the design and optimization of multi-agent systems for answering various queries. Instead of finding a single best multi-agent system, it learns a distribution over possible multi-agent architectures (the \"agentic supernet\").\n\n1. **Initialization:** The algorithm starts with an initial distribution (`pi`) over agentic operators (like CoT, ReAct, etc.) and a controller network (`Q`).\n\n2. **Training Loop:**  The algorithm iterates through the training data, sampling a query-dependent multi-agent system (`G`) from the agentic supernet for each query.\n\n3. **Sampling:**  The `sampleOperators` function uses the controller network (`Q`) to probabilistically select operators at each layer of the multi-agent system, conditioned on the query and previously selected operators. An \"early exit\" operator allows for variable-depth architectures.\n\n4. **Execution:** The sampled multi-agent system (`G`) is executed to generate an answer.\n\n5. **Optimization:**  The algorithm calculates the loss with respect to both the distribution (`pi`) and the operators (`O`). The loss with respect to `pi` is calculated using a Monte Carlo procedure (Eq. 11).  The loss with respect to `O` is estimated using \"textual gradients,\" a technique where LLM agents provide feedback in natural language on how to improve their performance (Eq. 12).\n\n6. **Update:**  The distribution `pi` and the operators `O` are updated based on the calculated losses.  This updates the agentic supernet to favor architectures that generate high-quality answers with lower costs (e.g., token usage).\n\n7. **Inference:** After training, the optimized agentic supernet can be used to sample a customized multi-agent system for any new query.\n\n\nThe provided JavaScript code is a simplified illustrative implementation of the core MaAS algorithm.  Several components, such as the controller network (`Q`), the textual gradient mechanism, and the specific operator implementations, are abstracted as helper functions for clarity.  A complete implementation would require substantial additional code and integration with LLM APIs.",
  "simpleQuestion": "How can I build cost-effective, adaptable multi-agent LLMs?",
  "timestamp": "2025-02-08T06:04:31.393Z"
}