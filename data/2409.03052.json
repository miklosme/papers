{
  "arxivId": "2409.03052",
  "title": "An Introduction to Centralized Training for Decentralized Execution in Cooperative Multi-Agent Reinforcement Learning",
  "abstract": "Multi-agent reinforcement learning (MARL) has exploded in popularity in recent years. Many approaches have been developed but they can be divided into three main types: centralized training and execution (CTE), centralized training for decentralized execution (CTDE), and Decentralized training and execution (DTE). CTE methods assume centralization during training and execution (e.g., with fast, free and perfect communication) and have the most information during execution. That is, the actions of each agent can depend on the information from all agents. As a result, a simple form of CTE can be achieved by using a single-agent RL method with centralized action and observation spaces (maintaining a centralized action-observation history for the partially observable case). CTE methods can potentially outperform the decentralized execution methods (since they allow centralized control) but are less scalable as the (centralized) action and observation spaces scale exponentially with the number of agents. CTE is typically only used in the cooperative MARL case since centralized control implies coordination on what actions will be selected by each agent. CTDE methods are the most common as they can use centralized information during training but execute in a decentralized manner-using only information available to that agent during execution. CTDE is the only paradigm that requires a separate training phase where any available information (e.g., other agent policies, underlying states) can be used. As a result, they can be more scalable than CTE methods, do not require communication during execution, and can often perform well. CTDE fits most naturally with the cooperative case, but can be potentially applied in competitive or mixed settings depending on what information is assumed to be observed. Decentralized training and execution methods make the fewest assumptions and are often simple to implement. In fact, any single-agent RL method can be used for DTE by just letting each agent learn separately. Of course, there are pros and cons to such approaches. It is worth noting that DTE is required if no centralized training phase is available (e.g., though a centralized simulator), requiring all agents to learn during online interactions without prior coordination. DTE methods can be applied in cooperative, competitive, or mixed cases. MARL methods can be further broken up into value-based and policy gradient methods. Value-based methods (e.g., Q-learning) learn a value function and then choose actions based on those values. Policy gradient methods learn an explicit policy representation and attempt to improve the policy in the direction of the gradient. Both classes of methods are widely used in MARL. This text is an introduction to CTDE MARL. It is meant to explain the setting, basic concepts, and common methods. It does not cover all work in CTDE MARL as the subarea is quite extensive. I have included work that I believe is important for understanding the main concepts in the subarea and apologize to those that I have omitted. I will first give a brief description of the cooperative MARL problem in the form of the Dec-POMDP. Then, I present an overview of CTDE and the two main classes of CTDE methods: value function factorization methods and centralized critic actor-critic methods. Value function factorization methods include the well-known VDN, QMIX, and QPLEX approaches, while centralized critic methods include MADDPG, COMA, and MAPPO. Finally, I discuss other forms of CTDE such as adding centralized information to decentralized (i.e., independent) learners (such as parameter sharing) and decentralizing centralized solutions. The basics of reinforcement learning (in the single-agent setting) are not presented in this text. Anyone interested in RL should read the book by Sutton and Barto. Similarly, for a broader overview of MARL, the recent book by Albrecht, Christianos and Sch√§fer is recommended.",
  "summary": "This paper explores Centralized Training for Decentralized Execution (CTDE) in cooperative Multi-Agent Reinforcement Learning (MARL). CTDE allows agents to leverage shared information during training, leading to better coordination, while still acting independently during execution. This is particularly relevant to LLM-based multi-agent systems where LLMs can be used as agents. The paper dives into two main CTDE methods: value function factorization (like VDN, QMIX, and QPLEX) where a joint value function is broken down into individual agent values, and centralized critic methods (like MADDPG, COMA, and MAPPO) which employ a central critic to guide the learning of decentralized actors. The use of state information in critics and the tradeoffs between different types of critics are also analyzed.  The paper concludes with a discussion on other CTDE forms, highlighting areas for future research, such as developing globally optimal model-free MARL methods.",
  "takeaways": "This paper provides a great overview of CTDE approaches in MARL, which is a powerful paradigm for developing multi-agent systems where agents can learn to cooperate effectively in a decentralized manner. While the paper doesn't focus on LLMs, the insights can be applied to LLM-based multi-agent applications, especially when considering web development scenarios. Here's how a JavaScript developer can leverage these ideas:\n\n**1. Decentralized Collaborative Content Creation:**\n\n* **Scenario:** Imagine building a web application where multiple users, each powered by an LLM agent, collaborate on writing a story. Each agent has its own style and preferences but needs to work together to create a cohesive narrative.\n* **Applying CTDE:** You could train a centralized critic that evaluates the quality and coherence of the story as a whole. Each agent (implemented using a JavaScript LLM library like `transformers.js`) would have its own actor, generating text snippets. The centralized critic, potentially a more complex LLM or a combination of rules and LLM-based assessments, provides feedback to the individual actors, guiding them towards a well-structured and engaging story.\n* **Frameworks:**  You might use Node.js for server-side coordination, React for the frontend UI, and Socket.IO for real-time communication between agents.\n\n**2. Multi-User Strategy Game with LLM-powered Bots:**\n\n* **Scenario:**  Developing a browser-based strategy game where players can team up with LLM-controlled bots. The bots need to understand the game rules, adapt to different player strategies, and contribute effectively to the team's success.\n* **Applying CTDE:** You could train a centralized critic (potentially a deep neural network combined with game-specific heuristics) that evaluates the overall performance of the team (players and bots) in the game. Each bot would be an LLM agent (using `transformers.js` or similar) with its own actor network, making game decisions. The centralized critic's feedback during training helps the bots learn better strategies.\n* **Frameworks:** Phaser or PixiJS for game development, Node.js for server-side logic, and WebSockets for communication.\n\n**3. Cooperative Task Management with LLM Assistants:**\n\n* **Scenario:** A web application designed for project management where LLM-powered agents assist users with tasks like scheduling meetings, prioritizing tasks, and generating reports. The agents need to understand individual user needs and collaborate to ensure smooth project execution.\n* **Applying CTDE:** Train a centralized critic (perhaps a rule-based system enhanced with LLM-based understanding of tasks and priorities) to evaluate the overall progress and efficiency of the project. Individual agents, using a JavaScript LLM framework, would act as personal assistants, taking actions based on user instructions and the project context.  The centralized critic's feedback helps agents learn how to better coordinate and prioritize actions.\n* **Frameworks:** React for frontend UI, Node.js with Express for backend APIs, and a database like MongoDB for storing task and project data.\n\n**Key JavaScript Tools and Considerations:**\n\n* **LLM Libraries:** `transformers.js` provides access to pre-trained LLM models and allows fine-tuning for specific tasks.  \n* **Web Frameworks:** React, Vue.js, or Angular for building interactive frontends.\n* **Server-Side Frameworks:** Node.js with Express or NestJS for building robust backends.\n* **Real-Time Communication:** Socket.IO or WebSockets for enabling efficient agent interaction.\n\n**Remember:**\n\n* This paper primarily focuses on value-based and policy gradient methods. Explore how these methods translate to using LLMs for action selection and policy updates in your agents.\n* Carefully consider the type of critic (centralized, decentralized, history-based, etc.) that is appropriate for your specific web development scenario.\n* Start with simpler implementations and gradually increase the complexity of your multi-agent system as you gain more experience. \n\nBy embracing the CTDE approach and utilizing appropriate JavaScript tools, developers can build sophisticated LLM-powered multi-agent web applications that offer enhanced collaboration, intelligent assistance, and a richer user experience.",
  "pseudocode": "```javascript\n// Value Decomposition Networks (VDN) - Finite Horizon\nfunction vdn(learningRate, explorationRate, targetUpdateFrequency) {\n  // Set hyperparameters\n  const alpha = learningRate;\n  const epsilon = explorationRate;\n  const C = targetUpdateFrequency; \n\n  // Initialize network parameters for each Q_i (denoted as Q)\n  let theta = {};\n  for (let i = 1; i <= numAgents; i++) {\n    theta[i] = initializeNetworkParameters(); // Replace with your network initialization\n  }\n\n  // Initialize target networks\n  let thetaMinus = {};\n  for (let i = 1; i <= numAgents; i++) {\n    thetaMinus[i] = {...theta[i]}; // Create a copy\n  }\n\n  // Initialize replay buffer D\n  let D = [];\n\n  // Episode index\n  let e = 1;\n\n  // Loop through episodes\n  while (true) { // Replace with your episode termination condition\n    // Initialize agent histories\n    let h = {};\n    for (let i = 1; i <= numAgents; i++) {\n      h[i] = []; // Empty initial history\n    }\n\n    // Loop through time steps within an episode\n    for (let t = 1; t <= horizon; t++) { \n      // Select actions for each agent\n      let a = {};\n      for (let i = 1; i <= numAgents; i++) {\n        a[i] = selectAction(h[i], theta[i], epsilon); // Replace with your action selection logic (e.g., epsilon-greedy)\n      }\n\n      // Interact with the environment\n      let [jointReward, observations] = environmentStep(a); // Replace with your environment interaction logic\n\n      // Store experience in the replay buffer\n      D.push([a, observations, jointReward]);\n\n      // Update agent histories\n      for (let i = 1; i <= numAgents; i++) {\n        h[i].push(a[i], observations[i]); \n      }\n\n      // Sample an episode from the replay buffer\n      let sampledEpisode = sampleEpisode(D); // Replace with your sampling logic\n\n      // Loop through time steps of the sampled episode\n      for (let t = 1; t <= horizon; t++) {\n        // Retrieve experience from the sampled episode\n        let [a, o, r] = sampledEpisode[t]; \n\n        // Calculate target value y\n        let y = r;\n        for (let i = 1; i <= numAgents; i++) {\n          y += gamma * Math.max(...calculateQValues(h[i], thetaMinus[i])); // Replace calculateQValues with your logic\n        }\n\n        // Perform gradient descent on each agent's network parameters\n        for (let i = 1; i <= numAgents; i++) {\n          theta[i] = gradientDescent(theta[i], alpha, y, h[i], a[i]); // Replace with your gradient descent implementation\n        }\n      }\n\n      // Update target networks\n      if (e % C === 0) {\n        for (let i = 1; i <= numAgents; i++) {\n          thetaMinus[i] = {...theta[i]}; \n        }\n      }\n\n      // Increment episode index\n      e++;\n    }\n  }\n\n  // Return learned Q functions\n  return theta;\n}\n\n// Example usage:\nconst numAgents = 4; // Replace with your number of agents\nconst horizon = 10; // Replace with your time horizon\nconst gamma = 0.99; // Replace with your discount factor\nconst learnedQFunctions = vdn(0.001, 0.1, 100); \n```\n\n**Explanation:**\n\nThis JavaScript code implements the Value Decomposition Networks (VDN) algorithm for a multi-agent reinforcement learning scenario with a finite time horizon. \n\n* **Purpose:** VDN is a centralized training for decentralized execution (CTDE) method. It aims to learn individual Q-functions for each agent that can be used for decentralized action selection during execution. These individual Q-functions are trained by combining them into a joint Q-function and using a centralized critic to update the parameters. \n\n* **Algorithm Breakdown:**\n    1. **Initialization:** The code starts by initializing hyperparameters (learning rate, exploration rate, target update frequency), network parameters for each agent's Q-function (using your chosen network architecture), target networks, and an empty replay buffer.\n    2. **Episode Loop:**  The main loop iterates through episodes. \n    3. **Action Selection:** Within each episode, the agents select actions based on their current history and Q-function, potentially using an exploration strategy like epsilon-greedy. \n    4. **Environment Interaction:** The selected actions are used to interact with the environment, resulting in joint rewards and observations for each agent.\n    5. **Experience Storage:** The experience (actions, observations, reward) is stored in the replay buffer.\n    6. **History Updates:**  The agents' histories are updated by appending the latest actions and observations.\n    7. **Sampling and Training:**  An episode is sampled from the replay buffer. The algorithm then iterates through the time steps of the sampled episode. A target value (`y`) is calculated using the target networks, representing the expected future return. Gradient descent is performed on each agent's Q-function parameters to minimize the difference between the predicted Q-value and the target value.\n    8. **Target Network Updates:**  The target networks are periodically updated with the parameters of the main networks.\n    9. **Decentralized Execution:** After training, the learned individual Q-functions can be used by each agent for decentralized action selection during execution.\n\n* **Key Points:**\n    * **Centralized Critic:** The training uses a centralized critic that observes the joint history and actions to calculate the target value (`y`).\n    * **Decentralized Actors:** During execution, each agent acts independently based on its own history and learned Q-function.\n    * **Factorization:** VDN approximates the joint Q-function as a simple sum of individual agent Q-functions, which simplifies the learning and action selection process.\n\nRemember that you need to replace the placeholder functions (`initializeNetworkParameters`, `selectAction`, `environmentStep`, `sampleEpisode`, `calculateQValues`, `gradientDescent`) with your specific implementations based on your chosen network architecture, environment, and optimization algorithm.",
  "simpleQuestion": "How train agents centrally, act decentrally?",
  "timestamp": "2024-09-06T05:02:56.012Z"
}