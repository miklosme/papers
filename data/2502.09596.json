{
  "arxivId": "2502.09596",
  "title": "KIMAS: A Configurable Knowledge Integrated Multi-Agent System",
  "abstract": "Knowledge-intensive conversations supported by large language models (LLMs) have become one of the most popular and helpful applications that can assist people in different aspects. Many current knowledge-intensive applications are centered on retrieval-augmented generation (RAG) techniques. While many open-source RAG frameworks facilitate the development of RAG-based applications, they often fall short in handling practical scenarios complicated by heterogeneous data in topics and formats, conversational context management, and the requirement of low-latency response times. This technical report presents a configurable knowledge integrated multi-agent system, KIMAS, to address these challenges. KIMAS features a flexible and configurable system for integrating diverse knowledge sources with 1) context management and query rewrite mechanisms to improve retrieval accuracy and multi-turn conversational coherency, 2) efficient knowledge routing and retrieval, 3) simple but effective filter and reference generation mechanisms, and 4) optimized parallelizable multi-agent pipeline execution. Our work provides a scalable framework for advancing the deployment of LLMs in real-world settings. To show how KIMAS can help developers build knowledge-intensive applications with different scales and emphases, we demonstrate how we configure the system to three applications already running in practice with reliable performance.",
  "summary": "This paper introduces KIMAS, a configurable multi-agent system designed to enhance retrieval-augmented generation (RAG) for real-world applications, especially knowledge-intensive conversations.  KIMAS uses multiple specialized agents for context management, information retrieval from diverse sources (local vector databases, online search engines, APIs), and answer summarization.  Key features for LLM-based multi-agent systems include flexible query rewriting based on conversation and knowledge context, efficient knowledge routing using embedding similarity and manual overrides, reranking of retrieved information for filtering before summarization, and a streamlined citation generation process for increased trustworthiness and transparency.  KIMAS optimizes the agent pipeline for parallel execution to minimize latency, crucial for real-time applications.  The system is demonstrated on three use cases of increasing scale and complexity, highlighting its adaptability and practical value for building robust and efficient LLM-powered knowledge-intensive applications.",
  "takeaways": "This paper introduces KIMAS, a multi-agent system for knowledge-intensive conversations using LLMs. Here are practical examples of how JavaScript developers can apply these insights to LLM-based multi-agent AI projects, focusing on web development scenarios:\n\n**1. Contextual Chatbots with LangChainJS:**\n\n* **Scenario:** Building a chatbot for a specific product documentation site.\n* **KIMAS Insight:** Context management and query rewriting are crucial for accurate responses.\n* **Implementation:** Use LangChainJS to manage conversation history and enrich user queries. For example, if a user asks, \"How do I install it?\" after a discussion about a specific feature, LangChainJS can be used to prepend the feature's name to the query, forming \"How do I install [feature name]?\".  This can be further enhanced with custom JavaScript functions to handle pronouns and references.\n\n```javascript\nimport { LLMChain, PromptTemplate } from \"langchain\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst template = \"Context: {history}\\nQuestion: {query}\\nAnswer:\";\nconst prompt = new PromptTemplate({ template, inputVariables: [\"history\", \"query\"] });\nconst chain = new LLMChain({ llm: model, prompt });\n\nlet history = \"\";\n\nasync function getResponse(userQuery) {\n  const enrichedQuery = enhanceQuery(userQuery, history); // Custom function\n  const response = await chain.call({ history: history, query: enrichedQuery });\n  history += `User: ${userQuery}\\nBot: ${response.text}\\n`;\n  return response.text;\n}\n\n// Example custom query enhancement function\nfunction enhanceQuery(query, history) {\n  if (query.includes(\"it\")) {\n    // Logic to extract the referent from history\n    const referent = extractReferent(history);\n    return query.replace(\"it\", referent); \n  }\n  return query;\n}\n\n// ... implementation for extractReferent ...\n\n```\n\n\n**2. Multi-Source Knowledge Integration with Agents:**\n\n* **Scenario:** A customer support chatbot that accesses product documentation, a knowledge base, and community forums.\n* **KIMAS Insight:** Efficient routing and retrieval from heterogeneous knowledge sources are essential.\n* **Implementation:** Create specialized agents (JavaScript classes or functions) for each data source. Use a routing function (potentially LLM-powered, leveraging LangChainJS chains) to decide which agents should be queried based on the user's question.  Consider embeddings and clustering (e.g., using libraries like TensorFlow.js) for improved routing efficiency.\n\n```javascript\n// Example of a routing function\nasync function routeQuery(query) {\n  const embedding = await generateEmbedding(query); // Using a library like TensorFlow.js or an API\n  const closestAgent = findClosestAgent(embedding); // Comparing to agent clusters\n  return closestAgent.query(query);\n}\n```\n\n\n**3. Citation Generation with Post-Processing:**\n\n* **Scenario:** An educational chatbot that provides sources for its answers.\n* **KIMAS Insight:** Citation generation can be handled efficiently as a post-processing step.\n* **Implementation:** After generating an answer using an LLM, send the answer and retrieved knowledge chunks to a separate LLM call (using LangChainJS for simpler integration) to generate citations. This avoids blocking the main response stream.\n\n\n**4. Optimizing for Latency with Asynchronous Operations:**\n\n* **Scenario:** Any real-time multi-agent web application.\n* **KIMAS Insight:** Parallelize as much as possible for reduced latency.\n* **Implementation:** Leverage JavaScript's `async/await` and `Promise.all` to run independent agent operations concurrently. This could involve parallel knowledge retrieval, context analysis, or citation generation.\n\n```javascript\nasync function getResponse(query) {\n  const [context, routedAgents] = await Promise.all([\n    contextManager.analyze(query),\n    routeQuery(query),\n  ]);\n\n  const agentResponses = await Promise.all(routedAgents.map(agent => agent.query(query)));\n\n  // ... consolidate agentResponses and generate final answer ...\n}\n\n```\n\n**Key JavaScript Libraries and Frameworks:**\n\n* **LangChainJS:** For managing chains, prompts, and interacting with LLMs.\n* **TensorFlow.js or similar:** For embedding generation and clustering for routing.\n* **Any HTTP client library (e.g., Axios, fetch):** For accessing external APIs and knowledge sources.\n* **Web Workers:** For offloading computationally intensive tasks (like embedding generation) to separate threads, improving UI responsiveness.\n\n\nBy combining the insights from KIMAS with these practical JavaScript examples and libraries, developers can build more efficient and effective LLM-based multi-agent web applications. Remember that these examples are simplified, and the specifics will depend on the complexity and requirements of your project.  Don't be afraid to experiment and adapt these concepts to your specific needs.",
  "pseudocode": "No pseudocode block found.",
  "simpleQuestion": "How can KIMAS improve LLM multi-agent app performance?",
  "timestamp": "2025-02-14T06:07:13.957Z"
}