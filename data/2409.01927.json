{
  "arxivId": "2409.01927",
  "title": "From Grounding to Planning: Benchmarking Bottlenecks in Web Agents",
  "abstract": "General web-based agents are increasingly essential for interacting with complex web environments, yet their performance in real-world web applications remains poor, yielding extremely low accuracy even with state-of-the-art frontier models. We observe that these agents can be decomposed into two primary components: Planning and Grounding. Yet, most existing research treats these agents as black boxes, focusing on end-to-end evaluations which hinder meaningful improvements. We sharpen the distinction between the planning and grounding components and conduct a novel analysis by refining experiments on the Mind2Web dataset. Our work proposes a new benchmark for each of the components separately, identifying the bottlenecks and pain points that limit agent performance. Contrary to prevalent assumptions, our findings suggest that grounding is not a significant bottleneck and can be effectively addressed with current techniques. Instead, the primary challenge lies in the planning component, which is the main source of performance degradation. Through this analysis, we offer new insights and demonstrate practical suggestions for improving the capabilities of web agents, paving the way for more reliable agents.",
  "summary": "This paper analyzes the performance bottlenecks of AI agents designed for web navigation. It dissects these agents into two components: Planning (deciding the sequence of actions) and Grounding (identifying and interacting with web elements). \n\nContrary to prior assumptions, the research found that the primary bottleneck is not grounding but planning. Even with perfect grounding, current LLM-based agents struggle to plan complex action sequences. This highlights the need for incorporating external knowledge and contextual information to improve the planning capabilities of LLM-based multi-agent systems.",
  "takeaways": "This paper provides a roadmap for JavaScript developers venturing into the world of LLM-based multi-agent AI, particularly within the context of web development. Let's translate the insights into actionable examples:\n\n**1. Enhancing Element Grounding:**\n\n* **DOM-based PU with CSS Selectors:**  The paper highlights the effectiveness of a DOM-based Page Understanding (PU) approach for element grounding. JavaScript developers can leverage their existing DOM manipulation skills (using libraries like jQuery or native APIs) and combine them with CSS Selectors to accurately identify and extract relevant elements from web pages. \n    * **Example:** Imagine building a multi-agent system for automating travel bookings. You could write a JavaScript function that uses CSS Selectors (as outlined in the paper) to reliably extract flight details, prices, dates, and button elements for booking.\n\n* **Ranking Refinement:** The paper proposes ranking heuristics based on element text length and spatial positioning on the page.\n    * **Example:** In a multi-agent application for e-commerce, prioritize product elements with shorter descriptions and those located near the top-left corner of product listing pages. This can be implemented in JavaScript by calculating element text length and bounding box positions. Libraries like  Popper.js could be used for positioning calculations.\n\n**2. Addressing Planning Bottlenecks:**\n\n* **Pre-Planning with LLMs:**  The paper suggests using LLMs to generate a high-level pre-plan before step-by-step actions. \n    * **Example:** For a multi-agent system designed to assist users in filling out complex forms, you can send the form's context and the user's objective to the LLM beforehand. The LLM could generate a concise plan like \"First, fill personal details, then address information, and finally, submit.\" This plan can guide the agent's subsequent actions, making them more context-aware. \n\n* **JavaScript Task Orchestration:** Node.js and libraries like `async.js` or `bull` can be used to manage the asynchronous nature of LLM calls and orchestrate complex workflows in your multi-agent system.\n\n**3. Mitigating Dataset Issues:**\n\n* **Duplicate Element Handling:**  The paper identifies duplicate elements as a significant challenge.\n    * **Example:** Develop JavaScript-based heuristics or use computer vision techniques to identify groups of similar elements. Then, incorporate additional logic (possibly leveraging LLM reasoning) to disambiguate and select the most appropriate element based on context.\n\n* **Data Augmentation:** Explore browser automation frameworks like Puppeteer or Selenium to create more robust training datasets by simulating varied user interactions on different web pages.\n\n**4. Libraries and Frameworks:**\n\n* **LLM Interaction:**  Use JavaScript libraries like `langchain.js` or the official OpenAI API client to seamlessly integrate and interact with LLMs like GPT-4.\n\n* **Web Scraping & Automation:** Frameworks like Puppeteer and Cheerio.js are invaluable for programmatically interacting with web pages, extracting data, and simulating user actions in your multi-agent system.\n\n**Remember:**\n\n* **Context is Key:**  Always provide LLMs with sufficient context about the current webpage (using DOM information), past interactions, and the overall objective.\n\n* **Experiment and Iterate:**  The field is rapidly evolving. Use these insights as a starting point and experiment with different approaches to find what works best for your specific use case.",
  "pseudocode": "No pseudocode block found.",
  "simpleQuestion": "**Question:**  How to improve web agent planning?",
  "timestamp": "2024-09-04T05:01:06.603Z"
}