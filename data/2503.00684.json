{
  "arxivId": "2503.00684",
  "title": "Factorized Deep Q-Network for Cooperative Multi-Agent Reinforcement Learning in Victim Tagging",
  "abstract": "Abstract-Mass casualty incidents (MCIs) are a growing concern, characterized by complexity and uncertainty that demand adaptive decision-making strategies. The victim tagging step in the emergency medical response must be completed quickly and is crucial for providing information to guide subsequent time-constrained response actions. In this paper, we present a mathematical formulation of multi-agent victim tagging to minimize the time it takes for responders to tag all victims. Five distributed heuristics are formulated and evaluated with simulation experiments. The heuristics considered are on-the-go, practical solutions that represent varying levels of situational uncertainty in the form of global or local communication capabilities, showcasing practical constraints. We further investigate the performance of a multi-agent reinforcement learning (MARL) strategy, factorized deep Q-network (FDQN), to minimize victim tagging time as compared to baseline heuristics. Extensive simulations demonstrate that between the heuristics, methods with local communication are more efficient for adaptive victim tagging, specifically choosing the nearest victim with the option to replan. Analyzing all experiments, we find that our FDQN approach outperforms heuristics in smaller-scale scenarios, while heuristics excel in more complex scenarios. Our experiments contain diverse complexities that explore the upper limits of MARL capabilities for real-world applications and reveal key insights.",
  "summary": "This paper explores how to minimize the time it takes for a team of responders (e.g., robots, humans, or a hybrid team) to tag all victims in a mass casualty incident (MCI).  It formalizes victim tagging as an optimization problem and evaluates five distributed heuristics alongside a factorized deep Q-network (FDQN) reinforcement learning approach.\n\nFor LLM-based multi-agent systems, the key takeaways are: (1) Decentralized decision-making with a shared global state is a viable approach for coordinating multiple agents in complex, uncertain environments like MCIs. (2) Factorizing the Q-function allows for scalable learning by avoiding the combinatorial explosion of joint actions. (3) Action masking, informed by real-world constraints (like an FSM), improves learning efficiency. (4) Agent-to-victim ratios are more critical for learning performance than the absolute size of the environment. (5) While promising for smaller-scale scenarios, FDQN struggles as complexity increases, highlighting ongoing challenges in applying deep RL to large-scale multi-agent problems.  This emphasizes the need for more sophisticated communication protocols, coordination strategies, and perhaps hybrid approaches combining learned policies with rule-based systems in future research.",
  "takeaways": "This research paper offers several valuable insights for JavaScript developers working with LLM-based multi-agent systems in web development. Here are practical examples applying the key findings:\n\n**1. Local Communication and Adaptability:**\n\nThe paper emphasizes that local communication strategies often outperform global ones in uncertain, dynamic environments like MCIs. This translates directly to web applications where agents might have limited or unreliable access to a central server.\n\n* **Scenario:** Imagine building a collaborative online whiteboard application with multiple LLM-powered agents assisting users in real-time. Instead of relying on a central server to coordinate every agent's action, you could leverage peer-to-peer communication libraries like PeerJS or SimplePeer. Each agent would communicate directly with its neighbors, sharing relevant information (e.g., changes to the whiteboard, user input) and making decisions locally. This decentralized approach improves responsiveness and resilience to network interruptions.\n\n* **JavaScript Implementation:** Using PeerJS, an agent could broadcast its actions to nearby agents:\n\n```javascript\n// Establish peer connections\nconst peer = new Peer();\npeer.on('connection', (conn) => {\n  conn.on('data', (data) => {\n    // Process data from other agents (e.g., whiteboard updates)\n  });\n});\n\n// Send an action to connected peers\npeer.connections.forEach((conn) => {\n  conn.send({ action: 'draw', coordinates: [x, y] });\n});\n```\n\n**2. Heuristics for Victim Tagging (Task Prioritization):**\n\nThe paper explores various heuristics for victim tagging, which can be adapted for task prioritization in web applications.\n\n* **Scenario:** Consider a multi-agent customer support system.  LLM agents could use a modified Local Nearest Victim Policy (LNVP) to prioritize customer requests.  \"Proximity\" could be defined by the urgency of the request (e.g., based on keywords, sentiment analysis), wait time, or the agent's expertise.\n\n* **JavaScript Implementation:**\n\n```javascript\nconst customerRequests = [\n  { id: 1, urgency: 'high', waitTime: 10 },\n  { id: 2, urgency: 'low', waitTime: 5 },\n  // ...\n];\n\nfunction selectNextRequest(agent, requests) {\n  return requests.filter(r => !r.assigned).sort((a, b) => {\n      const aPriority = calculatePriority(agent, a); // agent specific\n      const bPriority = calculatePriority(agent, b); // agent specific\n\n      return aPriority - bPriority; // ascending, so lowest first\n  })[0]; // LNVP\n}\n```\n\n**3. Factorized Q-Learning with Shared State:**\n\nThe FDQN architecture uses a shared state space, enabling agents to have a global view while making decentralized decisions.\n\n* **Scenario:**  Building a multi-agent system for collaborative document editing. A shared state could represent the document's current content, user cursors, and edit history. Each agent (specialized for tasks like grammar checking, style suggestions, or plagiarism detection) can access this shared state and decide independently how to contribute.\n\n* **JavaScript Implementation:** Use a shared data structure (e.g., a CRDT like Yjs or Automerge) to represent the document state, accessible to all agents. TensorFlow.js can be used to implement the neural network for each agent's Q-function.\n\n**4. Action Masking:**\n\nThe paper introduces action masking to constrain actions based on the agent's state.\n\n* **Scenario:** In the document editing example, an agent specialized for grammar checking should only offer suggestions when the user is writing, not when idle or navigating. Action masking can prevent irrelevant actions.\n\n\n**5. Experimentation with Scalability:**\n\nThe research demonstrates that FDQN is more effective in smaller-scale scenarios. JavaScript developers can easily experiment with various environment sizes and agent numbers using browser-based simulations and Node.js for backend processing.\n\n**Libraries and Frameworks:**\n\n* **PeerJS/SimplePeer:** Peer-to-peer communication.\n* **Yjs/Automerge:** Shared state management.\n* **TensorFlow.js:** Machine learning models.\n* **LangChain.js:** LLM Agent development\n\n\nBy combining these insights with readily available JavaScript tools, developers can effectively build and deploy LLM-based multi-agent web applications that are robust, responsive, and capable of intelligent collaboration.  The focus on local communication, adaptive heuristics, and shared state provide a solid foundation for building scalable and efficient multi-agent systems in real-world web scenarios.",
  "pseudocode": "The heuristics described in the paper are represented by mathematical formulas rather than pseudocode blocks.  Therefore, the answer is \"No pseudocode block found\". However, I can provide JavaScript code snippets illustrating the core logic of those heuristics for a JavaScript developer working on a multi-agent simulation:\n\n```javascript\n// Example setup (replace with your actual agent and victim data)\nconst responders = [\n    { id: 1, position: { x: 10, y: 20 }, speed: 1, taggingTime: 3 },\n    // ... more responders\n];\nconst victims = [\n    { id: 1, position: { x: 30, y: 40 }, tagged: false, health: 0.3 },\n    // ... more victims\n];\n\n\nfunction distance(a, b) {\n    return Math.sqrt((a.x - b.x)**2 + (a.y - b.y)**2);\n}\n\n// Local Nearest Victim Policy (LNVP) Example\nfunction lnvp(responder) {\n    let closestVictim = null;\n    let minDistance = Infinity;\n\n    for (const victim of victims) {\n      if (!victim.tagged) {  //consider untagged victims only\n        const dist = distance(responder.position, victim.position);\n        if (dist < minDistance) {\n          minDistance = dist;\n          closestVictim = victim;\n        }\n      }\n    }\n    return closestVictim;\n}\n\n// Example usage\nconst nextVictim = lnvp(responders[0]);\n\nif (nextVictim) {\n    console.log(`Responder ${responders[0].id} targets victim ${nextVictim.id}`);\n    // ... update simulation state:  Move responder, tag victim after taggingTime etc.\n}\n\n//  You can implement the other heuristics (RVP, NVP, LCVP, LGAP) similarly.\n\n// For the FDQN approach you would replace the heuristic function  \n// (like lnvp above) with actions derived from your trained FDQN model.\n```\n\n\nThese snippets are illustrative. A full implementation would require integrating this logic into a multi-agent simulation environment, managing agent states, handling concurrent actions, and implementing the training process for the FDQN if used. The paper also uses an FSM for each responder, which would also be part of a complete JavaScript implementation.",
  "simpleQuestion": "Can FDQN improve victim tagging speed?",
  "timestamp": "2025-03-04T06:05:15.839Z"
}