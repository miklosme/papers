{
  "arxivId": "2503.08740",
  "title": "Cooperative Bearing-Only Target Pursuit via Multiagent Reinforcement Learning: Design and Experiment",
  "abstract": "Abstract-This paper addresses the multi-robot pursuit problem for an unknown target, encompassing both target state estimation and pursuit control. First, in state estimation, we focus on using only bearing information, as it is readily available from vision sensors and effective for small, distant targets. Challenges such as instability due to the nonlinearity of bearing measurements and singularities in the two-angle representation are addressed through a proposed uniform bearing-only information filter. This filter integrates multiple 3D bearing measurements, provides a concise formulation, and enhances stability and resilience to target loss caused by limited field of view (FoV). Second, in target pursuit control within complex environments, where challenges such as heterogeneity and limited FoV arise, conventional methods like differential games or Voronoi partitioning often prove inadequate. To address these limitations, we propose a novel multiagent reinforcement learning (MARL) framework, enabling multiple heterogeneous vehicles to search, localize, and follow a target while effectively handling those challenges. Third, to bridge the sim-to-real gap, we propose two key techniques: incorporating adjustable low-level control gains in training to replicate the dynamics of real-world autonomous ground vehicles (AGVs), and proposing spectral-normalized RL algorithms to enhance policy smoothness and robustness. Finally, we demonstrate the successful zero-shot transfer of the MARL controllers to AGVs, validating the effectiveness and practical feasibility of our approach. The accompanying video is available at https://youtu.be/HO7FJyZiJ3E.",
  "summary": "This paper tackles the problem of coordinating multiple robots to pursue a moving target using only bearing information (direction), as obtainable from vision sensors. It combines a novel bearing-only state estimation filter with a multi-agent reinforcement learning (MARL) framework for pursuit control.  The system is designed for heterogeneous robots, some with omnidirectional movement and others with unicycle-like motion.  Sim-to-real techniques, including adjustable low-level control gains and spectral-normalized RL, are implemented to ensure smooth, robust control transferable to real-world robots.\n\n\nFor LLM-based multi-agent systems, the key takeaways are the robust bearing-only state estimation filter and the emphasis on sim-to-real transfer.  The filter could be incorporated into LLM agents reliant on noisy or incomplete sensory information.  The paper's focus on practical implementation through methods like spectral normalization, which smooths control outputs, addresses common challenges in deploying LLM-based agents in real-world scenarios.  The adaptable, model-free MARL approach could be combined with LLMs for higher-level reasoning and decision-making in complex, dynamic environments.",
  "takeaways": "This paper offers valuable insights for JavaScript developers working on LLM-based multi-agent applications, especially in web-based simulations and collaborative environments. Here's how you can apply the key concepts:\n\n**1. Bearing-Only State Estimation with u-PLIF:**\n\n* **Practical Scenario:** Imagine a collaborative web game where multiple user-controlled agents (represented by browser clients) need to locate a hidden object using only directional information.  This simulates limited sensory input, similar to bearing-only sensing.\n* **JavaScript Implementation:**  You could implement the u-PLIF algorithm using a JavaScript numerical library like NumJs or TensorFlow.js. Each agent, on the client-side, would receive bearing data to the target from its perspective. The u-PLIF algorithm would run locally on each client, allowing agents to estimate the target's position independently.  The estimated position can then be shared via WebSockets (e.g., Socket.IO) for collaborative tracking.\n* **Example Code Snippet (Conceptual):**\n\n```javascript\n// Using NumJs for matrix operations (simplified illustration)\nconst bearing = getBearingToTarget(); // Agent's bearing measurement\nconst P = calculateProjectionMatrix(bearing); // From paper's Eq. (3)\n// ... other u-PLIF calculations to update information matrix and state estimate\n\n// Share estimate via WebSockets\nsocket.emit('targetEstimate', { position: estimatedTargetPosition });\n```\n\n\n**2. MARL-based Pursuit Control:**\n\n* **Practical Scenario:**  Extend the previous example. After estimating the target's location, agents must coordinate to reach it.  Challenges include limited field of view (FoV), collision avoidance, and potentially different agent capabilities (heterogeneity).\n* **JavaScript Implementation:** Use a JavaScript reinforcement learning library like ml5.js or ReinforceJS to train a pursuit policy.  The simulator environment can be built with a web graphics library like Three.js or Babylon.js.  The server can coordinate training or act as a central hub for sharing experiences amongst agents.\n* **LLM Integration:** Integrate LLMs for more sophisticated agent behavior. For instance, an LLM can act as a \"coordinator\" agent, issuing natural language instructions to other agents based on the evolving game state.  Individual agents could also use LLMs to interpret instructions or generate their own sub-goals.\n* **Framework Integration:**  Combine this with front-end frameworks like React or Vue.js to manage the user interface and data visualization for your multi-agent simulation.\n\n**3. Sim-to-Real Considerations (for Robotics):**\n\nWhile the paper focuses on physical robots, the sim-to-real bridging concepts are valuable for any system involving a simulation-training phase.\n\n* **Adjustable Control Gains:**  Introduce control gain parameters in your JavaScript simulation.  Experiment with different gain values to simulate varying responsiveness or latency, which could be relevant for network-controlled agents in a web application.\n\n* **Spectral Normalization:**  When training your MARL agents using JavaScript libraries, implement spectral normalization (if the library supports it, or by modifying the underlying neural network implementation) to promote smoother control actions. This helps mitigate the \"jerkiness\" often observed in RL-controlled agents, leading to a more stable and predictable user experience.\n\n\n**4. Visualization and User Interface:**\n\nUse JavaScript visualization libraries like D3.js or Chart.js to display real-time metrics like target estimation error, agent trajectories, and reward values. This provides valuable feedback during both training and deployment.\n\n\n\nBy combining the u-PLIF algorithm with MARL and leveraging existing JavaScript libraries and frameworks, developers can create engaging and sophisticated multi-agent web applications, pushing the boundaries of what's possible in collaborative online environments.  Furthermore, the sim-to-real techniques can be adapted to improve the robustness and user experience of these applications.",
  "pseudocode": "The paper describes algorithms, but they are presented as a series of mathematical equations and textual descriptions rather than formal pseudocode blocks.  Therefore, a direct conversion to JavaScript is not feasible.  However, I can provide JavaScript-like conceptual implementations of the core algorithms based on the paper's descriptions.\n\n**1. 3D Bearing-Only Cooperative PLIF (u-PLIF)**\n\nThis algorithm estimates the target's position and velocity using bearing-only measurements from multiple pursuers.\n\n```javascript\nclass UPlif {\n  constructor(dt, Q, sigma) {\n    this.dt = dt;  // Time step\n    this.Q = Q;    // Process noise covariance\n    this.sigma = sigma; // Measurement noise standard deviation\n    this.Yk = math.identity(6); // Initial information matrix (identity)\n    this.yk_hat = math.zeros(6,1); //Initial information vector (zero)\n\n    // Define A and B matrices based on dt (see paper equation 1)\n     this.A = math.matrix([[math.identity(3), math.multiply(this.dt, math.identity(3))], [math.zeros(3, 3), math.identity(3)]]);\n     this.B = math.matrix([[math.multiply(0.5* this.dt*this.dt, math.identity(3))], [math.multiply(this.dt, math.identity(3))]]);\n\n  }\n\n\n\n  predict() {\n    const Mk_1 = math.multiply(math.transpose(math.inv(this.A)), this.Yk, math.inv(this.A));\n\n\n    this.Yk = math.inv(math.add(math.identity(6), math.multiply(Mk_1, this.B, this.Q, math.transpose(this.B)))).dot(Mk_1)\n\n\n    this.yk_hat = math.inv(math.add(math.identity(6), math.multiply(Mk_1, this.B, this.Q, math.transpose(this.B)))).dot(math.transpose(math.inv(this.A))).dot(this.yk_hat);\n  }\n\n\n  correct(pursuers, target_est) {\n\n    let sum_H_Rinv_HT = math.zeros(6,6)\n    let sum_H_Rinv_z = math.zeros(6,1)\n\n    for (const pursuer of pursuers) {\n\n      const pi = pursuer.position;\n      const xi_measured = pursuer.bearing;\n\n\n      const ri_vec = math.subtract(target_est.position, pi);\n\n      const ri = math.norm(ri_vec);\n      const xi_hat = math.divide(ri_vec,ri);\n\n      const Pi = math.subtract(math.identity(3), math.divide(math.multiply(ri_vec, math.transpose(ri_vec)),math.multiply(ri,ri)))\n\n      const Hi = math.matrix([[Pi, math.zeros(3, 3)]]);\n\n\n      const Ri = math.multiply(ri*ri*this.sigma*this.sigma,Pi)\n      const Rinvi = math.pinv(Ri)\n\n\n      sum_H_Rinv_HT = math.add(sum_H_Rinv_HT,math.multiply(Hi,Rinvi,math.transpose(Hi)))\n\n      sum_H_Rinv_z = math.add(sum_H_Rinv_z,math.multiply(Hi,Rinvi, math.transpose(math.matrix([xi_measured]))).dot(pi))\n\n\n      this.Yk = math.add(this.Yk, sum_H_Rinv_HT);\n      this.yk_hat = math.add(this.yk_hat, sum_H_Rinv_z);\n    }\n\n\n\n    target_est.position = (math.subset(math.multiply(math.inv(this.Yk), this.yk_hat), math.index(math.range(0, 3))))\n    target_est.velocity = (math.subset(math.multiply(math.inv(this.Yk), this.yk_hat), math.index(math.range(3, 6))))\n\n\n    return target_est;\n\n  }\n}\n\n\n// Example usage (conceptual):\n\nconst dt = 0.1; // Time step\nconst Q = math.identity(3).dotpow(0.25) // Process noise covariance\nconst sigma = 0.01; // Measurement noise\n\nconst filter = new UPlif(dt, Q, sigma);\n\n// Inside the main loop (e.g., every time step):\nfilter.predict();\n\nconst pursuers = [\n  {position: [1, 0, 0], bearing: [0.7, 0.7, 0]},  // Example bearing measurements\n  // ... other pursuers\n];\n\nlet target_est = {position: [0,0,0], velocity:[0,0,0]} //Initial Guess\n\ntarget_est = filter.correct(pursuers, target_est);\n\n\n\n\n```\n\n*Purpose:*  Estimates the target state (position and velocity) by fusing noisy bearing measurements from multiple observers in 3D space while addressing challenges like nonlinearities and singularities.  Uses an information filter formulation for efficiency and robustness.\n\n\n**2.  Multi-Agent Reinforcement Learning (MARL) for Pursuit**\n\nThe paper uses a MADDPG-based approach.  A complete JavaScript implementation would require a dedicated RL library. However, I can illustrate the core concepts.\n\n```javascript\n// Conceptual representation of agent policy (using a hypothetical RL library)\nfunction agentPolicy(observation) {\n\n  // observation would include ego-state, allies' states, and target state (if visible)\n  const action = rlLibrary.selectAction(observation, policyNetwork);\n\n\n  // Actions: [vh, vx, vy]  (heading angular velocity, linear x velocity, linear y velocity)\n  return action;\n}\n\n\n\n// Example of a training step (conceptual):\n\nfor (const agent of agents) {\n  const observation = getAgentObservation(agent, agents, target);\n  const action = agentPolicy(observation);\n\n  // Apply action to the environment (simulator)\n  const nextObservation = simulator.step(action);\n  const reward = calculateReward(agent, agents, target);\n\n  // Store the experience for training the policy network\n  rlLibrary.storeExperience(observation, action, reward, nextObservation);\n\n\n  // Update the policy network using the MADDPG algorithm \n  rlLibrary.updatePolicyNetwork(agent.policyNetwork); // Assuming separate policy networks per agent.\n\n}\n\n\n\n\n// Example Action, Observation & Reward\n\nfunction getAgentObservation(agent, otherAgents, target){\n  // Ego state (position, velocity, heading) - fetch from agent object.\n\n  // Allies' states (relative pos, vel, heading, target detection flag)\n  const alliesState = otherAgents.filter(a=> a != agent).map(other => {\n     return calculateRelativeState(agent, other); // Calculate relative position, velocity, etc.\n\n\n  });\n   // Target state (position, velocity, if target is in FOV) - fetch from target object if in FOV\n   const targetState = calculateTargetState(agent,target)\n\n  return {...egoState, allies: alliesState, target: targetState};\n}\n\nfunction calculateReward(agent, otherAgents, target){\n\n  let reward = 0;\n\n  if(isCounterClockwiseRotation(agent)){\n     reward+=0.2;\n  }\n   if(isInFOV(agent,target)){\n      reward+=1;\n   }\n\n  if(isTargetInRange(agent, target)){\n      reward+=1;\n\n  }\n\n   reward+=calculateObservability(agent,otherAgents, target)\n\n   if(isCollision(agent,otherAgents)){\n      reward -= 10;\n   }\n\n  return reward;\n}\n\n\n```\n\n*Purpose:* Trains agents to cooperatively pursue a target in a dynamic environment, considering constraints like field of view, observability, and collision avoidance.\n\n\n\nThese JavaScript snippets are conceptual illustrations.  A real-world implementation would require a robust reinforcement learning library, potentially incorporating features like spectral normalization for smoother policy outputs.  The math library (like `mathjs`) would be crucial for matrix and vector operations. Furthermore, a physics-based simulator (like `matter.js` or a custom solution using libraries like `p5.js`) would be necessary to accurately model the agent and environment dynamics. Using tools like TensorFlow.js within the browser for building the neural networks within the rlLibrary is also possible.",
  "simpleQuestion": "Can MARL improve bearing-only multi-robot target pursuit?",
  "timestamp": "2025-03-13T06:03:56.607Z"
}