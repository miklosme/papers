{
  "arxivId": "2412.15163",
  "title": "Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents",
  "abstract": "Social norms are standards of behaviour common in a society. However, when agents make decisions without considering how others are impacted, norms can emerge that lead to the subjugation of certain agents. We present RAWL-E, a method to create ethical norm-learning agents. RAWL-E agents operationalise maximin, a fairness principle from Rawlsian ethics, in their decision-making processes to promote ethical norms by balancing societal well-being with individual goals. We evaluate RAWL-E agents in simulated harvesting scenarios. We find that norms emerging in RAWL-E agent societies enhance social welfare, fairness, and robustness, and yield higher minimum experience compared to those that emerge in agent societies that do not implement Rawlsian ethics.",
  "summary": "This paper introduces RAWL-E, a method for creating fairer multi-agent systems by incorporating Rawlsian ethics (specifically the \"maximin\" principle) into agent decision-making.  The goal is to ensure that the least advantaged agents are not exploited by the emergence of selfish social norms.  Experiments in simulated harvesting environments showed that RAWL-E agents formed more cooperative norms, leading to higher overall social welfare, greater fairness (lower inequality and higher minimum individual experience), and increased robustness (longer survival times) compared to baseline agents without ethical considerations.\n\nKey points for LLM-based multi-agent systems:\n\n* **Ethical Considerations:**  RAWL-E addresses the important issue of ethics in multi-agent learning by explicitly incorporating fairness principles into agent design. This is crucial for developing responsible LLM-based agents.\n* **Norm Emergence:** The research demonstrates how agents can learn and adapt norms based on ethical considerations. This has implications for shaping desired behaviors in LLM-based multi-agent interactions.\n* **Reward Shaping:**  The paper uses reward shaping to guide agent learning towards ethical outcomes, providing a practical mechanism for aligning LLM-based agent behavior with desired social values.\n* **Generalizability:** While the specific scenarios are simplified, the underlying principles and the modular design of RAWL-E could be adapted to more complex LLM-based multi-agent applications.",
  "takeaways": "This paper presents RAWL-E, a method for incorporating Rawlsian ethics (maximin principle) into reinforcement learning agents to promote fair norm emergence in multi-agent systems. Here's how a JavaScript developer can apply these insights to LLM-based multi-agent web applications:\n\n**1. Scenario: Collaborative Content Creation Platform**\n\nImagine a platform where multiple LLM-powered agents collaborate on writing articles, code, or stories.  Unequal contribution or resource allocation (e.g., one agent dominating the writing process) can lead to unfair outcomes.\n\n* **RAWL-E Implementation:**\n\n    * **Ethics Module (JavaScript):** Create a function `ethicsModule(agents)` that takes an array of agent objects (each with a `wellbeing` property representing their contribution, satisfaction, or other relevant metric).  Inside this function, implement the maximin principle using JavaScript's `Math.min()`:\n    ```javascript\n    function ethicsModule(agents) {\n        let minWellbeing = Infinity;\n        for (const agent of agents) {\n            minWellbeing = Math.min(minWellbeing, agent.wellbeing);\n        }\n        return minWellbeing;\n    }\n    ```\n    * **Reward Shaping (JavaScript):** Modify the reward function of your RL environment to incorporate the output of `ethicsModule`.  Increase the reward if an action improves the minimum well-being and penalize it otherwise.\n    ```javascript\n    function rewardFunction(agents, action) {\n      let baseReward = calculateBaseReward(agents, action);  // existing reward logic\n      let minWellbeingBefore = ethicsModule(agents);\n      // Simulate action to predict new state and agent wellbeing\n      let newAgents = simulateAction(agents, action);  \n      let minWellbeingAfter = ethicsModule(newAgents);\n\n\n      let ethicsReward = 0;\n      if (minWellbeingAfter > minWellbeingBefore) {\n          ethicsReward = 0.4; // positive ethical reward as per the paper\n      } else if(minWellbeingAfter < minWellbeingBefore){\n          ethicsReward = -0.4; // negative ethical reward\n\n      }\n\n      return baseReward + ethicsReward;\n\n    }\n    ```\n\n    * **LLM Integration:** The LLM agents would receive the shaped rewards, influencing their behavior towards fairer collaboration. You can use libraries like `LangChain` or `LlamaIndex` for interacting with LLMs, handling prompts, and integrating them into your application.\n\n\n**2. Scenario: Decentralized Task Management System**\n\nConsider a multi-agent system where LLMs manage and assign tasks in a project.  Fairness is crucial to prevent some agents from being overloaded while others remain idle.\n\n* **RAWL-E Implementation:**\n\n    * **Agent Wellbeing (JavaScript):** Define `agent.wellbeing` as a function of the agent's workload, task complexity, deadlines, etc.\n    * **Ethics and Reward:**  Use the same `ethicsModule` and reward shaping logic as in the previous example, but with `agent.wellbeing` based on task allocation fairness.\n    * **LLM-based Task Allocation:** Use the LLMs to generate task assignments, taking into account agent capabilities and the ethically shaped rewards.\n\n\n**3. JavaScript Frameworks and Libraries:**\n\n* **Reinforcement Learning Libraries:**  Use JavaScript RL libraries like `ReinforceJS`, `ml5.js`, or build custom implementations using TensorFlow.js or other machine learning libraries.\n* **Agent Frameworks:** Consider agent modeling frameworks like `Agent.js` or create custom agent classes.\n* **Frontend Frameworks:** React, Vue, or Angular can be used to build interactive interfaces for visualizing agent behavior and system metrics (e.g., Gini coefficient, minimum experience).\n\n**4. Experimentation and Evaluation:**\n\n* **Simulate different scenarios:** Vary the number of agents, task complexity, and resource availability to observe how RAWL-E influences norm emergence and system performance.\n* **Track metrics:**  Implement functions in JavaScript to calculate Gini coefficient, minimum experience, social welfare, and robustness as defined in the paper.  Visualize these metrics to understand the impact of RAWL-E on fairness and sustainability.\n\nBy following these steps, JavaScript developers can leverage the insights of the RAWL-E paper to build fairer and more sustainable LLM-based multi-agent web applications. Remember to adapt the code examples and scenarios to your specific requirements and the chosen LLM.  Start with a simplified simulation environment and gradually increase complexity as you gain experience.",
  "pseudocode": "```javascript\n// Algorithm 1: Ethics Module\nfunction ethicsModule(Ut, Ut_plus_1) {\n  // Get the minimum experience from the well-being vectors at time t and t+1\n  const Umin_t = Math.min(...Ut);\n  const Umin_t_plus_1 = Math.min(...Ut_plus_1);\n\n  // Calculate the sanction based on the change in minimum experience\n  let F_t_plus_1;\n  if (Umin_t_plus_1 > Umin_t) {\n    F_t_plus_1 = xi; // Positive sanction (xi is a predefined positive value)\n  } else if (Umin_t_plus_1 === Umin_t) {\n    F_t_plus_1 = 0; // Neutral sanction\n  } else {\n    F_t_plus_1 = -xi; // Negative sanction \n  }\n\n  return F_t_plus_1;\n}\n\n\n\n// Algorithm 2: Norms Module\nfunction normsModule(vt, at, r_t_plus_1) {\n  // Retrieve a behavior from the behavior base that matches the precondition (vt) and action (at)\n  const zeta = behaviorBase.retrieve(vt, at);\n\n  if (zeta !== null) {\n    // If a matching behavior is found, update its fitness\n    behaviorBase.updateFitness(zeta, r_t_plus_1); \n  } else {\n    // If no matching behavior is found, create a new one\n    const newZeta = behaviorLearner.create(vt, at);\n    behaviorBase.add(newZeta);\n  }\n\n\n  // Periodically clip the behavior base to maintain a maximum size (maxLen)\n  if (t % clipNorm === 0 && behaviorBase.length > maxLen) {\n    behaviorBase.clip(); // Remove least fit behaviors\n  }\n\n  // Update the norm base with newly emerged norms\n  normBase.updateEmergedNorms(behaviorBase);\n}\n\n\n// Algorithm 3: Interaction Module\nfunction interactionModule(st) {\n\n  // Select an action based on the current state using the policy (epsilon-greedy exploration)\n  const at = policy.selectAction(st);\n\n  // Execute the action and observe the next state and reward\n  const [r_t_plus_1, st_plus_1, done] = environment.step(at);\n\n  // Get the well-being vectors from the current and next state\n  const Ut = getWellbeing(st);\n  const Ut_plus_1 = getWellbeing(st_plus_1);\n\n\n  // Obtain sanction from ethics module\n  const F_t_plus_1 = ethicsModule(Ut, Ut_plus_1);\n\n\n  // Shape reward using the sanction\n  const shaped_r_t_plus_1 = r_t_plus_1 + F_t_plus_1;\n\n\n  // Update policy (e.g., using DQN update rule)\n  policy.update(st, at, shaped_r_t_plus_1, st_plus_1, done);\n\n\n  // Get the agent's view from the current state\n  const vt = getView(st);\n\n  // Update the norms module\n  normsModule(vt, at, shaped_r_t_plus_1);\n\n  return [st_plus_1, done]; // Return the next state and done flag\n}\n\n```\n\n\n\n**Algorithm 1: Ethics Module**\n\n* **Purpose:**  Evaluates the ethical implications of an agent's action by assessing its impact on the minimum experience (well-being) within the agent society.  It implements the Rawlsian maximin principle.\n* **Explanation:** The module takes well-being vectors (Ut and Ut+1) at two consecutive time steps as input.  It calculates the minimum well-being in both vectors. If the action taken between t and t+1 increases the minimum well-being, a positive sanction (`xi`) is returned. If it decreases, a negative sanction (`-xi`) is returned.  A neutral sanction (0) is returned if the minimum well-being remains unchanged. This sanction is used in the interaction module to shape the reward, encouraging actions that benefit the least advantaged agents.\n\n**Algorithm 2: Norms Module**\n\n* **Purpose:** Learns and stores behaviors and norms within the agent society.\n* **Explanation:** Takes the current state's view (`vt`), action (`at`), and the shaped reward (`r_t_plus_1`) as input. If a behavior matching `vt` and `at` exists in the `behaviorBase`, it updates the fitness of that behavior using the reward. If not, a new behavior is created and added.  The `behaviorBase` is periodically clipped to remove the least fit behaviors, preventing it from growing indefinitely.  The module also updates the `normBase` with any newly emerged norms. A norm emerges when a behavior is adopted by a sufficient percentage of the population.\n\n**Algorithm 3: Interaction Module**\n\n* **Purpose:**  Handles the agent's interaction with the environment, including action selection, reward processing, policy updates (using reinforcement learning), and norm learning.\n* **Explanation:** This module is the core of the agent's decision-making process. It uses the agent's policy to select actions, observes the environment's response (next state and reward), updates its policy based on the received reward (shaped by the Ethics Module), and provides input to the Norms Module for norm learning.  It uses epsilon-greedy exploration for action selection. It also updates the target network of the DQN periodically.\n\n\n\nThese algorithms implement a system where agents learn norms while considering the well-being of others using the Rawlsian maximin principle. The ethics module influences the learning process by shaping the rewards, and the norms module tracks emerging behaviors and consolidates them into norms. The interaction module manages the agent's actions and learning within the environment.",
  "simpleQuestion": "How can I build fair, norm-learning AI agents?",
  "timestamp": "2024-12-20T06:06:19.054Z"
}