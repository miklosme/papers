{
  "arxivId": "2409.11852",
  "title": "XP-MARL: Auxiliary Prioritization in Multi-Agent Reinforcement Learning to Address Non-Stationarity",
  "abstract": "Abstract-Non-stationarity poses a fundamental challenge in Multi-Agent Reinforcement Learning (MARL), arising from agents simultaneously learning and altering their policies. This creates a non-stationary environment from the perspective of each individual agent, often leading to suboptimal or even unconverged learning outcomes. We propose an open-source framework named XP-MARL, which augments MARL with auxiliary prioritization to address this challenge in cooperative settings. XP-MARL is 1) founded upon our hypothesis that prioritizing agents and letting higher-priority agents establish their actions first would stabilize the learning process and thus mitigate non-stationarity and 2) enabled by our proposed mechanism called action propagation, where higher-priority agents act first and communicate their actions, providing a more stationary environment for others. Moreover, instead of using a predefined or heuristic priority assignment, XP-MARL learns priority-assignment policies with an auxiliary MARL problem, leading to a joint learning scheme. Experiments in a motion-planning scenario involving Connected and Automated Vehicles (CAVs) demonstrate that XP-MARL improves the safety of a baseline model by 84.4% and outperforms a state-of-the-art approach, which improves the baseline by only 12.8%. Code: github.com/cas-lab-munich/sigmarl",
  "summary": "- This paper tackles the problem of non-stationarity in multi-agent reinforcement learning (MARL), where agents struggle to learn effectively because other agents are constantly changing their policies.\n\n- The paper introduces XP-MARL, a framework that prioritizes agents, letting higher-priority agents act first and communicate their actions. This action propagation helps stabilize the learning environment for lower-priority agents.  Instead of fixed priorities, XP-MARL learns which agent should be prioritized in a given situation. This is particularly relevant to LLM-based multi-agent systems, offering a potential mechanism for managing complex interactions and improving coordination between LLM agents.",
  "takeaways": "## Bridging XP-MARL concepts to LLM-based multi-agent web apps:\n\nThis paper presents interesting challenges for JavaScript developers working on LLM-based multi-agent systems, particularly in how to practically implement the prioritization and action propagation mechanisms. Here's how you can leverage its insights:\n\n**1. Prioritizing LLM Agents:**\n\n* **Scenario:** Imagine building a collaborative writing app where multiple LLM agents assist users in real-time.\n* **Challenge:** LLMs concurrently generating text can lead to conflicting suggestions and a confusing user experience.\n* **Solution:** Implement XP-MARL's prioritization concept:\n    * **Priority Assignment:** Train a separate LLM agent (or a dedicated module within each agent) to assign priorities based on factors like user activity, writing style consistency, and suggestion relevance.\n    * **JavaScript Implementation:**\n        * Use a JavaScript framework like TensorFlow.js to train and run the priority assignment LLM.\n        * Develop a priority queue (e.g., using `PriorityQueue` from a library like `priority-queue`) to manage LLM agent order.\n        * Prioritize text generation requests based on the assigned priority scores.\n    * **Benefits:**  This ensures that the most relevant and coherent suggestions are presented first, creating a smoother writing experience.\n\n**2. Action Propagation for Contextual Awareness:**\n\n* **Scenario:** Developing a multi-agent customer support chatbot system.\n* **Challenge:**  Each LLM agent might individually provide decent responses, but they lack awareness of previous interactions, leading to repetitive or contradictory information.\n* **Solution:**  Apply XP-MARL's action propagation:\n    * **Shared Context:**  Create a shared context object (e.g., using a library like Socket.IO) accessible to all agents, storing conversation history and previous agent actions.\n    * **Action Propagation:**\n        * Whenever a higher-priority agent responds, update the shared context with its action (e.g., message sent) and relevant information.\n        * Lower-priority agents access this context before generating their responses, ensuring awareness of prior interactions.\n    * **JavaScript Implementation:**\n        * Use a real-time communication framework like Socket.IO to propagate actions between agents.\n        * Employ JavaScript's asynchronous capabilities (Promises, async/await) to handle communication delays effectively.\n    * **Benefits:** LLMs gain contextual awareness, leading to more informed, consistent, and personalized customer interactions.\n\n**3. Experimenting with JavaScript Libraries & Frameworks:**\n\n* **Reinforcement Learning:**  While the paper focuses on MARL algorithms, you can adapt the core principles to other LLM architectures. Explore JavaScript libraries like:\n    * TensorFlow.js: Train and run LLM agents for both prioritization and decision-making tasks.\n    * Synaptic: Experiment with different neural network architectures for your agents.\n* **Multi-Agent System Frameworks:**\n    * Agent.js:  Develop and manage communication between your LLM agents.\n    * BML: Model complex agent behaviors and interactions.\n* **WebSockets:** Use libraries like Socket.IO or native WebSockets for real-time action propagation between agents in your web application.\n\n**Overall, this paper highlights the practical value of prioritizing LLM actions and enabling communication between agents for building effective multi-agent systems. By adapting these concepts and utilizing relevant JavaScript tools, developers can create more robust and user-friendly LLM-powered web applications.**",
  "pseudocode": "```javascript\nfunction priorityAssignment(jointObservation, jointPolicy) {\n  // Initialize an empty array to store priority scores\n  let priorityScores = [];\n\n  // Iterate over each agent in the joint observation\n  for (let i = 0; i < jointObservation.length; i++) {\n    // Get the observation for the current agent\n    const observation = jointObservation[i];\n\n    // Get the policy for the current agent\n    const policy = jointPolicy[i];\n\n    // Sample an action from the policy based on the observation\n    const action = policy.sampleAction(observation);\n\n    // Append the action to the priority scores array\n    priorityScores.push(action);\n  }\n\n  // Sort the priority scores in descending order and get the indices\n  const priorityRank = priorityScores.map((score, index) => [score, index]).sort().reverse().map((pair) => pair[1]);\n\n  // Return the priority rank\n  return priorityRank;\n}\n\nfunction overallAlgorithm(jointObservationsD, jointObservationsP, jointPoliciesD, jointPoliciesP, observableAgents) {\n  // Call the priorityAssignment function to get the priority rank\n  const priorityRank = priorityAssignment(jointObservationsP, jointPoliciesP);\n\n  // Initialize an empty array to store the joint action for decision-making\n  let jointAction = [];\n\n  // Iterate over each agent in the priority rank\n  for (let i = 0; i < priorityRank.length; i++) {\n    // Get the index of the current agent in the priority rank\n    const agentIndex = priorityRank[i];\n\n    // Get the observable higher-priority agents for the current agent\n    const observableHigherPriorityAgents = priorityRank.slice(0, i);\n\n    // Filter the observable higher-priority agents to only include those observable by the current agent\n    const observableAgentsForCurrentAgent = observableHigherPriorityAgents.filter((agent) => observableAgents[agentIndex].includes(agent));\n\n    // Get the actions of the observable higher-priority agents\n    const actionsOfObservableHigherPriorityAgents = observableAgentsForCurrentAgent.map((agent) => jointAction[agent]);\n\n    // Append the actions of the observable higher-priority agents to the current agent's observation\n    const modifiedObservation = [...jointObservationsD[agentIndex], ...actionsOfObservableHigherPriorityAgents];\n\n    // Get the decision-making policy for the current agent\n    const policy = jointPoliciesD[agentIndex];\n\n    // Generate an action for the current agent based on the modified observation\n    const action = policy.sampleAction(modifiedObservation);\n\n    // Append the action to the joint action array\n    jointAction.push(action);\n  }\n\n  // Return the joint action for decision-making\n  return jointAction;\n}\n```\n\n**Explanation:**\n\n1. **`priorityAssignment(jointObservation, jointPolicy)`**:\n   - **Purpose**: This function determines the priority order of agents based on their observed states and policies.\n   - **Input**:\n     - `jointObservation`: An array of observations, one for each agent.\n     - `jointPolicy`: An array of policy objects, one for each agent. Each policy object should have a `sampleAction` method that takes an observation as input and returns an action.\n   - **Output**: `priorityRank`: An array of agent indices, sorted in descending order of priority.\n\n2. **`overallAlgorithm(jointObservationsD, jointObservationsP, jointPoliciesD, jointPoliciesP, observableAgents)`**:\n   - **Purpose**: This function orchestrates the overall decision-making process, incorporating priority assignment and action propagation.\n   - **Input**:\n     - `jointObservationsD`: An array of observations for the decision-making stage.\n     - `jointObservationsP`: An array of observations for the priority-assignment stage.\n     - `jointPoliciesD`: An array of decision-making policy objects.\n     - `jointPoliciesP`: An array of priority-assignment policy objects.\n     - `observableAgents`: A 2D array where each element `observableAgents[i]` is an array of agent indices observable by agent `i`.\n   - **Output**: `jointAction`: An array of actions for all agents, determined sequentially based on their priority ranking.\n\n**Key Concepts Implemented**:\n\n- **Priority-Based Action Selection**: Agents are ranked by priority, and higher-priority agents act first, making their actions known.\n- **Action Propagation**: Higher-priority agents' actions are communicated to lower-priority agents, influencing their decisions.\n- **Partial Observability**: Agents have limited awareness of other agents' states and actions, modeled by the `observableAgents` parameter. \n\nThis JavaScript code provides a practical illustration of how the theoretical concepts from the research paper can be translated into a functional multi-agent decision-making system.",
  "simpleQuestion": "How to stabilize multi-agent learning in non-stationary environments?",
  "timestamp": "2024-09-19T05:01:34.141Z"
}