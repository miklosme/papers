{
  "arxivId": "2501.06832",
  "title": "A novel multi-agent dynamic portfolio optimization learning system based on hierarchical deep reinforcement learning",
  "abstract": "Deep Reinforcement Learning (DRL) has been extensively used to address portfolio optimization problems.  The DRL agents acquire knowledge and make decisions through unsupervised interactions with their environment without requiring explicit knowledge of the joint dynamics of portfolio assets.  Among these DRL algorithms, the combination of actor-critic algorithms and deep function approximators is the most widely used DRL algorithm. Here, we find that training the DRL agent using the actor-critic algorithm and deep function approximators may lead to scenarios where the improvement in the DRL agent's risk-adjusted profitability is not significant. We propose that such situations primarily arise from the following two problems: sparsity in positive reward and the curse of dimensionality. These limitations prevent DRL agents from comprehensively learning asset price change patterns in the training environment. As a result, the DRL agents cannot explore the dynamic portfolio optimization policy to improve the risk-adjusted profitability in the training process. To address these problems, we propose a novel multi-agent Hierarchical Deep Reinforcement Learning (HDRL) algorithmic framework in this research. Under this framework, the agents work together as a learning system for portfolio optimization. Specifically, by designing an auxiliary agent that works together with the executive agent for optimal policy exploration, the learning system can focus on exploring the policy with higher risk-adjusted return in the action space with positive return and low variance. In this way, we can overcome the issue of the curse of dimensionality and improve the training efficiency in the positive reward sparse environment. The performance of the proposed HDRL algorithm is evaluated using a portfolio of 29 stocks from the Dow Jones index in four different experiments. During training, the risk-adjusted profitability of the DRL agent in the training environment is significantly improved. Hence, we can prove that the strategies executed by our learning system in out-sample experiments originate from the DRL agents' comprehensive learning of asset price change patterns in the training environment. Furthermore, each back-test experiment compares the proposed learning system to sixteen traditional strategies and ten strategies based on machine learning algorithms in the performance of profitability and risk control ability. The empirical results in the four evaluation experiments demonstrate the efficacy of our learning system, which outperforms all other strategies by at least 6.3% in terms of return per unit risk. Moreover, our proposed HDRL algorithm framework also outperforms individual DRL agents in the ablation study framework by a margin of at least 9.7% in terms of return per unit risk.",
  "summary": "This paper introduces a novel multi-agent system for optimizing investment portfolios using hierarchical deep reinforcement learning (HDRL).  It aims to improve profitability and manage risk by allocating funds across a basket of stocks (specifically the Dow Jones Industrial Average).\n\nKey points for LLM-based multi-agent systems:\n\n* **Hierarchical structure:** The system uses HDRL, dividing the complex task of portfolio optimization into sub-tasks handled by different agents.  This hierarchical approach is relevant to LLM-based multi-agent systems, where complex tasks can be decomposed and delegated.\n* **Auxiliary agent:** An auxiliary agent is used to pre-train and guide the main agent by finding baseline portfolio allocations, addressing the sparsity of positive rewards often encountered in reinforcement learning.  This is analogous to using LLMs to bootstrap or provide initial guidance to other agents in a multi-agent system.\n* **Addressing curse of dimensionality:** The hierarchical structure and auxiliary agent mitigate the curse of dimensionality, a common challenge in reinforcement learning with large action spaces (like portfolio optimization).  This has implications for LLM-based systems operating in complex environments.\n* **Policy learning:**  The system focuses on learning optimal *policies* for portfolio allocation, which are rules for making decisions. This relates to how LLMs can be used to generate and refine decision-making policies in multi-agent systems.\n* **Continuous action space:** The system handles a continuous action space (the proportion of funds allocated to each stock), which is also relevant to LLM-based systems that may need to operate in continuous action environments.",
  "takeaways": "This paper introduces a hierarchical deep reinforcement learning (HDRL) framework for portfolio optimization, which can be adapted for various LLM-based multi-agent web applications. Here are some practical examples for JavaScript developers, focusing on translating the core concepts:\n\n**1. Collaborative Content Creation:**\n\n* **Scenario:** Imagine a multi-agent system for collaborative story writing, where LLMs act as agents contributing paragraphs.  The HDRL structure can be used to manage the overall coherence and quality of the story.\n* **JavaScript Implementation:**\n    * **Auxiliary Agent:** An LLM-based agent (e.g., using a library like `langchainjs` or interacting with an API like OpenAI's) acts as a \"story architect.\" This agent sets a high-level direction for the story, generating outlines, themes, or key plot points.  This addresses the \"sparsity of positive reward\" issue by giving other agents a clear target.\n    * **Executive Agents:** Multiple LLM-based agents specialize in different writing styles or character perspectives. They receive the \"story architect's\" guidance and contribute paragraphs accordingly.  The reward function could be based on factors like coherence with the outline, creativity, and grammar, calculated using JavaScript libraries for text analysis (e.g., `compromise`, `natural`).\n    * **HDRL Framework:**  A JavaScript framework like `TensorFlow.js` or `Brain.js` could manage the training and interaction of these agents. The auxiliary agent's output guides the executive agents' exploration, mimicking the paper's hierarchical structure.  The framework would also handle the reward function calculation and policy updates.\n    * **Frontend Integration:** The story can be dynamically displayed on a web page using a framework like React or Vue.js, with agent contributions appearing in real-time.\n\n**2. Personalized Recommendations:**\n\n* **Scenario:**  An e-commerce website uses multi-agent AI to provide personalized product recommendations.\n* **JavaScript Implementation:**\n    * **Auxiliary Agent:** An LLM-based agent analyzes the user's browsing history, purchase patterns, and potentially even social media activity. It generates a high-level profile of the user's preferences.  This profile acts as the baseline, addressing the \"curse of dimensionality\" by reducing the search space for the executive agents.\n    * **Executive Agents:** Multiple LLM-based agents specialize in different product categories. They receive the user profile from the auxiliary agent and refine recommendations based on specific product attributes, current trends, and deals.  The reward function could be based on click-through rates, conversion rates, and user feedback.\n    * **HDRL Framework:**  A JavaScript backend framework like Node.js with a library like `ml5.js` or by interacting with cloud-based ML services can implement the HDRL structure. It manages agent communication and reward calculations.\n    * **Frontend Integration:** Recommendations are dynamically displayed on the product pages using a frontend framework like React, updated in real-time as the user interacts with the website.\n\n**3. Interactive Chatbots:**\n\n* **Scenario:** A customer service chatbot system uses multiple LLM-based agents to handle different aspects of customer queries.\n* **JavaScript Implementation:**\n    * **Auxiliary Agent:** An LLM-based agent acts as a \"conversation router,\" classifying the intent of the user's query (e.g., technical support, billing, order status).\n    * **Executive Agents:**  LLM-based agents specialize in handling specific query types. They receive the classified intent from the \"conversation router\" and engage in targeted conversations with the user. This hierarchical approach improves efficiency and response accuracy.  Reward functions could be based on customer satisfaction scores, resolution time, and query escalation rates.\n    * **HDRL Framework:** Implemented on the backend with Node.js and a reinforcement learning library, managing agent training and communication.\n    * **Frontend Integration:** The chatbot interface is implemented using a JavaScript framework and communicates with the backend system via APIs.\n\n\n**Key JavaScript Libraries and Concepts:**\n\n* **LLM Interaction:** `langchainjs`, OpenAI API, other LLM APIs\n* **Reinforcement Learning:** `TensorFlow.js`, `Brain.js`, `ml5.js`, cloud-based ML services\n* **Text Analysis:** `compromise`, `natural`\n* **Frontend Frameworks:** React, Vue.js, Angular\n* **Backend Framework:** Node.js\n\n\nBy adapting this paper's HDRL framework and employing these JavaScript tools, developers can create more sophisticated and efficient multi-agent applications, especially in situations where the action space is vast or the reward signal is sparse. This is particularly relevant for complex web applications where LLMs play diverse, collaborative roles. Remember that the specific implementation details, including reward function design and training processes, need to be tailored to the application's requirements.",
  "pseudocode": "```javascript\n// Algorithm 1: Training process of the Hierarchical Deep Reinforcement Learning (HDRL) agent for portfolio optimization.\n\n// Input parameters (explained in Appendix B)\nconst K = 5; // Trading days per period\nconst M = 40; // Periods in historical return tensor\nconst n = 29; // Number of stocks\nconst rL = 0.03; // Annual borrowing rate\nconst rS = 0.03; // Annual stock borrowing rate\nconst lambda1 = 10; // Risk aversion\nconst lambda2 = 0.001; // Transaction scale limit\nconst lambda3 = 50; // Risk aversion in target value\nconst targetStepM_au = 1080;\nconst minibatchSize_au = 128;\nconst replayBufferSize_au = Math.pow(2, 14);\nconst replayBufferSize_ex = Math.pow(2, 14);\nconst totalSteps_au = 3e5;\nconst totalSteps_ex = 6e5;\nconst learningRate_au = 1e-5;\nconst learningRate_ex = 1e-6;\nconst learningRate_critic = 1e-8;\nconst gamma = 0.99; // Discount factor\nconst tau = 0.005; // Momentum coefficient\nconst tradingPeriods = /* Number of trading periods (from Table 2) */;\n\n\n// Initialize auxiliary agent policy network (pi_au) and parameters (phi)\nlet phi = initializePolicyNetworkParameters();\nlet pi_au = (state_au) => /* Implementation of policy function */;\n\n// Train auxiliary agent\nlet train_au = true;\nlet t_au = 1;\nlet v0_au = 1e8;\nlet T0 = 1e8;\nlet c0 = 1e8;\nlet accumulatedSteps_au = 0;\nlet replayBuffer_au = [];\nlet current_state_au = observeState_au();\n\nwhile (train_au) {\n  for (let step = 0; step < targetStepM_au; step++) {\n    let action_au = pi_au(current_state_au); // Baseline portfolio weights\n    let next_state_au = executeAction_au(action_au); // Transition to next state\n    replayBuffer_au.push({\n      state: current_state_au,\n      action: action_au,\n      next_state: next_state_au\n    });\n    current_state_au = next_state_au;\n    t_au++;\n    if (t_au > tradingPeriods) {\n      t_au = 1;\n      current_state_au = observeState_au();\n    }\n  }\n\n  // Update auxiliary agent's policy network (gradient update not included here for simplicity)\n  for (let batch = 0; batch < Math.floor(replayBuffer_au.length / minibatchSize_au); batch++) {\n    // ... (Extract minibatch and perform gradient update on phi)\n  }\n\n  accumulatedSteps_au += targetStepM_au;\n  if (accumulatedSteps_au > totalSteps_au) {\n    train_au = false;\n  }\n}\n\n\n\n// ... (Initialize executive agent networks and parameters theta, w, theta', w', N)\n\n\n// Train executive agent (similar structure as auxiliary agent training)\n\n\n// ... (Rest of Algorithm 1 - updating executive agent networks and replay buffer)\n\n```\n\n\n**Explanation of the Algorithm:**\n\nThis JavaScript code implements the core logic of the hierarchical deep reinforcement learning (HDRL) algorithm described in the paper for portfolio optimization.  It involves training two agents: an auxiliary agent and an executive agent.\n\n1. **Auxiliary Agent:** The auxiliary agent's task is to learn a baseline portfolio allocation strategy (represented by `pi_au`). This strategy doesn't need to be optimal, but it should provide a reasonable starting point for the executive agent.  It uses a policy gradient method for training.\n\n2. **Executive Agent:** The executive agent's task is to learn an optimal portfolio allocation strategy based on the baseline provided by the auxiliary agent. It uses the Deep Deterministic Policy Gradient (DDPG) algorithm, an actor-critic method. The critic network (`Qw`) learns to evaluate the quality of actions, and the actor network (`pi_ex`) learns to choose actions that maximize the expected return.\n\nThe code structure mirrors the algorithm described in the paper, including the hierarchical training process, updates to the policy and critic networks, and the use of replay buffers (`replayBuffer_au` and `replayBuffer_ex`) for experience replay.\n\n**Purpose:** The purpose of this algorithm is to develop a sophisticated portfolio optimization strategy that maximizes returns while managing risk. The hierarchical approach allows the executive agent to learn more efficiently by leveraging the knowledge gained by the auxiliary agent.  This helps to address the challenges of high dimensionality and sparse positive rewards in the portfolio optimization problem.\n\n\n**Note:**  This JavaScript implementation is a simplified version of the full algorithm. Several key components are not explicitly defined (e.g., initialization of networks, gradient update calculations, random process N, specific reward functions), as they require further implementation details based on the specific neural network architectures and mathematical equations outlined in the paper. Furthermore, some variables like `tradingPeriods`  need to be fetched from external sources like Table 2 in the paper. This simplified code provides a high-level understanding of the algorithm's structure and flow.",
  "simpleQuestion": "Can multi-agent HDRL improve portfolio optimization?",
  "timestamp": "2025-01-14T06:03:16.895Z"
}