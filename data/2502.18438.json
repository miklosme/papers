{
  "arxivId": "2502.18438",
  "title": "TOMCAT: THEORY-OF-MIND FOR COOPERATIVE AGENTS IN TEAMS VIA MULTIAGENT DIFFUSION POLICIES",
  "abstract": "In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents in Teams), a new framework for generating ToM-conditioned trajectories. It combines a meta-learning mechanism, that performs ToM reasoning over teammates' underlying goals and future behavior, with a multiagent denoising-diffusion model, that generates plans for an agent and its teammates conditioned on both the agent's goals and its teammates' characteristics, as computed via ToM. We implemented an online planning system that dynamically samples new trajectories (replans) from the diffusion model whenever it detects a divergence between a previously generated plan and the current state of the world. We conducted several experiments using ToMCAT in a simulated cooking domain. Our results highlight the importance of the dynamic replanning mechanism in reducing the usage of resources without sacrificing team performance. We also show that recent observations about the world and teammates' behavior collected by an agent over the course of an episode combined with ToM inferences are crucial to generate team-aware plans for dynamic adaptation to teammates, especially when no prior information is provided about them.",
  "summary": "1. This paper introduces TOMCAT, a new framework for coordinating AI agents in team-based tasks.  TOMCAT helps agents understand and predict their teammates' behaviors, even if those teammates have different goals or act suboptimally.  It uses a \"Theory of Mind\" model (ToMnet) to predict teammate actions and motivations and a diffusion model (MADiff) to generate coordinated action plans.  A key feature is dynamic replanning, which allows agents to adapt to unexpected teammate actions by generating new plans as needed.\n\n2. Relevant to LLM-based multi-agent systems:\n    *  TOMCAT offers a mechanism for LLMs to reason about the intentions and behaviors of other LLMs in a multi-agent environment.\n    *  The ToMnet component could be adapted to leverage the rich contextual understanding of LLMs.\n    *  MADiff provides a way to generate flexible and coordinated action plans for multiple LLMs.  The dynamic replanning feature is especially relevant for real-world applications where LLM interactions are unpredictable.\n    *  The concept of agent \"profiles\" used in the research can be extended to represent the diverse personalities, knowledge bases, and communication styles of different LLMs.\n    *  While the current research relies on training data from simpler agents, future work could explore directly applying TOMCAT to interactions between pre-trained LLMs.\n    * The authors acknowledge the challenge of unknown teammates, which is a critical aspect of open multi-agent LLM systems where arbitrary LLMs can interact.  Their proposed future research on adapting TOMCAT to handle this scenario is of particular interest.",
  "takeaways": "This paper introduces TOMCAT, a framework for building cooperative, LLM-based multi-agent systems that can adapt to each other's behavior. Let's translate these insights into practical examples for a JavaScript developer:\n\n**Scenario 1: Collaborative Writing App**\n\nImagine building a collaborative writing app where multiple LLMs assist users with different writing tasks: one LLM suggests topic sentences, another refines grammar and style, and a third provides relevant citations.\n\n* **Theory of Mind (ToM):** You can implement ToM using a recurrent neural network (RNN) in TensorFlow.js or Brain.js. This RNN, trained on past interaction data between LLMs, would predict the next action or suggestion of other LLMs based on their recent contributions to the document.  For instance, if the topic sentence LLM just suggested a sentence about climate change, the citation LLM's ToM model would predict a higher likelihood of it searching for climate-related citations.\n* **Multiagent Diffusion Policies (MADiff):** MADiff could be used to generate coherent, collaborative writing plans.  Using a JavaScript library like NumJs to handle the numerical computations, you would implement MADiff to sample possible collaborative sequences of actions (e.g., suggest topic sentence, refine grammar, add citation). This sampling would be conditioned by the ToM predictions, ensuring consistency and cooperation between LLMs.  For example, if the ToM model predicts the grammar LLM will focus on a specific paragraph, the MADiff model would guide the other LLMs to work on different parts of the text, preventing conflicts.\n* **Dynamic Replanning:** In a live writing environment, plans can quickly become outdated.  If a user drastically edits a section, the existing plan becomes irrelevant.  You can implement dynamic replanning in JavaScript.  Monitor the divergence between the planned text modifications and the user's actual edits. When this divergence exceeds a threshold, trigger the MADiff model to generate a new plan, conditioned by the updated context and ToM predictions.\n\n**Scenario 2: Multi-User Virtual Environment**\n\nConsider building a multi-user virtual environment for education or gaming, populated by LLM-driven non-player characters (NPCs) that dynamically interact with users and each other.\n\n* **ToM for Personalized Interaction:** Implement ToM models for each NPC in TensorFlow.js. These models, trained on user interaction histories, predict how different users will react to NPC actions. An NPC encountering a player known for aggressive behavior would act differently compared to when facing a more passive player.\n* **MADiff for Coordinating NPC Actions:** Utilize MADiff to coordinate complex interactions between multiple NPCs.  For example, in a rescue scenario, one NPC could distract a threat while another guides players to safety. MADiff would sample a sequence of actions for each NPC, conditioned by the ToM predictions about the player's actions and the overall game state, ensuring coordination and realistic behavior. This could be implemented using a Node.js backend with a library like ml5.js for easier LLM integration.\n* **Dynamic Replanning for Unexpected Events:** The virtual environment is dynamic, with users taking unforeseen actions.  Implement dynamic replanning in JavaScript. If a player deviates significantly from the predicted path, retrigger the MADiff model to generate a new, adapted plan for the NPCs, maintaining the realism and coherence of the virtual world.\n\n**JavaScript Libraries and Frameworks**\n\n* **TensorFlow.js/Brain.js:** For implementing ToMnet, the neural network component of TOMCAT.\n* **NumJs/ml5.js:** To handle numerical computations within the MADiff and ToM models.\n* **Node.js:** For creating the backend logic of dynamic replanning, especially if it involves complex calculations or coordination between multiple users and agents.\n* **React/Vue.js:** To manage the frontend of web applications utilizing these multi-agent systems, handling data updates and UI interactions.\n\n**Key takeaways for JavaScript developers:**\n\n* TOMCAT allows building more adaptive and responsive multi-agent systems for web apps.\n* JavaScript libraries like TensorFlow.js and NumJs provide the tools to implement the core concepts.\n* Dynamic replanning is crucial in real-world scenarios.\n* Focus on collecting interaction data for training ToM and MADiff models effectively.\n\n\nBy combining LLM capabilities with ToM and MADiff, implemented in JavaScript, developers can create truly dynamic and intelligent multi-agent systems for web applications, offering highly personalized and adaptive user experiences.",
  "pseudocode": "```javascript\n// Algorithm 1: Online Dynamic Conditional Replanning (JavaScript)\n\nasync function onlineDynamicConditionalReplanning(N, i, Vi, D, Npast, fe, gPhi, eta, lambda, epsilonDelta, Ixi, omega, C, H) {\n  let echar = await calculateCharacterEmbedding(fe, Vi, D, Npast, i);\n\n  let done = false;\n  while (!done) { // Main loop, continues until the \"done\" flag is true (not defined in pseudocode)\n\n    let oi = await observe(); // Get current observation (not defined in pseudocode)\n    let h = new Queue(C); // Observation history queue\n    h.enqueueMany(oi); // Add current observation to history\n\n    let t = 0;\n    let oHatI = [];\n    let Tplan = new Queue(H);\n\n    if (Tplan.isEmpty() || squaredDistance(eta(oi), eta(oHatI)) > lambda) { // Replanning condition\n      let ement = gPhi(Vi, h, echar); // Calculate mental embedding\n      let yTi = [Vi, echar, ement]; // Construct conditioning variables\n\n      let tauK = generateGaussianNoise(H + C);\n\n      // Denoising loop\n      for (let k = K; k >= 1; k--) {\n        tauK.sliceInPlace(0, C, h); // In-paint history and current observation\n\n        let epsilon = epsilonDelta(tauK, k) + omega * (epsilonDelta(tauK, yTi, k) - epsilonDelta(tauK, k));\n        tauK = denoise(tauK, epsilon, k);\n      }\n\n\n      Tplan = new Queue(H);\n      Tplan.enqueueMany(tauK.slice(C + 1)); // Populate plan queue\n\n      oHatI = oi;\n\n    }\n\n    let nextObservation = Tplan.dequeue();\n    let ai = Ixi(oHatI, nextObservation);\n    oHatI = nextObservation;\n\n    await executeAction(ai); // Execute action (not defined in pseudocode)\n    await teammatesAct(N, i); // Let teammates act (not defined in pseudocode)\n\n    // Update loop conditions (not defined in pseudocode, added for clarity)\n    // ... logic to determine when 'done' becomes true ...\n\n    t++;\n  }\n}\n\n\n\n\n// Helper Functions (not defined in the pseudocode, implementations would be task specific)\n\nasync function calculateCharacterEmbedding(fe, Vi, D, Npast, i) {\n  // ... Implementation for calculating character embedding using fe, Vi, D and Npast ...\n  return echar;\n}\n\nasync function observe() {\n  // ... Implementation to get observation from environment ...\n  return observation;\n}\n\nfunction squaredDistance(v1, v2) {\n  // ... Implementation to calculate squared euclidean distance ...\n  return distance;\n}\n\nfunction generateGaussianNoise(length) {\n   // ... Implementation to generate gaussian noise of specified length ...\n   return noise;\n}\n\nfunction denoise(tau, epsilon, k) {\n  // ... Implementation for denoising step ...\n  return denoisedTau;\n}\n\nasync function executeAction(action) {\n  // ... Implementation to execute an action in the environment ...\n}\n\nasync function teammatesAct(N, i) {\n  // ... Implementation to allow other teammates in set N - i to act ...\n}\n\nclass Queue {\n  // ... Implementation for a FIFO Queue ...\n}\n```\n\n**Explanation of Algorithm 1 and its Purpose:**\n\nThis algorithm implements a dynamic replanning strategy for a multi-agent system.  Its purpose is to allow an agent (agent *i*) within a team of *N* agents to adapt its behavior based on observations, predictions about teammates (Theory of Mind, ToM), and past experiences.\n\nKey aspects of the algorithm:\n\n1. **Conditional Replanning:** The agent doesn't replan at every timestep. It replans only when the observed state diverges significantly from the predicted state (controlled by the `lambda` parameter), or when the current plan is exhausted.\n\n2. **ToM Integration:** The algorithm incorporates ToM reasoning through the `echar` (character embedding) and `ement` (mental embedding) variables. These embeddings represent the agent's beliefs about its teammates' characteristics and current mental states.  These embeddings are used to condition the generation of new plans.\n\n3. **Diffusion-based Planning:** The core planning mechanism uses a diffusion model (`epsilonDelta` and `denoise` functions) to generate trajectories (sequences of predicted observations). The diffusion model is conditioned by the ToM embeddings and the agent's own profile (`Vi`).\n\n4. **History In-painting:** When generating a new plan, the algorithm uses \"history in-painting\" to constrain the start of the generated trajectory to match the recent history of observations.\n\n5. **Decentralized Execution:** The algorithm assumes a decentralized execution framework, where each agent plans independently without direct communication.  However, the MADiff model is trained on team data to generate plans that are consistent with expected teammate behavior.\n\n\nThis implementation provides a basic structure for the algorithm. The helper functions (e.g., `calculateCharacterEmbedding`, `observe`, `denoise`, `executeAction`, `teammatesAct`) are not fully defined, as they are task-specific and depend on the particular environment and ToM model being used.  The `Queue` class is also included for managing observation history and planned trajectories.",
  "simpleQuestion": "How can LLMs improve multi-agent cooperation through ToM?",
  "timestamp": "2025-02-26T06:02:03.739Z"
}