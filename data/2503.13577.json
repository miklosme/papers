{
  "arxivId": "2503.13577",
  "title": "When Should We Orchestrate Multiple Agents?",
  "abstract": "Strategies for orchestrating the interactions between multiple agents, both human and artificial, can wildly overestimate performance and underestimate the cost of orchestration. We design a framework to orchestrate agents under realistic conditions, such as inference costs or availability constraints. We show theoretically that orchestration is only effective if there are performance or cost differentials between agents. We then empirically demonstrate how orchestration between multiple agents can be helpful for selecting agents in a simulated environment, picking a learning strategy in the infamous Rogers' Paradox from social science, and outsourcing tasks to other agents during a question-answer task in a user study.",
  "summary": "This paper explores how to best utilize multiple agents (e.g., humans, LLMs, human+LLM) for a given task, considering factors like agent cost, availability, and expertise in different sub-tasks (\"regions\").  It introduces a framework for dynamically selecting the most appropriate agent for each sub-task as data arrives, rather than pre-assigning agents.  \n\nFor LLM-based multi-agent systems, the key points are:\n* **Dynamic Orchestration:** Real-time agent selection based on context and estimated utility can outperform static assignments and improve overall accuracy.\n* **Cost and Constraints:**  Incorporating real-world factors like LLM API costs and regulatory limitations into the orchestration framework significantly influences optimal agent selection.\n* **Human-in-the-Loop:**  Humans can be valuable agents within a multi-agent system, and the framework addresses how to balance human and AI contributions.\n* **User Studies:** Experiments demonstrate that users struggle to effectively leverage multiple agents without orchestration, suggesting the value of automated assistance for agent selection in real-world applications.\n* **Addressing Rogers' Paradox:** The framework can be used to mitigate the negative impacts of naive social learning in multi-agent scenarios where agents learn from each other, showing a path to effective human-AI collaboration in complex environments.",
  "takeaways": "This research paper provides valuable insights for JavaScript developers working with LLM-based multi-agent systems in web applications. Here are some practical examples of how to apply these insights, along with relevant JavaScript frameworks and libraries:\n\n**1. Agent Selection based on Appropriateness of Orchestration:**\n\n* **Scenario:** Imagine building a customer support chatbot system where multiple LLMs specialize in different product categories.  You could have agents like \"BillingExpert,\" \"TechnicalSupport,\" and \"ProductInfo.\"\n* **Implementation:**  Before routing a user's query, implement the \"appropriateness of orchestration\" metric in JavaScript. Calculate `Cmax` (best possible accuracy among agents for a given region/topic) and `Crand` (expected accuracy of random agent selection). This helps determine if orchestration is even necessary. If `Cmax/Crand` is close to 1, any agent is as good as another.\n* **JavaScript Example:**\n\n```javascript\n// Example region probabilities and agent correctness (replace with your values)\nconst regionProbabilities = { billing: 0.4, technical: 0.3, product: 0.3 };\nconst agentCorrectness = {\n  BillingExpert: { billing: 0.9, technical: 0.5, product: 0.6 },\n  TechnicalSupport: { billing: 0.6, technical: 0.8, product: 0.6 },\n  ProductInfo: { billing: 0.7, technical: 0.5, product: 0.8 },\n};\n\nfunction calculateCmax(region) {\n  return Math.max(...Object.values(agentCorrectness).map(a => a[region]));\n}\n\nfunction calculateCrand() {\n  let crand = 0;\n  for (const region in regionProbabilities) {\n    let sum = 0;\n    for (const agent in agentCorrectness) {\n      sum += agentCorrectness[agent][region];\n    }\n    crand += regionProbabilities[region] * (sum / Object.keys(agentCorrectness).length);\n  }\n  return crand;\n}\n\nconst appropriateness = calculateCmax(\"billing\") / calculateCrand();\n\nif (appropriateness > 1.2) { //  Orchestration is likely beneficial\n    // Implement orchestration logic\n} else {\n  // Select any agent (perhaps the least expensive)\n}\n\n\n```\n\n* **Libraries/Frameworks:**  LangChain.js can be used for overall agent management and orchestration logic.\n\n\n**2. Real-time Agent Capability Estimation and Updates:**\n\n* **Scenario:** In the same chatbot system, agent capabilities might drift over time.  The \"BillingExpert\" LLM might become less accurate due to changes in billing policies.\n* **Implementation:**  Use a Bayesian approach (as suggested in the paper) to update agent correctness probabilities in real-time using user feedback (was the LLM's answer helpful?).  Store these probabilities in a database (e.g., using Node.js with MongoDB).\n* **JavaScript Example (Conceptual):**\n\n```javascript\n// Update agent correctness for BillingExpert in the \"billing\" region\nfunction updateAgentCorrectness(agent, region, wasCorrect) {\n  // Fetch current Beta distribution parameters from database\n  // ...\n\n  // Update parameters based on wasCorrect (Bayesian update)\n  // ...\n\n  // Store updated parameters back to the database.\n  // ...\n}\n```\n\n\n**3. Cost-Aware Orchestration:**\n\n* **Scenario:** Different LLMs might have different usage costs per API call.  \"BillingExpert\" might be more expensive than \"ProductInfo.\"\n* **Implementation:** Incorporate cost into the agent selection logic. The paper proposes the total empirical utility (Û≥t(Ak)). Modify your JavaScript orchestration function to account for cost when calculating agent utility.\n* **JavaScript Example:**\n\n```javascript\n// Incorporate cost (e.g. per token or API call). Example values\nconst agentCosts = { BillingExpert: 0.1, TechnicalSupport: 0.05, ProductInfo: 0.02 };\n\n\nfunction calculateAgentUtility(agent, region) {\n\n  // ... (Calculate C≥t(Ak) as before) ...\n\n  return  C≥t(Ak) / agentCosts[agent]; // Adjust for cost\n\n\n}\n```\n\n\n\n**4. Handling Constraints (Availability, Expertise):**\n\n* **Scenario:** The \"TechnicalSupport\" agent might be unavailable during off-peak hours, or \"BillingExpert\" may not be trained to handle questions about specific product types.\n* **Implementation:**  Use JavaScript functions to define constraints. These functions should check if an agent is available or suitable for a given region/topic based on current conditions.\n* **JavaScript Example:**\n\n```javascript\nfunction isAgentAvailable(agent) {\n    // Check if the agent is currently online/available (e.g. using an API)\n    // …\n}\n\nfunction isAgentSuitableForRegion(agent, region) {\n    // Check if the agent has expertise in the given region\n    // …\n}\n\n\n// In orchestration logic, filter agents based on availability and suitability before calculating utility.\n```\n\n**5. Experimenting with Rogers' Paradox:**\n\n* **Scenario:**  Explore different social learning strategies in your chatbot application. Simulate a scenario where users can learn from other users' interactions or AI-generated responses.\n* **Implementation:**  Create a JavaScript simulation environment where agents (representing users) interact and learn using different learning strategies. Track the overall \"world understanding\" as a metric.\n* **Libraries:**  Libraries like `chart.js` can help visualize the simulation results and how different orchestration strategies impact learning outcomes.\n\n\nBy following these examples, JavaScript developers can bridge the gap between multi-agent AI research and practical web application development, leading to more effective and robust LLM-based systems. Remember to adapt these examples to your specific projects and use appropriate JavaScript libraries and frameworks to streamline the implementation process.",
  "pseudocode": "```javascript\n// Function to calculate onwards correctness of an agent\nfunction onwardCorrectness(agent, region, regionProbabilities, agentCorrectness) {\n  let correctnessAtTimeStep = agentCorrectness[agent][region];\n  let futureLongRunningCorrectness = 0;\n\n  for (let m = 0; m < regionProbabilities.length; m++) {\n    futureLongRunningCorrectness += regionProbabilities[m] * agentCorrectness[agent][m];\n  }\n\n  return correctnessAtTimeStep * futureLongRunningCorrectness;\n}\n\n// Function to estimate region probabilities (wt)\nfunction estimateRegionProbabilities(observedRegionCounts, dirichletPriors) {\n  let wt = [];\n  let totalCounts = 0;\n\n  for (let j = 0; j < observedRegionCounts.length; j++) {\n    totalCounts += observedRegionCounts[j] + dirichletPriors[j] - 1;\n  }\n\n  for (let m = 0; m < observedRegionCounts.length; m++) {\n    wt.push((observedRegionCounts[m] + dirichletPriors[m] - 1) / totalCounts);\n  }\n\n  return wt;\n}\n\n// Function to estimate agent correctness per region (Ct,km)\nfunction estimateAgentCorrectness(correctResponses, incorrectResponses, betaPriors) {\n  return (correctResponses + betaPriors[1] - 1) / (correctResponses + incorrectResponses + betaPriors[0] + betaPriors[1] - 2);\n}\n\n\n// Orchestration logic\nfunction orchestrate(inputData, regionProbabilities, agentCorrectness, agentCosts, constraints) {\n  let bestAgent = -1;\n  let maxUtility = -Infinity;\n\n  for (let k = 0; k < agentCorrectness.length; k++) { // Iterate over agents\n    if (constraints && !constraints(inputData, k)) continue; // Apply constraints\n    let utility = onwardCorrectness(k, inputData.region, regionProbabilities, agentCorrectness) / agentCosts[k][inputData.region];\n    \n    if (utility > maxUtility) {\n      maxUtility = utility;\n      bestAgent = k;\n    }\n  }\n  return bestAgent;\n}\n\n\n\n// Example usage (illustrative)\nlet numRegions = 3;\nlet numAgents = 2;\n\n// Initialize data structures (replace with actual data)\nlet observedRegionCounts = [10, 5, 15]; // Example counts of observations from each region\nlet dirichletPriors = [1, 1, 1];  // Example Dirichlet priors for regions\n\n\nlet correctResponses = [[5, 2, 8], [3, 1, 6]]; // Example: correct responses per agent per region.\nlet incorrectResponses = [[5, 8, 2], [7, 9, 4]]; // Example: incorrect responses per agent per region.\nlet betaPriors = [1, 1]; // Example Beta priors for agent correctness.\n\nlet agentCorrectness = [];\nfor(let a = 0; a < numAgents; a++){\n    agentCorrectness.push([]);\n    for(let r = 0; r < numRegions; r++){\n        agentCorrectness[a].push(estimateAgentCorrectness(correctResponses[a][r], incorrectResponses[a][r], betaPriors));\n    }\n}\n\n\n\nlet agentCosts = [[1, 2, 3], [2, 1, 2]]; //Example Agent Costs\n\n\nlet regionProbabilities = estimateRegionProbabilities(observedRegionCounts, dirichletPriors);\n\n//Example constraints (EU AI Act Example)\nlet isHighRisk = true;\n\nfunction constraints(inputData, agentIndex){\n    if(!isHighRisk || agentIndex == 0){ // Agent 0 is human\n        return true; // Feasible\n    } else {\n        return false; // Not feasible\n    }\n}\n\nlet inputData = { value: \"some input\", region: 0 }; //Example input\n\n\nlet selectedAgent = orchestrate(inputData, regionProbabilities, agentCorrectness, agentCosts, constraints);\n\nconsole.log(\"Selected Agent:\", selectedAgent);\n\n\n\n```\n\n**Explanation of the Algorithms:**\n\n1. **`onwardCorrectness(agent, region, regionProbabilities, agentCorrectness)`:** This function calculates the \"onwards correctness\" of a given agent for a specific input region. It combines the agent's immediate correctness for that region with a weighted average of its future correctness across all regions. This future correctness is based on the estimated probability of encountering each region. This helps in considering future performance when selecting an agent for the current input.\n\n\n2. **`estimateRegionProbabilities(observedRegionCounts, dirichletPriors)`:** This function estimates the probabilities of observing data from each region. It uses a Bayesian approach with a Dirichlet prior and a multinomial likelihood. The `observedRegionCounts` are the counts of observations encountered from each region so far, and `dirichletPriors` are the prior beliefs about the region distribution. The output is a vector of probabilities, one for each region.\n\n3. **`estimateAgentCorrectness(correctResponses, incorrectResponses, betaPriors)`:** This function estimates an agent's correctness within a given region. It uses a Bayesian approach with a Beta prior and a Binomial likelihood. `correctResponses` and `incorrectResponses` are the counts of correct and incorrect responses from the agent in the region so far, respectively, while `betaPriors` represent prior beliefs about the agent's correctness.\n\n4. **`orchestrate(inputData, regionProbabilities, agentCorrectness, agentCosts, constraints)`:** This is the core orchestration function.  It takes the current `inputData` (including its region), estimated `regionProbabilities`, `agentCorrectness` estimates, `agentCosts`, and optional `constraints`, and returns the index of the best agent to use.  It iterates through each agent, calculates its total empirical utility (onwards correctness divided by cost), applies any constraints, and selects the agent with the highest utility.\n\n\n\n**Purpose of the Algorithms:**\n\nThe overall purpose of these algorithms is to dynamically select the most appropriate agent for a given task within a multi-agent system.  This is achieved by estimating the performance and cost of each agent in real-time and considering real-world constraints. The framework allows for online learning of agent capabilities as more data is observed. The \"appropriateness of orchestration\" concept helps to determine when this dynamic selection process is more beneficial than randomly selecting agents or simply using a single best agent.  The Rogers' Paradox example and the user study demonstrate the practical applications and benefits of this orchestration framework.",
  "simpleQuestion": "When is multi-agent orchestration worthwhile?",
  "timestamp": "2025-03-19T06:02:24.722Z"
}