{
  "arxivId": "2502.20073",
  "title": "Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents",
  "abstract": "Large language models (LLMs) based agent systems have made great strides in real-world applications beyond traditional NLP tasks. This paper proposes a new LLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on the popular Overcooked-AI game with more applicable and challenging tasks in interactive environments. Collab-Overcooked extends existing benchmarks from two novel perspectives. First, it provides a multi-agent framework supporting diverse tasks and objectives and encourages collaboration through natural language communication. Second, it introduces a spectrum of process-oriented evaluation metrics to assess the fine-grained collaboration capabilities of different LLM agents, a dimension often overlooked in prior work. We conduct extensive experiments over 10 popular LLMs and show that, while the LLMs present a strong ability in goal interpretation, there is a significant discrepancy in active collaboration and continuous adaption that are critical for efficiently fulfilling complicated tasks. Notably, we highlight the strengths and weaknesses in LLM-MAS and provide insights for improving and evaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30 open-ended tasks, and an integrated evaluation package are now publicly available at https://github.com/YusaeMeow/Collab-Overcooked.",
  "summary": "This paper introduces Collab-Overcooked, a benchmark for evaluating how well large language models (LLMs) can work together as agents in a multi-agent system.  It uses a modified version of the Overcooked-AI game where two LLM agents, a chef and an assistant, must communicate and coordinate to complete cooking tasks.\n\nKey points for LLM-based multi-agent systems:\n\n* **Collaboration definition:**  The benchmark defines collaboration as the ability to both *initiate* requests for help and *respond* effectively to those requests.\n* **Forced Collaboration and Asymmetric Knowledge:** Agents operate in isolated environments with different capabilities and only one agent knows the recipe, forcing them to collaborate through natural language.\n* **Process-Oriented Evaluation:**  Instead of just measuring task completion, Collab-Overcooked introduces new metrics (TES and ITES) to assess the *process* of collaboration, looking at the efficiency and correctness of individual actions within a task sequence.\n* **Scalability and Bottlenecks:** Experiments with various LLMs reveal that while larger models generally perform better, all models struggle with increasingly complex tasks, revealing a bottleneck in maintaining consistent collaboration and adapting to dynamic situations.  Initiating collaboration is a bigger challenge than responding.\n* **Task Decomposition, Context Tracking:** Task decomposition influences performance, but does not fully explain the decline in collaboration as tasks get harder.  LLMs also exhibit a strong positional dependence in their ability to execute action sequences, suggesting limited context tracking abilities.",
  "takeaways": "This paper introduces Collab-Overcooked, a benchmark for evaluating the collaboration capabilities of LLM-based multi-agent systems. Here's how a JavaScript developer can apply these insights to their projects:\n\n**1. Building Collaborative Web Applications:**\n\n* **Scenario:** Imagine building a collaborative document editing application where multiple users, represented by LLM agents, work together on a single document.\n* **Application:** Use the Collab-Overcooked principles to define specific roles for each agent (e.g., \"writer,\" \"editor,\" \"fact-checker\"). Implement \"resource isolation\" by assigning different sections or permissions to each agent.  Facilitate communication through a message queue system (e.g., using libraries like Socket.IO or a serverless function backend). Track actions and evaluate collaboration effectiveness using metrics inspired by TES and ITES, adapted for document editing (e.g., character additions/deletions, edits accepted/rejected).  This setup lets you evaluate which agent collaborations and LLM prompts work best in this real-world use-case.\n\n* **Scenario:** Develop a multi-agent project management tool where LLMs assist with task allocation, progress tracking, and communication.\n* **Application:** Define clear task descriptions (like recipes in Collab-Overcooked) and use a message queue for inter-agent communication.  Leverage JavaScript frameworks like React or Vue.js to build the frontend UI.  Track agent interactions and measure their effectiveness in meeting project milestones, adapting metrics like PC.  You could even implement \"asymmetric knowledge,\" where some agents have specialized project information or capabilities.  This example shows that the framework provided in the paper can be generalized to many collaborative scenarios.\n\n**2. Experimenting with Collaboration Strategies:**\n\n* **Scenario:** Explore different communication protocols for LLM agents in a chat application.\n* **Application:** Use LangChain.js or a similar framework to manage the LLM interactions.  Implement different communication strategies (e.g., direct requests, broadcast messages, negotiation protocols) and assess their impact on conversation flow and task completion.  This will allow you to determine which conversation strategies result in the best collaborations for different agents and tasks.\n\n\n**3. Addressing Collaboration Bottlenecks:**\n\n* **Scenario:** Improve the \"initiating collaboration\" capability of LLMs in a collaborative code editor.\n* **Application:** Use the insights from the failure analysis in the paper.  Specifically, focus on providing clearer context and task descriptions in the prompts. Implement \"reflection\" mechanisms to allow agents to learn from past failed collaborations. This will result in agents that are less likely to initiate pointless or repetitive collaboration.\n\n**4. JavaScript Libraries and Frameworks:**\n\n* **LangChain.js:**  Ideal for managing prompts, chains, and memory for LLM agents.\n* **Socket.IO:** Enables real-time, bidirectional communication between agents in web applications.\n* **React/Vue.js:** Useful for building interactive UIs for multi-agent applications.\n* **Serverless functions (e.g., AWS Lambda, Google Cloud Functions):**  Can be used to create a backend for managing agent communication and state.\n\n\n**Key Takeaways for JavaScript Developers:**\n\n* **Collaboration is crucial:**  Multi-agent systems can significantly improve the performance and capabilities of LLM-powered applications.\n* **Focus on the process:**  Don't just focus on task completion.  Analyze the interaction process to identify collaboration bottlenecks and improve agent strategies.\n* **Communication is key:**  Ensure efficient and effective communication between agents using appropriate protocols and message formats.\n* **Experiment and iterate:** Use the Collab-Overcooked principles as a starting point and experiment with different strategies to find what works best for your specific application.\n\n\nBy understanding the principles and challenges of LLM-based multi-agent collaboration highlighted in this paper, JavaScript developers can build more sophisticated and effective web applications that leverage the power of multiple interacting AI agents.",
  "pseudocode": "```javascript\n// Function to calculate the Trajectory Efficiency Score (TES)\nfunction calculateTES(historicalActions, referenceActions, beta = 0.95) {\n  let maxTES = 0;\n\n  for (const rat of referenceActions) { // Iterate through all RATs\n    const mk = rat.length;\n    const nk = historicalActions.length;\n    const dMax = calculateDMax(historicalActions, rat);\n    const tes = (1 + beta) * dMax / (mk + (beta * beta * nk));\n    maxTES = Math.max(maxTES, tes);\n  }\n  return maxTES;\n}\n\n\n// Helper function to calculate DMax (Longest Order-Preserving Subsequence)\nfunction calculateDMax(historicalActions, rat) {\n  // Implementation of longest order-preserving subsequence algorithm.\n  // This can be achieved using dynamic programming similar to LCS.\n  let n = historicalActions.length;\n  let m = rat.length;\n  let dp = Array(n + 1).fill(0).map(() => Array(m + 1).fill(0));\n\n\n  for (let i = 1; i <= n; i++) {\n    for (let j = 1; j <= m; j++) {\n      if (historicalActions[i - 1] === rat[j - 1]) {\n        dp[i][j] = dp[i - 1][j - 1] + 1;\n      } else {\n        dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]);\n      }\n    }\n  }\n  return dp[n][m];\n}\n\n\n\n\n// Function to calculate the Incremental Trajectory Efficiency Score (ITES)\nfunction calculateITES(action, historicalActions, referenceActions, beta = 0.95) {\n  const tesBefore = calculateTES(historicalActions, referenceActions, beta);\n  const tesAfter = calculateTES([...historicalActions, action], referenceActions, beta);\n  return tesAfter - tesBefore;\n}\n\n\n// Example usage (replace with actual action and RAT data)\nconst historicalActions = [\"pickup(onion, dispenser)\", \"place_obj_on_counter()\"];\nconst referenceActions = [\n  [\"pickup(onion, dispenser)\", \"place_obj_on_counter()\", \"pickup(tomato, dispenser)\", \"place_obj_on_counter()\"],\n  // ... other possible RATs\n];\nconst action = \"pickup(tomato, dispenser)\";\n\nconst tes = calculateTES(historicalActions, referenceActions);\nconst ites = calculateITES(action, historicalActions, referenceActions);\n\n\n\nconsole.log(\"TES:\", tes);\nconsole.log(\"ITES:\", ites);\n\n\n```\n\n**Explanation of the Algorithms:**\n\n1. **`calculateTES(historicalActions, referenceActions, beta)`:**\n   - **Purpose:** Computes the Trajectory Efficiency Score (TES) which measures how closely an agent's historical actions align with a given set of Referential Action Trajectories (RATs).\n   - **Algorithm:** Iterates through each RAT in `referenceActions`. For every RAT, it calculates the length of the Longest Order-Preserving Subsequence (`dMax`) using the helper function `calculateDMax`. It then calculates the TES for that specific RAT. Finally, it returns the *maximum* TES across all RATs.\n\n2. **`calculateDMax(historicalActions, rat)`:**\n   - **Purpose:** Calculates the length of the Longest Order-Preserving Subsequence (similar to Longest Common Subsequence, but preserving the order) between the `historicalActions` and a single `rat`.\n   - **Algorithm:**  Uses dynamic programming.  It creates a 2D array `dp` where `dp[i][j]` stores the length of the LOPS between the first `i` elements of `historicalActions` and the first `j` elements of `rat`. The values in `dp` are computed iteratively using the following logic:\n      - If `historicalActions[i-1]` is equal to `rat[j-1]`, then `dp[i][j] = dp[i-1][j-1] + 1` (we extend the LOPS).\n      - Otherwise, `dp[i][j] = max(dp[i-1][j], dp[i][j-1])` (we take the maximum LOPS length from either skipping an element in `historicalActions` or skipping an element in `rat`).\n\n3. **`calculateITES(action, historicalActions, referenceActions, beta)`:**\n   - **Purpose:** Computes the Incremental Trajectory Efficiency Score (ITES). It measures the improvement in TES achieved by adding a single `action` to the `historicalActions`.\n   - **Algorithm:**  Calculates the TES *before* adding the action and the TES *after* adding the action. The ITES is the difference between these two TES values.\n\n\n**Overall Purpose:**\n\nThese JavaScript functions implement the TES and ITES metrics described in the research paper.  They are used to evaluate the performance of collaborating agents in task-oriented environments. TES gives a coarse measure of how well the agent is progressing, and ITES provides a fine-grained measure of how each individual action contributes to progress. The higher the TES and ITES scores, the better the agent is performing. These metrics are important because they help analyze collaboration capabilities, identify bottlenecks, and improve the design of multi-agent systems.",
  "simpleQuestion": "How can LLMs best collaborate in complex tasks?",
  "timestamp": "2025-02-28T06:08:49.159Z"
}