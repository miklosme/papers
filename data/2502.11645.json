{
  "arxivId": "2502.11645",
  "title": "DEVIATION RATINGS: A GENERAL, CLONE INVARIANT RATING METHOD",
  "abstract": "Many real-world multi-agent or multi-task evaluation scenarios can be naturally modelled as normal-form games due to inherent strategic (adversarial, cooperative, and mixed motive) interactions.  These strategic interactions may be agentic (e.g. players trying to win), fundamental (e.g. cost vs quality), or complementary (e.g. niche finding and specialization). In such a formulation, it is the strategies (actions, policies, agents, models, tasks, prompts, etc.) that are rated. However, the rating problem is complicated by redundancy and complexity of N-player strategic interactions. Repeated or similar strategies can distort ratings for those that counter or complement them. Previous work proposed \"clone invariant\" ratings to handle such redundancies, but this was limited to two-player zero-sum (i.e. strictly competitive) interactions. This work introduces the first N-player general-sum clone invariant rating, called deviation ratings, based on coarse correlated equilibria. The rating is explored on several domains including LLMs evaluation.",
  "summary": "This paper introduces *deviation ratings*, a new method for evaluating the performance of multiple AI agents (or strategies) interacting strategically.  It addresses the problem of traditional rating systems being skewed by the inclusion of many similar strategies, particularly in complex scenarios like those found in modern LLM evaluations.\n\nFor LLM-based multi-agent systems, deviation ratings offer several benefits:\n\n* **Clone Invariance:** Adding copies of existing agents doesn't change the ratings, making the evaluation robust to redundant data and preventing manipulation through the submission of many similar prompts or models.\n* **N-player General-Sum Applicability:** Unlike simpler methods like Elo, deviation ratings work for any number of agents and in scenarios where interactions aren't strictly competitive (e.g., cooperative or mixed-motive situations). This is highly relevant to complex LLM interactions, such as those involving multiple models and prompts.\n* **Focus on Distinguishing Tasks:** The method identifies tasks that are most effective at differentiating between top-performing models, providing valuable insights for dataset curation and targeted model improvement.\n* **Promoting Holistic Improvement:** By focusing on the strictest equilibrium in a multi-agent setting, deviation ratings encourage the development of more robust and generally capable LLMs rather than those over-specialized in narrow areas.",
  "takeaways": "This research paper introduces Deviation Ratings, a novel approach for evaluating and improving LLM-based multi-agent systems. Here's how a JavaScript developer can apply these insights:\n\n**1. LLM-based Chatbot Arena with Clone Invariance:**\n\n* **Scenario:**  Imagine building a chatbot arena similar to Chiang et al. (2024), but with multiple chatbots interacting in a free-flowing conversation.  Users judge the overall quality of the interaction.  The challenge is that some developers might submit multiple slightly modified versions (\"clones\") of the same chatbot to inflate their ratings.\n* **Applying Deviation Ratings:** Implement a rating system in JavaScript using the Deviation Ratings algorithm. This involves formulating the interactions as a normal-form game (NFG). Each chatbot is a player, and their possible dialogue turns are their strategies. Payoffs are derived from user ratings. Libraries like `linear-program-solver` can be used to solve the linear programming problem within the Deviation Ratings algorithm.  This ensures that cloned chatbots don't distort ratings, leading to fairer evaluations and encouraging genuine innovation.\n\n```javascript\n// Simplified example of calculating deviation ratings\n// Using a hypothetical linear programming library\n\nimport { solve } from 'linear-program-solver';\n\n// Construct the constraint matrix A and vector b\n// based on chatbot interactions and user ratings\n\nconst solution = solve(A, b);\n\n// Extract deviation ratings from the solution\nconst deviationRatings = solution.objectiveValue;\n\n// ... further processing and display of ratings\n```\n\n**2. Multi-Task LLM Benchmarking:**\n\n* **Scenario:** You are developing an LLM benchmark similar to LiveBench (White et al., 2024), but expanded to include cooperation and competition between LLMs. You want to evaluate LLMs across various tasks, including question answering, text summarization, and code generation.  Some tasks might correlate strongly, so a clone-invariant approach is necessary.\n* **Applying Deviation Ratings:** Construct a model vs. model vs. task NFG, where each LLM and the task itself are players. Use Deviation Ratings to evaluate the performance of the LLMs, making the evaluation robust to redundant tasks and incentivizing diverse LLM strengths. Display the results on a web dashboard using JavaScript frameworks like React or Vue.js. Visualize the task contributions (as in Figure 2b) to provide insights into each LLM's strengths and weaknesses across tasks.\n\n**3.  Dynamic Content Generation in Multi-Agent Web Apps:**\n\n* **Scenario:**  Developing a web application with multiple LLM agents that dynamically generate website content (text, images, etc.) based on user interactions.  The agents may cooperate or compete to provide the best user experience, and some agents may specialize in particular types of content.\n* **Applying Deviation Ratings:** Use Deviation Ratings to evaluate and improve the performance of these content-generating agents. Formulate an NFG where the agents are players, and their content generation strategies are actions. Payoffs could be based on user engagement metrics (time spent on page, click-through rates). This ensures that specialized agents are appropriately valued, even if some agents produce similar outputs. Implement the rating updates in a Node.js backend and communicate with the frontend using WebSockets for dynamic updates.\n\n**4. A/B Testing with Multi-Agent Systems:**\n\n* **Scenario:** You are A/B testing different versions of a multi-agent system on a webpage. Each system consists of several LLMs that interact to personalize user recommendations.\n* **Applying Deviation Ratings:** Use Deviation Ratings to evaluate the performance of each multi-agent system in the A/B test. Formulate the interaction as an NFG where the multi-agent systems are players, and different configurations are their strategies. Payoffs are derived from conversion rates. Use the rating results to select the best-performing multi-agent system for deployment.\n\nBy formulating these scenarios as NFGs and implementing the Deviation Ratings algorithm in JavaScript, developers can create more robust, fair, and scalable LLM-based multi-agent systems.  This approach opens up exciting possibilities for more complex and dynamic web applications, driving innovation in user interfaces, personalized content generation, and other areas of web development.  Furthermore, visualizing the results and task contributions through web dashboards makes the evaluation process transparent and provides valuable insights for continuous improvement of these AI systems.",
  "pseudocode": "```javascript\nfunction calculateCCEDeviationRating(G) {\n  // G is the payoff function, represented as a multi-dimensional array or object.\n  // It should be accessible as G[p][a1][a2]...[aN] for N players.\n  // The function returns an object containing deviation ratings for each player and strategy.\n\n  const numPlayers = G.length;\n  const numStrategies = G.map(p => p.length);\n\n  // Initialize ratings and active constraints\n  const ratings = G.map(p => Array(p.length).fill(0));\n  const activeConstraints = Array(numPlayers).fill(null).map(() => new Set());\n\n  while (activeConstraints.some((constraints, p) => constraints.size < numStrategies[p])) {\n    let maxDeviation = -Infinity;\n    let maxDeviationPlayer = -1;\n    let maxDeviationStrategy = -1;\n\n\n    for (let p = 0; p < numPlayers; p++) {\n      for (let ap = 0; ap < numStrategies[p]; ap++) {\n        if (!activeConstraints[p].has(ap)) {\n          // Solve LP to calculate deviation gain for this strategy\n\n          // Construct LP problem.  The exact formulation depends\n          // on the LP solver used. The general idea is to minimize\n          // the deviation for the current player p and strategy ap\n          // subject to existing constraints defined in ratings and\n          // activeConstraints. Consult LP documentation for\n          // concrete implementations. Placeholder for illustration:\n\n          const deviation = solveLP(G, p, ap, ratings, activeConstraints);\n\n          if (deviation > maxDeviation) {\n            maxDeviation = deviation;\n            maxDeviationPlayer = p;\n            maxDeviationStrategy = ap;\n          }\n\n        }\n\n      }\n    }\n\n    // Update ratings and active constraints based on the maximum deviation\n    ratings[maxDeviationPlayer][maxDeviationStrategy] = maxDeviation;\n    activeConstraints[maxDeviationPlayer].add(maxDeviationStrategy);\n\n    for(let p = 0; p < numPlayers; p++){\n        for(let ap = 0; ap < numStrategies[p]; ap++){\n            if(!activeConstraints[p].has(ap)){\n                ratings[p][ap] = solveLP(G, p, ap, ratings, activeConstraints);\n\n            }\n\n        }\n\n    }\n  }\n\n\n  return ratings;\n}\n\nfunction solveLP(G, p, ap, ratings, activeConstraints){\n\n    // Placeholder function to indicate the structure needed to solve the\n    // linear program. Adapt to whatever concrete LP solver is being used.\n\n    // Here are some general considerations:\n\n    // Objective: Minimize deviation of Gp(ap, a-p) - Gp(a) under distribution sigma\n    // Variables: sigma(a)\n    // Constraints: sigma sums to 1, sigma >= 0\n    // Additionally: for all active constraints in activeConstraints, the deviation should be equal to its rating\n\n\n    // Example using a fictional LP solver interface:\n    const lpSolver = new LPSolver();\n    // Add objective\n    // Add variables\n    // Add constraints based on activeConstraints\n    // Add basic CCE constraints\n    const solution = lpSolver.solve();\n\n\n    return solution.objectiveValue; // Return deviation\n}\n\n\n\n\n\n```\n\n**Explanation of the Algorithm (Algorithm 1 in the paper):**\n\nThe CCE Deviation Rating algorithm calculates a rating for each strategy in a normal-form game based on the concept of Coarse Correlated Equilibrium (CCE).  Its core idea is to iteratively minimize the maximum deviation gain a player can achieve by switching from a recommended strategy to another strategy, given a joint probability distribution over all players' strategies (the CCE).\n\n**Purpose:** To provide a robust, clone-invariant rating system that's applicable to general-sum, N-player games. This ensures that the ratings are not skewed by redundant strategies (clones) and offer a good measure of a strategy's effectiveness in complex multi-agent scenarios.\n\n**JavaScript Implementation Notes:**\n\n1. **Payoff Function Representation:** The JavaScript code assumes the payoff function `G` is represented as a nested array or object, making it straightforward to access payoffs for any combination of player strategies.\n\n2. **LP Solver Integration:** The `solveLP` function is a placeholder and needs to be replaced with a real implementation using a JavaScript LP solver library.  There are several options available, including open-source and commercial solvers.  The implementation needs to be adapted to the specific API of the chosen solver.\n\n3. **Constraint Handling:** The code uses sets (`activeConstraints`) to keep track of which constraints are active in each iteration. This ensures the LP solver only considers the relevant constraints and that the ratings are computed correctly.\n\n4. **Numerical Stability:** Quantization, as mentioned in the paper, can be implemented within the `solveLP` function or as a pre-processing step to the input payoffs to handle potential numerical instability issues arising from very small differences in floating-point numbers.\n\n\n**Key Improvements over Traditional Rating Methods:**\n\n* **Clone Invariance:** Unlike Elo or other rating methods that can be manipulated by adding copies of existing strategies, the CCE Deviation Rating is unaffected by clones.\n* **N-player General-Sum Applicability:** It works for a broader class of games compared to many other methods limited to 2-player zero-sum games.\n* **Robustness and Stability:** By focusing on the strictest equilibrium, it identifies strategies that are less susceptible to deviations, providing a more stable measure of their effectiveness.\n\n\nThis translation and explanation should give JavaScript developers a clear understanding of the CCE Deviation Rating algorithm and a solid foundation to implement and experiment with it in multi-agent applications using LLMs.",
  "simpleQuestion": "How can I fairly rate LLMs in multi-agent games?",
  "timestamp": "2025-02-18T06:04:03.352Z"
}