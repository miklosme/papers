{
  "arxivId": "2411.03519",
  "title": "AI METROPOLIS: SCALING LARGE LANGUAGE MODEL-BASED MULTI-AGENT SIMULATION WITH OUT-OF-ORDER EXECUTION",
  "abstract": "With more advanced natural language understanding and reasoning capabilities, large language model (LLM)-powered agents are increasingly developed in simulated environments to perform complex tasks, interact with other agents, and exhibit emergent behaviors relevant to social science research and innovative gameplay development. However, current multi-agent simulations frequently suffer from inefficiencies due to the limited parallelism caused by false dependencies, resulting in performance bottleneck. In this paper, we introduce AI Metropolis, a simulation engine that improves the efficiency of LLM agent simulations by incorporating out-of-order execution scheduling. By dynamically tracking real dependencies between agents, AI Metropolis minimizes false dependencies, enhancing parallelism and enabling efficient hardware utilization. Our evaluations demonstrate that AI Metropolis achieves speedups from 1.3× to 4.15× over standard parallel simulation with global synchronization, approaching optimal performance as the number of agents increases.",
  "summary": "This paper introduces AI Metropolis, a simulation engine designed to make large language model (LLM)-based multi-agent simulations faster and more efficient.  Current simulations are slow due to unnecessary synchronization between agents, limiting how many LLM requests can be processed concurrently. AI Metropolis solves this by using \"out-of-order execution,\" allowing independent agents to progress at different speeds based on their individual workloads and dependencies.  This is achieved by tracking the true dependencies between agents using a spatiotemporal graph, grouping related agents into clusters, and prioritizing tasks based on their simulation time step.  This approach improves performance significantly, approaching the theoretical optimal throughput.  Key to LLM-based systems is the focus on optimizing LLM inference throughput, the elimination of false dependencies through runtime dependency tracking, and the prioritizing of requests within the simulation's critical path.",
  "takeaways": "This paper introduces AI Metropolis, a simulation engine designed to improve the efficiency of LLM-based multi-agent simulations by implementing out-of-order execution. Here are practical examples demonstrating how JavaScript developers can apply these insights to their LLM-based multi-agent projects, focusing on web development scenarios:\n\n**1. Simulating a Collaborative Virtual World:**\n\nImagine building a virtual world where LLM-powered agents collaborate on tasks, like a collaborative code editor or a virtual design studio.  Users interact with these agents via a web interface built with a framework like React or Vue.js.\n\n* **Challenge:**  Traditional step-based simulations create bottlenecks when some agents take longer to respond due to complex LLM prompts or network latency. This leads to a jerky, unresponsive user experience.\n* **AI Metropolis Solution:** Implement a dependency graph in JavaScript. Track which agents are coupled (need to synchronize) based on their proximity and interactions within the virtual world. Use a library like `vis-network` or `cytoscape.js` to visualize and manage the dependency graph. Only synchronize coupled agents, allowing independent agents to proceed asynchronously.  This dramatically improves responsiveness, as demonstrated in the paper.\n* **Code Example (Conceptual):**\n```javascript\n// Dependency graph using vis-network (simplified)\nconst nodes = new vis.DataSet([\n  { id: 'agent1', label: 'Agent 1', step: 0 },\n  { id: 'agent2', label: 'Agent 2', step: 0 },\n  // ... more agents\n]);\n\nconst edges = new vis.DataSet([\n  // Coupled agents have an edge\n  { from: 'agent1', to: 'agent2' } \n]);\n\nconst data = { nodes, edges };\nconst network = new vis.Network(container, data, options);\n\n// Check for dependencies before advancing an agent\nfunction canAgentProceed(agentId) {\n  // Check if any coupled agents are behind\n  const coupledAgents = network.getConnectedNodes(agentId);\n  for (const coupledAgent of coupledAgents) {\n    if (nodes.get(coupledAgent).step < nodes.get(agentId).step) {\n      return false;\n    }\n  }\n  return true;\n}\n\n\nasync function agentAction(agentId) {\n if(canAgentProceed(agentId)) {\n  const llmResponse = await queryLLM(agent.prompt) // using Langchain, or other LLM lib\n  nodes.update({ id: agentId, step: nodes.get(agentId).step + 1 }); // update the dependency graph as well\n  // process and render the LLM response\n }\n}\n\n```\n\n**2. Multi-Agent Chat Application:**\n\nConsider building a customer support chat application where multiple LLM agents specialize in different areas (e.g., billing, technical support).  A user's question is routed to the relevant agents.\n\n* **Challenge:**  If agents need to collaborate, waiting for all agents to respond before presenting the user with an answer can create delays.\n* **AI Metropolis Solution:**  Use a priority queue in JavaScript to manage agent responses.  Prioritize agents whose responses are on the \"critical path\" of the conversation, as defined by the dependency graph, or by pre-defined rules of your system.  This ensures that the most important information is presented to the user first, even if other agents are still processing.  Use a library like `priorityqueue` to implement the priority queue.\n\n**3. Real-time Strategy Game with LLM-powered Bots:**\n\nDevelop a browser-based RTS game where players compete against LLM-controlled bots.\n\n* **Challenge:**  Synchronizing all game actions across all bots at each time step can limit the scalability of the game.\n* **AI Metropolis Solution:**  Implement agent clustering. Group bots that are close together and likely to interact into clusters. Within a cluster, use parallel-sync to manage agent actions, but allow clusters to operate independently and asynchronously. This reduces synchronization overhead and improves performance, enabling more bots and a more complex game world.\n\n**Key JavaScript Tools and Libraries:**\n\n* **`vis-network` or `cytoscape.js`:** For visualizing and managing dependency graphs.\n* **`priorityqueue`:** For implementing priority queues for agent responses or actions.\n* **Langchain or other LLM client libraries:**  For interacting with LLMs.\n* **WebSockets:** For real-time communication between the client and server for dynamic updates.\n* **React, Vue.js, or other front-end frameworks:** To build dynamic and responsive user interfaces.\n\nBy applying these concepts and using appropriate JavaScript tools, developers can create highly performant and scalable multi-agent AI applications for the web, leveraging the insights of AI Metropolis to unlock the full potential of LLM agents. Remember that the paper's emphasis on out-of-order execution, dependency management, and agent clustering are key to achieving efficient scaling in these complex simulated environments.",
  "pseudocode": "Here are the JavaScript versions of the pseudocode algorithms from the paper, along with explanations:\n\n```javascript\n// Algorithm 1: Traditional Simulation Scheduling\nfunction traditionalSimulation(targetStep, agents, world) {\n  let step = 0;\n  while (step < targetStep) {\n    let actions = [];\n    for (const agent of agents) {\n      actions.push(agent.proceed(world)); // LLM call within proceed\n    }\n    world.step(actions);\n    step++;\n  }\n}\n\n// Explanation:\n// This algorithm represents the standard way of stepping through a multi-agent \n// simulation.  It iterates through a fixed number of steps. In each step, it collects\n// actions from all agents based on the current world state.  It then updates \n// the world state based on these collected actions. This synchronous approach \n// ensures temporal consistency but can lead to inefficiencies due to agents idling \n// while waiting for others to finish their LLM calls.\n\n\n\n// Algorithm 2: Proceed Function in GenAgent\nfunction agentProceed(agent, world) {\n  const perceivedEvents = agent.perceive(world);\n  const retrievedEvents = agent.retrieve(perceivedEvents);\n  const action = agent.plan(world, retrievedEvents);\n  return action;\n}\n\n// Explanation:\n// This represents the behavior of an individual agent within a time step.  \n// An agent first perceives its environment, then retrieves relevant memories based \n// on what it perceived, and finally plans its next action. Each of these steps \n// (perceive, retrieve, plan) could involve calls to an LLM.\n\n\n// Algorithm 3: AI Metropolis Scheduling Workflow\n// (Simplified for clarity - omits details like priority queue, conflict resolution)\n\n// Controller process\nfunction metropolisController(targetStep, agents, world) {\n  let baseStep = 0;\n  let readyAgents = agents;\n  const workerPool = new WorkerPool(workerRoutine);\n\n  while (baseStep < targetStep) {\n    const readyClusters = geoClustering(readyAgents);\n\n    for (const cluster of readyClusters) {\n      workerPool.submitJob(cluster, world); // Submit clusters to workers\n    }\n\n    // ... (Omitted: Receiving acknowledgements from workers, updating readyAgents, baseStep)\n  }\n}\n\n// Worker process\nfunction workerRoutine(cluster, world) {\n  let actions = [];\n  for (const agent of cluster) {\n      actions.push(agent.proceed(world));\n  }\n  world.resolveConflictAndCommit(actions);\n  // ... (Omitted: Update dependency graph)\n\n}\n\n// Explanation:\n// This algorithm presents the core of AI Metropolis, showing its asynchronous and \n// out-of-order execution nature.  The controller clusters agents based on their \n// spatial proximity and submits these clusters as jobs to a worker pool. Workers \n// process the actions of agents within their assigned cluster.  By allowing \n// clusters to proceed independently, AI Metropolis maximizes parallelism and \n// reduces idle time. The omitted parts handle dependency management and step \n// updates which are more complex to represent concisely in JavaScript pseudocode\n// but are crucial for the algorithm's correctness.\n\n\n// Helper function (simplified)\nfunction geoClustering(agents) {\n  const clusters = [];\n  // ... Logic to group agents based on distance\n  return clusters;\n}\n```\n\nKey improvements in Algorithm 3 (AI Metropolis) compared to Algorithm 1 (Traditional):\n\n* **Asynchronous Processing:**  Agents are grouped into clusters, and those clusters can proceed independently.  This allows agents in different clusters to be at different simulation steps simultaneously.\n* **Out-of-Order Execution:**  The `baseStep` is only advanced when dependencies are met.  This allows agents who are not blocked to move ahead, even if other agents are at earlier steps.  This dramatically increases parallelism and reduces the impact of imbalanced workloads from LLM calls.\n\n\nThis restructuring of the simulation loop is central to AI Metropolis's performance gains. It decouples the simulation progression from the lock-step approach, allowing much better hardware utilization and shorter completion times.",
  "simpleQuestion": "How to speed up LLM agent simulations?",
  "timestamp": "2024-11-07T06:02:23.305Z"
}