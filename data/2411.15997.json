{
  "arxivId": "2411.15997",
  "title": "ENSURING FAIR LLM SERVING AMID DIVERSE APPLICATIONS",
  "abstract": "In a multi-tenant large language model (LLM) serving platform hosting diverse applications, some users may submit an excessive number of requests, causing the service to become unavailable to other users and creating unfairness. Existing fairness approaches do not account for variations in token lengths across applications and multiple LLM calls, making them unsuitable for such platforms. To address the fairness challenge, this paper analyzes millions of requests from thousands of users on MS CoPilot, a real-world multi-tenant LLM platform hosted by Microsoft. Our analysis confirms the inadequacy of existing methods and guides the development of FAIRSERVE, a system that ensures fair LLM access across diverse applications. FAIRSERVE proposes application-characteristic aware request throttling coupled with a weighted service counter based scheduling technique to curb abusive behavior and ensure fairness. Our experimental results on real-world traces demonstrate FAIRSERVE's superior performance compared to the state-of-the-art method in ensuring fairness. We are actively working on deploying our system in production, expecting to benefit millions of customers world-wide.",
  "summary": "This paper introduces FAIRSERVE, a system for ensuring fair access to large language models (LLMs) in multi-tenant environments where diverse applications with varying resource needs coexist.  It addresses the problem of some users or applications monopolizing LLM resources, making the service unavailable or slow for others.\n\nKey points for LLM-based multi-agent systems:\n\n* **Multi-agent LLM interactions:**  FAIRSERVE explicitly recognizes and accounts for the fact that modern LLM applications often involve multiple interconnected LLM calls (agents) forming an \"interaction graph\" to generate a single user response. Existing fairness approaches typically don't account for this.\n* **Interaction-aware throttling:**  Instead of throttling individual LLM requests, FAIRSERVE throttles at the interaction level and only during system overload, minimizing wasted compute and incomplete user responses common with standard rate-limiting.\n* **Application-specific weights:**  FAIRSERVE uses a weighted service counter that considers application-specific characteristics (like expected input/output token lengths) to calculate the service received by users. This ensures fairness by accounting for the diverse resource demands of different applications.  \n* **Reduced queueing delays:** By prioritizing users in the middle of a multi-agent interaction, FAIRSERVE significantly reduces queueing delays and improves overall user experience.",
  "takeaways": "This paper introduces FAIRSERVE, a system for ensuring fairness in multi-tenant LLM serving platforms, particularly relevant for multi-agent LLM applications. Here's how JavaScript developers can apply these insights to their projects:\n\n**1. Overload and Interaction-driven Throttling (OIT) in JavaScript:**\n\n* **Scenario:** A web app uses multiple LLM agents for tasks like summarizing articles, generating code, and translating text.  Users might overload the system with requests, especially during peak hours.\n* **Implementation:**  A JavaScript developer can implement a client-side rate-limiting mechanism using libraries like `axios-rate-limit` or a custom implementation with the browser's `localStorage` or `sessionStorage`.  This can prevent users from sending excessive requests.  On the server-side,  monitor the system load (e.g., CPU/memory usage, request queue length) using Node.js performance hooks or dedicated monitoring tools. When overload is detected, throttle new requests at the *interaction* level. This means delaying the start of a new multi-agent interaction (e.g., summarizing and translating an article) rather than individual LLM calls (e.g., just summarizing).\n\n```javascript\n// Client-side rate limiting with axios-rate-limit\nimport rateLimit from 'axios-rate-limit';\n\nconst http = rateLimit(axios.create(), { maxRequests: 5, perMilliseconds: 60000 }); // 5 requests per minute\n\n// Server-side overload detection (simplified example)\nconst os = require('os');\n\nsetInterval(() => {\n  const loadAverage = os.loadavg()[0];\n  if (loadAverage > os.cpus().length) { // Overload detected\n    // Throttle new interactions\n    isOverloaded = true;\n  } else {\n    isOverloaded = false;\n  }\n}, 5000); // Check every 5 seconds\n```\n\n**2. Weighted Service Counter (WSC) in JavaScript:**\n\n* **Scenario:** A collaborative writing platform where different users are using LLM agents for different tasks (e.g., grammar correction, style suggestions, idea generation). Each task has varying token requirements.\n* **Implementation:** Maintain a service counter for each user on the server. When a user initiates a multi-agent interaction, calculate the expected total tokens based on the application characteristics (e.g., average tokens for grammar correction, style suggestions, etc.). As LLM calls complete, update the user's service counter based on the *ratio* of tokens processed to the expected total tokens.  Prioritize users with lower service counter values using a priority queue implementation in JavaScript (e.g., using a min-heap data structure).\n\n```javascript\n// Simplified WSC logic (server-side)\nconst userCounters = new Map();\nconst priorityQueue = []; // Implement as min-heap\n\nfunction calculateWeight(app, stage) {\n  // Fetch average token lengths from database or configuration\n  // ...\n  return weight;\n}\n\nfunction updateServiceCounter(user, app, stage, processedTokens) {\n  const weight = calculateWeight(app, stage);\n  const expectedTokens =  // ... get expected tokens for app and stage\n  const serviceIncrement = processedTokens / expectedTokens;\n\n  let counter = userCounters.get(user) || 0;\n  counter += serviceIncrement;\n  userCounters.set(user, counter);\n\n  // Update priority queue\n  // ...\n}\n\n// Prioritize requests based on service counter\n// ...\n```\n\n\n**3. Multi-agent Interaction Tracking:**\n\n* **Scenario:** A chatbot application where different LLM agents handle different conversation stages (e.g., intent recognition, response generation, context management).\n* **Implementation:**  Use a unique identifier for each interaction. Track the progress of each interaction (e.g., which stages are completed, which LLM agents are involved) on the server. This information is essential for OIT (to avoid throttling mid-interaction) and WSC (to calculate service accurately).\n\n**JavaScript Frameworks/Libraries:**\n\n* **Node.js:** For server-side logic, overload detection, and managing service counters.\n* **Express.js/NestJS:**  For building APIs and handling user requests.\n* **LangchainJS:**  For orchestrating LLM calls and managing multi-agent interactions.\n* **Axios:** For making HTTP requests to LLM providers.\n* **Data structures libraries (e.g., `heap.js`):** For implementing priority queues for WSC.\n\n\nBy incorporating these principles, JavaScript developers can build more robust and fair multi-agent LLM applications, ensuring that all users have equitable access to resources and minimizing resource waste. This is especially critical as multi-agent LLM systems become more prevalent in web development.",
  "pseudocode": "The paper contains one pseudocode block, presented as Algorithm 1.  Here's the JavaScript equivalent:\n\n```javascript\nclass FairServe {\n  constructor() {\n    this.B = []; // Dynamically changing current batch\n    this.serviceCounters = new Map(); // Service counter ui ← 0 for all user i\n    this.Q = new Map(); // Dynamically changing waiting queue\n    this.Mnew = []; // New minibatch to be added to B\n    this.Tr_global = Infinity;  // Global request throttling limit (set as needed)\n    this.Tr_app = new Map(); // Request throttling limit per app\n    this.requestCounts_user = new Map(); // Count for requests for user u\n    this.requestCounts_app = new Map(); // Count for requests for app a\n  }\n\n  monitoringStream() {\n    // In a real system, this would be an event listener or a loop checking for new requests\n    //  For demonstration, we assume a 'newRequest' function provides incoming requests.\n     while (true) {\n      const r = newRequest(); // Assume this function gets the next request, or null if none\n       if (r) {\n        const u = r.user;\n        const a = r.app;\n         if (!this.Q.has(u)) {\n          let e = null; \n          if (this.Q.size === 0) { \n             // Find most recent user to exit queue if this is first request from the current user\n             // In a production system, this would require queue history.  Simplified here.\n           } else {\n             e = Math.max(...this.Q.values().map(req => req.exitTime || 0));\n          }\n             this.serviceCounters.set(u, Math.max(this.serviceCounters.get(u) || 0, e || 0)); \n          }\n         \n           this.requestCounts_user.set(u, (this.requestCounts_user.get(u) || 0) + 1);\n          this.requestCounts_app.set(a, (this.requestCounts_app.get(a) || 0) + 1);\n\n\n          if (isSystemOverloaded() && !isInInteraction(r)) {\n            if (this.requestCounts_user.get(u) > this.Tr_global) {\n              // block r\n              console.log(\"Request blocked (global limit)\", r);\n                break;\n            } else if (this.requestCounts_app.get(a) > (this.Tr_app.get(a) || Infinity)) {\n               // block r\n              console.log(\"Request blocked (app limit)\", r);\n                break; \n            }\n          }\n\n\n          if (!this.Q.has(u)) {\n              this.Q.set(u, []); \n           }\n           this.Q.get(u).push(r);\n\n\n\n        }\n        \n      }\n\n  }\n\n  executionStream() {\n    while (true) {\n       this.Mnew = [];\n       while (true) {\n          const incompleteInteractions = findIncompleteInteractions(this.B);\n           if (incompleteInteractions.length > 0) {\n\n             // Prioritize incomplete interactions. (Simplified logic here)\n              let interaction = incompleteInteractions[0];\n              //.... (rest of interaction handling logic)\n            } else {\n               if (canAddNewRequest(this.B)) {\n\n                   // Select requests using the weighted service counter (WSC) \n                  // ....(WSC logic to choose user k and request r)\n                  if (r) { \n                      this.Mnew.push(r);\n                      this.Q.get(r.user).shift(); // Remove from queue\n                   }\n               } else {\n                   break; // Exit inner loop if batch is full\n                }\n\n            }\n       } // end of inner while\n\n        this.B = this.B.concat(this.Mnew);\n         this.B.forEach(request => {\n         // ...prefill, decode, update service counters, handling finished requests, etc\n       });\n         \n    } // end of outer while\n  }\n\n  start() {\n    this.monitoringStream();\n    this.executionStream();\n  }\n\n\n}\n\n\n\n// Helper functions (placeholders – need to be implemented based on the paper's details)\n\nfunction newRequest() {\n   // Simulate receiving new requests (replace with actual request mechanism in real app)\n   return null; \n}\n\nfunction isSystemOverloaded() {\n  // Check system load based on KV cache or other metrics \n    return false;\n}\n\nfunction isInInteraction(r) {\n   // Check if the request is part of an ongoing interaction.\n    return false;\n}\n\n\nfunction findIncompleteInteractions(B) {\n   // Find any interactions in the batch B that are not complete.\n    return [];\n}\n\nfunction canAddNewRequest(B) {\n  // Check if the current batch has room for another request\n    return false;\n}\n\n\n\n// Example usage\nlet fairServe = new FairServe();\nfairServe.start();\n\n\n\n```\n\n\n\n**Explanation:**\n\n* **FAIRSERVE's Goal:** The algorithm aims to ensure fair and efficient Large Language Model (LLM) resource allocation in a multi-tenant, multi-agent environment.  It tries to prevent \"abusive\" users from monopolizing resources while maximizing throughput and minimizing latency, especially for complex LLM interactions (multiple LLM calls to fulfill a user request).\n\n* **Two Main Components:**\n    * **Overload and Interaction-driven Throttling (OIT):**  Limits resource usage by users and applications, but only when the system is overloaded (represented by `isSystemOverloaded()` in the code) and *outside* of ongoing interactions. This prevents wasting tokens on incomplete interactions.\n    * **Weighted Service Counter (WSC):** Schedules requests from users who have received the \"least service\" so far. The service is calculated using a weighted formula that considers the input, system prompt, and output token lengths, as well as the characteristics of the user's application.\n\n* **Algorithm Structure:**\n    * **`monitoringStream()`:**  Continuously monitors incoming requests, checks for throttling conditions (using OIT), and adds requests to a queue (`this.Q`).  It also performs initial service counter adjustments.\n    * **`executionStream()`:**  Forms batches of requests (`this.B`) and processes them through the LLM inference engine.  It prioritizes completing ongoing interactions and uses the WSC to select requests from the queue when adding to a batch.\n    * The code uses placeholders (`// ...`) for several parts related to actual LLM interaction handling, prefill/decode operations, system load checking, and other application-specific logic.  These need to be filled in based on the specific LLM serving system and application requirements.\n\n\nThis JavaScript code provides a more structured and practical implementation than the pseudocode, separating concerns into classes and functions. It also includes helpful comments to explain the logic. However, it still relies on placeholder functions that need to be implemented according to the specific LLM environment and application requirements.  The core logic for OIT and WSC is implemented based on the pseudocode, handling throttling, queueing, and service counter updates.",
  "simpleQuestion": "How can we fairly serve diverse LLMs?",
  "timestamp": "2024-11-26T06:03:55.148Z"
}