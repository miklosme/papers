{
  "arxivId": "2502.08047",
  "title": "WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation",
  "abstract": "Current Graphical User Interface (GUI) agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state—such as the target software not being open or the interface not being in its default state—often lead to planning errors. This issue is widespread in real application scenarios, but existing benchmarks fail to evaluate it. In this paper, we present WorldGUI, a novel GUI benchmark that designs GUI tasks with various initial states to simulate real computer-user interactions. The benchmark spans a wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose GUI-Thinker, a holistic framework, leveraging a critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that GUI-Thinker significantly outperforms Claude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This improvement underscores the effectiveness of our critical-thinking-based framework in enhancing GUI automation.",
  "summary": "This paper introduces WorldGUI, a new benchmark for testing GUI automation agents, specifically focusing on their ability to handle dynamic, real-world scenarios with varying initial states. It also proposes GUI-Thinker, a novel agent framework incorporating critical thinking principles through modules like Planner-Critic, Step-Check, and Actor-Critic.  Key to LLM-based multi-agent systems are: dynamic task adaptation based on initial states and environment feedback, the use of instructional videos alongside text queries to provide richer context for complex tasks, and the integration of critical thinking modules for improved action planning, validation, and correction.  WorldGUI allows for robust evaluation of these capabilities, showcasing the potential for LLMs to handle complex, realistic GUI automation tasks.",
  "takeaways": "This paper introduces WorldGUI, a benchmark for testing GUI automation agents, and GUI-Thinker, a framework emphasizing \"critical thinking\" for building such agents.  Let's explore how a JavaScript developer can translate these concepts into practical LLM-based multi-agent web applications:\n\n**1. Dynamic State Management and Testing:**\n\n* **Scenario:** Imagine building a multi-agent web app for collaborative document editing, similar to Google Docs. Each agent (representing a user) interacts with the shared document's DOM.\n* **WorldGUI Inspiration:**  Instead of testing agents against static HTML, create a testing framework that dynamically modifies the initial DOM state.  This mirrors WorldGUI's dynamic initial states.  For example:\n    ```javascript\n    // Using a library like jsdom for DOM manipulation in Node.js\n    const { JSDOM } = require(\"jsdom\");\n    const dom = new JSDOM(`<!DOCTYPE html><div id=\"content\"></div>`);\n\n    function generateRandomInitialState() {\n      const contentDiv = dom.window.document.getElementById(\"content\");\n      // Randomly add text, headings, lists, etc.\n      // ...\n    }\n\n    // Test your agents with different initial states\n    generateRandomInitialState();\n    agent1.act(dom.window.document);\n    // ... assertions ...\n    ```\n* **Benefit:** This approach ensures robustness by testing agent behavior under various unexpected conditions, leading to more reliable multi-agent interactions.\n\n**2. Implementing \"Critical Thinking\" Modules with LLMs:**\n\n* **GUI-Thinker Components in JavaScript:**  The paper's \"Planner-Critic,\" \"Step-Check,\" and \"Actor-Critic\" modules can be implemented using LLMs within a JavaScript agent.\n* **Scenario:** A multi-agent shopping cart application. Agents need to manage items, update totals, and handle promotions.\n* **Example:**\n    ```javascript\n    class ShoppingCartAgent {\n      constructor(llm) {\n        this.llm = llm; //  LLM instance (e.g., from LangChain)\n      }\n\n      async act(cartState) {\n        // Planner: Generate a plan (e.g., \"add item,\" \"apply discount\")\n        const initialPlan = await this.llm.call(\"Generate plan:\", { cartState }); \n\n        // Planner-Critic:  Critique the plan (check for logic errors, etc.)\n        const critiquedPlan = await this.llm.call(\"Critique plan:\", { initialPlan, cartState });\n\n        // Step-Check: Verify if a step is still necessary based on current state\n        for (const step of critiquedPlan) {\n          const isValidStep = await this.llm.call(\"Is step valid:\", { step, cartState });\n          if (!isValidStep) continue; // Skip step if unnecessary\n\n          // Actor: Translate the plan step into a concrete action on the cart\n          const action = await this.llm.call(\"Translate to action:\", { step });\n          this.executeAction(action, cartState); // Perform action on cart DOM\n\n          // Actor-Critic: Evaluate the action's outcome, potentially retry or correct\n          const isSuccess = this.checkActionResult(cartState);\n          if (!isSuccess) {\n            const correctedAction = await this.llm.call(\"Correct action:\", { action, cartState });\n            // ... retry with corrected action ...\n          }\n        }\n      }\n      // ... executeAction and checkActionResult methods ...\n    }\n    ```\n* **Libraries:** Use LangChain or similar libraries to interact with LLMs and manage prompts efficiently.\n\n**3. Visual Feedback and Observation Space:**\n\n* **Challenge:**  Web apps are visually rich.  Agents need to process visual information, similar to how GUI-Thinker uses screenshots.\n* **Solution:**  Use a library like Puppeteer to capture screenshots of the web page's relevant areas (the \"observation space\"). Encode these screenshots (e.g., using ResNet) to create embeddings that can be provided as context to the LLM along with the DOM state.\n* **Framework:** Combine Puppeteer with a frontend framework like React to manage agent-UI interaction and visual feedback.\n\n**4. Multi-Agent Coordination:**\n\n* **Libraries:** Explore libraries like PeerJS or Socket.IO to implement real-time communication and coordination between your JavaScript agents in a browser environment.\n\n\nBy adapting these examples and leveraging appropriate JavaScript libraries, developers can effectively translate the core research concepts of the paper into real-world web applications, leading to more robust, adaptive, and intelligent LLM-based multi-agent systems. Remember to focus on iterative design and testing, drawing inspiration from the dynamic evaluation strategies employed in WorldGUI.",
  "pseudocode": "The provided research paper includes Algorithm 1, which describes the reasoning loop of the GUI-Thinker agent. Here's the JavaScript equivalent:\n\n```javascript\nasync function guiThinkerReasoningLoop(state, actionCode, screenshot, metadata, currentSubtask, criticCount, maxTrials, maxCritiqueTrials, env, planner, plannerCritic, stepCheck, actor, actorCritic) {\n  // Generate task plan with Planner and Planner-Critic\n  let plan = await planner.generatePlan(state, screenshot, metadata);\n  plan = await plannerCritic.critiquePlan(plan);\n\n  let t = 0;\n  let st = \"Continue\"; // Initial state\n  let St = plan[0];   // Initial subtask\n\n  while (St !== \"end\" && t < maxTrials) {\n\n    // Observe environment\n    let mt = await env.getMetadata();\n    let Vt = await env.getScreenshot();\n\n    // Step-Check\n    st = await stepCheck.check(St, Vt, mt);\n\n    if (st === \"Next\") {\n      St = nextSubtask(St, plan); // Helper function to get next subtask\n      continue; // Skip to the next iteration\n    }\n\n    // Potentially modify subtask (details omitted for brevity, would involve LLM interaction)\n    // ...\n\n    // Actor\n    let Ct = await actor.generateActionCode(St, mt, Vt);\n\n    // Execute action\n    await env.executeAction(Ct);\n\n    // Observe environment again\n    mt = await env.getMetadata();\n    Vt = await env.getScreenshot();\n\n    Ct = null;\n    t++;\n    st = \"Critic\"; // Set state for Actor-Critic\n\n    // Actor-Critic Loop\n    let z = 0;\n    while (st === \"Critic\" && z < maxCritiqueTrials) {\n      let actorCriticResult = await actorCritic.evaluateAndCorrect(St, Vt, mt, Ct);\n      st = actorCriticResult.state;\n      Ct = actorCriticResult.correctedActionCode;\n\n      if (st === \"Next\") {\n        St = nextSubtask(St, plan);\n        break; // Exit inner loop\n      }\n\n      if (Ct) {\n        await env.executeAction(Ct);\n        mt = await env.getMetadata();\n        Vt = await env.getScreenshot();\n        Ct = null;\n      }\n      z++;\n    }\n\n    if(st !== \"Next\") { // Only advance to next task if not already done by Actor-Critic\n        St = nextSubtask(St, plan);\n    }\n    t++;\n  }\n\n\n  // Helper function (Implementation depends on how the plan is structured)\n  function nextSubtask(currentSubtask, plan){\n      //Logic to determine next subtask based on currentSubtask and the plan\n      // This is a placeholder, replace with appropriate logic\n      let currentSubtaskIndex = plan.indexOf(currentSubtask);\n      if(currentSubtaskIndex < plan.length -1){\n          return plan[currentSubtaskIndex +1]\n      } else {\n          return \"end\"\n      }\n  }\n}\n```\n\n**Explanation:**\n\nThis algorithm represents the core logic of the GUI-Thinker agent, emphasizing a \"think-then-act\" approach. Its purpose is to automate GUI interactions based on a given user query and instructional video. It proceeds as follows:\n\n1. **Planning:**  The `planner` creates a high-level plan, which is then critiqued and refined by `plannerCritic`.\n\n2. **Step-Check:** Before executing each step (subtask), `stepCheck` verifies if the step is necessary or needs modification based on the current GUI state. This is crucial for handling dynamic GUI environments.\n\n3. **Action Execution:** The `actor` translates the subtask into executable code and executes it within the GUI environment.\n\n4. **Action Evaluation and Correction:** The `actorCritic` evaluates the action's success. If the action fails, `actorCritic` attempts to correct it iteratively.\n\n5. **Iteration:** The loop continues until the task is completed (or a maximum number of trials is reached).\n\nThis iterative process, coupled with the critical thinking components (`plannerCritic`, `stepCheck`, `actorCritic`), allows the agent to be more robust and adaptive to dynamic changes in the GUI, improving the overall success rate of GUI automation.  The provided JavaScript code uses async/await for cleaner handling of asynchronous operations, which are common when interacting with a GUI and LLMs. It also assumes the existence of helper classes/functions like `planner`, `actor`, `env`, etc., which would contain the specific implementations for planning, action execution, environment interaction, and so on.  The `nextSubtask` function is a placeholder and needs to be implemented based on how the plan is structured.  It's crucial to replace the placeholder comments with actual implementations for real-world use cases.",
  "simpleQuestion": "How can I make my LLM GUI agent robust to varying initial states?",
  "timestamp": "2025-02-13T06:03:07.193Z"
}