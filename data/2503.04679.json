{
  "arxivId": "2503.04679",
  "title": "Multi-Agent Inverse Q-Learning from Demonstrations",
  "abstract": "When reward functions are hand-designed, deep reinforcement learning algorithms often suffer from reward misspecification, causing them to learn suboptimal policies in terms of the intended task objectives. In the single-agent case, inverse reinforcement learning (IRL) techniques attempt to address this issue by inferring the reward function from expert demonstrations. However, in multi-agent problems, misalignment between the learned and true objectives is exacerbated due to increased environment non-stationarity and variance that scales with multiple agents. As such, in multi-agent general-sum games, multi-agent IRL algorithms have difficulty balancing cooperative and competitive objectives. To address these issues, we propose Multi-Agent Marginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficient framework for multi-agent IRL. For each agent, MAMQL learns a critic marginalized over the other agents' policies, allowing for a well-motivated use of Boltzmann policies in the multi-agent context. We identify a connection between optimal marginalized critics and single-agent soft-Q IRL, allowing us to apply a direct, simple optimization criterion from the single-agent domain. Across our experiments on three different simulated domains, MAMQL significantly outperforms previous multi-agent methods in average reward, sample efficiency, and reward recovery by often more than 2-5x. We make our code available at https://sites.google.com/view/mamql.",
  "summary": "This paper introduces MAMQL (Multi-Agent Marginal Q-Learning from Demonstrations), a new algorithm for training AI agents in scenarios with multiple agents where cooperation and competition are both present.  It addresses the problem of figuring out what motivates each agent based only on examples of expert behavior, especially when those motivations are complex and intertwined.  Instead of training agents to directly copy expert actions, MAMQL attempts to learn the underlying *reward functions* that drive expert decision-making.  It does this by considering how each agent's actions affect the others and by learning a simplified \"marginalized\" view of the environment for each agent.\n\nKey points relevant to LLM-based multi-agent systems include the ability to learn from expert demonstrations, the focus on mixed cooperative/competitive scenarios, and the potential for scaling to more complex environments with multiple agents. The focus on reward function recovery could enable LLMs to infer complex objectives and conventions from demonstration data, which could be valuable in developing more robust and adaptable multi-agent systems. The marginalization technique also offers a potential avenue for managing the computational complexity of coordinating multiple LLM agents.",
  "takeaways": "This research paper presents Multi-Agent Marginal Q-Learning from Demonstrations (MAMQL), a novel approach to training multi-agent AI systems by learning from expert demonstrations. Let's explore how a JavaScript developer can apply these insights to LLM-based multi-agent projects in web development:\n\n**1. Collaborative Content Creation:**\n\n* **Scenario:** Imagine building a web application where multiple LLM-powered agents collaborate to write a story, script, or marketing copy.\n* **MAMQL Application:**  Train agents with demonstrations from expert writers showcasing desirable collaboration patterns (e.g., maintaining consistent tone, building on each other's ideas, avoiding redundancy).  MAMQL can learn the underlying reward function that drives this effective collaboration, even if the objectives are implicit.\n* **JavaScript Implementation:** Use a platform like LangChain or LlamaIndex to manage the LLMs and design the agent interaction logic.  Structure the application using a frontend framework like React or Vue.js to display the collaborative output in real-time.  Implement the agent interaction and reward logic using Node.js and a library like TensorFlow.js for potential client-side training or inference.\n\n**2. Multi-User Interactive Narratives:**\n\n* **Scenario:**  Develop a text-based adventure game where multiple human players interact with LLM-powered characters in a shared environment.\n* **MAMQL Application:** Train the LLM agents with demonstrations that balance cooperation with NPCs (helping players achieve quests) and competition (creating challenging obstacles).  MAMQL allows defining distinct reward functions for each agent (player and NPCs), capturing the nuances of a general-sum game.\n* **JavaScript Implementation:** Use a Node.js server with Socket.IO for real-time communication between players and the game environment.  Implement the LLM agents using LangChain or a similar library.  Leverage a game engine like Phaser or PixiJS for visualizing the game state, even in a text-based setting.\n\n**3. Decentralized Autonomous Organizations (DAOs):**\n\n* **Scenario:** Build a web application for a DAO where LLM-powered agents represent members and vote on proposals.\n* **MAMQL Application:**  Train agents with demonstrations of effective DAO governance (e.g., considering diverse perspectives, reaching consensus, avoiding deadlock). MAMQL can infer the implicit reward functions that incentivize productive DAO behavior.\n* **JavaScript Implementation:** Utilize a blockchain library like Web3.js to interact with a smart contract that governs the DAO. Implement the LLM agents using LangChain or a similar framework.  A frontend library like React could provide a user interface for members to interact with the DAO and observe agent decisions.\n\n\n**Key JavaScript Considerations:**\n\n* **Asynchronous Communication:**  Multi-agent systems require handling asynchronous actions and communication. Use JavaScript's Promises, async/await, or libraries like RxJS to manage these asynchronous workflows effectively.\n* **State Management:**  Maintain a shared state representation for the multi-agent environment.  Consider using Redux, MobX, or other state management solutions to handle updates and synchronization across agents.\n* **Agent Architecture:**  Modularize the agent implementation using JavaScript classes or functions to promote code reusability and maintainability.\n* **Visualization:**  Use JavaScript charting libraries (e.g., Chart.js, D3.js) or game engines to visualize agent behavior and interactions for debugging and understanding emergent behavior.\n\n\nBy understanding the principles of MAMQL and applying them thoughtfully with JavaScript and related web technologies, developers can create more sophisticated and robust multi-agent web applications powered by LLMs.  This research bridges the gap between theory and practice, opening exciting new possibilities for the future of web development.",
  "pseudocode": "```javascript\n// MAMQL Algorithm in JavaScript\n\nasync function mamql(environment, expertTransitions, regularizationFn, gamma, alpha) {\n  // 1. Input:\n  const { n, S, A, T, p0 } = environment; // Environment parameters\n  const pE = expertTransitions;          // Expert demonstrations\n  const pR = [];                     // Agent rollout transitions buffer\n  const N = expertTransitions.length;\n\n\n  // 2. Initialize:\n  const V = Array(n).fill(null).map(() => ({}));   // Marginal critics Qψi, initialized as empty objects (acting as dictionaries) for each agent\n  const R_mu = Array(n).fill(null).map(() => ({})); // Reward function Rµi for each agent\n  const psi = Array(n).fill(null).map(() => ({}));\n\n  // Helper function to get Q value (defaults to 0 if not found)\n  const getQ = (agent, state, action) => psi[agent][state]?.[action] || 0;\n  \n\n  // Helper function to get Boltzmann policy for an agent in a state\n  const boltzmannPolicy = (agentIndex, state) => {\n    let actionValues = {}; \n    for (const action of A) {\n      actionValues[action] = Math.exp(getQ(agentIndex, state, action));\n    }\n    let normalizationFactor = Object.values(actionValues).reduce((sum, val) => sum + val, 0);\n    let policy = {};\n    for (const action in actionValues) {\n      policy[action] = actionValues[action] / normalizationFactor;\n    }\n\n    return policy;\n  };\n\n  // Helper function to sample from Boltzmann policy\n  const sampleAction = (policy) => {\n    const actions = Object.keys(policy);\n    const probabilities = Object.values(policy);\n    const randomValue = Math.random();\n    let cumulativeProbability = 0;\n    for (let i = 0; i < actions.length; i++) {\n      cumulativeProbability += probabilities[i];\n      if (randomValue <= cumulativeProbability) {\n        return actions[i];\n      }\n    }\n\n    return actions[actions.length-1]; // fallback. Should theoretically never be reached.\n  };\n\n  // 3. Repeat until convergence (Simplified convergence check. In reality, monitor performance metrics)\n  for (let iteration = 0; iteration < 1000; iteration++) { //Example max iterations\n    // 4. Rollout trajectory using current policy\n    let trajectory = generateTrajectory(n, S, A, T, p0, boltzmannPolicy, sampleAction); //Replace with environment interaction\n    pR.push(trajectory);\n\n    //5 - 9. Update critics and rewards for each agent\n    for (let i = 0; i < n; i++) {\n      const xE = pE[Math.floor(Math.random() * pE.length)]; // Sample expert transition\n      const xR = trajectory[Math.floor(Math.random() * trajectory.length)]; // Sample rollout transition\n\n\n      // One-sample estimate of expected value:\n      const nextStateValue = V[i][xR.nextState] || 0;\n     \n      // Simplified gradient update based on Eq. (5,6). In a real implementation, use an automatic differentiation library.\n      const psiUpdate = alpha * (nextStateValue - regularizationFn(getQ(i, xE.state, xE.action)));\n\n      // Update Psi and V (Marginal critic) - This is simplified. Use a suitable data structure to store Q-values.\n      psi[i][xE.state] = psi[i][xE.state] || {};\n      psi[i][xE.state][xE.action] = getQ(i, xE.state, xE.action) + psiUpdate;\n\n      V[i][xR.state] = V[i][xR.state] || 0;\n      V[i][xR.state] += psiUpdate/ (Object.keys(V[i]).length || 1); // Averaging the values\n\n      //Simplified reward update.  In a real application consider storing the rewards R_mu[i][state][action] and averaging across the buffer pR.\n       R_mu[i][xR.state] = R_mu[i][xR.state] || 0;\n       R_mu[i][xR.state] +=  getQ(i, xR.state, xR.action);\n    }\n  }\n\n  // 10. Output\n  return { policies: psi.map((agentPsi)=> boltzmannPolicy), rewardFunctions: R_mu };\n}\n\n\n// Placeholder for trajectory generation - replace with actual environment interaction\nfunction generateTrajectory(n, S, A, T, p0, policyFn, sampleActionFn){\n\n let trajectory =[];\n\n let currentState = p0; //Starting state - oversimplified. Sample from p0 in a real application.\n\n for (let t=0; t < 20; t++){ //Simplified fixed horizon. Adapt to the MDP.\n     let jointAction = [];\n     for (let i =0; i< n; i++){\n      const agentPolicy = policyFn(i, currentState);\n      jointAction.push(sampleActionFn(agentPolicy));\n      }\n\n     let nextState = T(currentState, jointAction); // This needs to be implemented based on your environment dynamics.\n     trajectory.push({state: currentState, action:jointAction, nextState });\n     currentState = nextState;\n }\n\n  return trajectory;\n\n}\n\n\n\n// Example usage (you'll need to define environment, expertTransitions, regularizationFn)\n// Note: \"environment\" needs S, A, T and p0.  \"expertTransitions\" should be array of {state, action, nextState}\n//  \"regularizationFn\" needs to be defined based on the reward regularization chosen.\n\nconst environment = { n:2, S: [0,1,2,3,4], A:[0,1,2], T: undefined, p0:0 };\nconst expertTransitions = [{state:0, action:[0,1], nextState:1}];\nconst regularizationFn = (x) => x*x;  // Example squared regularization\nconst gamma = 0.9;     // Discount factor\nconst alpha = 0.1;      // Learning rate\n\nasync function runMAMQL(){\nconst result = await mamql(environment, expertTransitions, regularizationFn, gamma, alpha);\n\nconsole.log(\"Learned Policies (psi - Q-values):\", result.policies); // These are functions that return boltzmann policies given a state.\nconsole.log(\"Learned Reward Functions:\", result.rewardFunctions);\n}\n\nrunMAMQL();\n\n\n\n\n```\n\n**Explanation and Purpose of MAMQL:**\n\nMulti-Agent Marginal Q-Learning from Demonstrations (MAMQL) is an algorithm designed to solve the Inverse Reinforcement Learning (IRL) problem in multi-agent settings, specifically general-sum games where agents have a mix of cooperative and competitive objectives.  It aims to infer the reward functions of each agent from expert demonstrations.\n\nThe core idea is to learn *marginalized critics* for each agent. Instead of considering the full joint action-value function Q(s, a1, a2, ..., an), MAMQL learns Q(s, ai) for each agent *i*, marginalizing over the other agents' actions. This simplification makes it possible to use Boltzmann policies in a multi-agent context.\n\n**Key Steps and Concepts in the JavaScript Code:**\n\n1. **Initialization:**  Critics `V`, reward functions `R_mu`, and the policy parameters `psi` are initialized. Critics and rewards are implemented as Javascript Objects (Dictionaries) using state-action pairs as keys.\n\n2. **Boltzmann Policy and Sampling:** The `boltzmannPolicy` function calculates the Boltzmann policy given the learned Q-values (represented by `psi`), and `sampleAction` samples an action according to that policy.\n\n3. **Trajectory Rollout:** The `generateTrajectory` function simulates interactions with the environment using the current Boltzmann policies. This is a placeholder in the code, needing replacement with your actual environment interaction logic based on your `environment.T` (Transition function) and  `environment.p0` (initial state distribution).\n\n4. **Critic and Reward Updates:**\n    - An expert transition and a rollout transition are sampled.\n    - A simplified gradient update rule (Eq. 5, 6 in the paper) is used to update the critic `psi`.  This is a drastically simplified update; in a real implementation, use an automatic differentiation library.\n    - The `V` values (soft value function) is updated based on the averaged `psi` updates for that state.\n    - The reward function `R_mu` is updated using the constraint from Equation (7). This also simplified; in a real world application, consider using the sampled transitions and Equation (9).\n\n\n5. **Output:** The algorithm returns the learned policies (which are functions that calculate the Boltzmann policy given a state) and reward functions.\n\n**Purpose of the Algorithm:**\n\nBy learning marginalized critics and updating reward functions based on these critics, MAMQL aims to recover reward functions that explain the observed expert behavior, even in complex multi-agent scenarios with mixed cooperative/competitive objectives. The algorithm's structure promotes sample efficiency and robustness compared to more traditional multi-agent IRL approaches. This implementation focuses on the core ideas of the paper but simplifies a few aspects for clarity and leaves room for optimizations and the usage of appropriate libraries for a full-fledged implementation.",
  "simpleQuestion": "How can I efficiently learn multi-agent rewards from demos?",
  "timestamp": "2025-03-07T06:04:10.602Z"
}