{
  "arxivId": "2503.22779",
  "title": "Policy Optimization and Multi-agent Reinforcement Learning for Mean-variance Team Stochastic Games",
  "abstract": "We study a long-run mean-variance team stochastic game (MV-TSG), where each agent shares a common mean-variance objective for the system and takes actions independently to maximize it. This problem is motivated by scenarios with multiple decision makers collaboratively optimizing risk metrics, which widely exists in practice. MV-TSG has two main challenges. First, the variance metric is neither additive nor Markovian in a dynamic setting. Second, simultaneous policy updates of all agents lead to a non-stationary environment for each individual agent. Both challenges make dynamic programming inapplicable. In this paper, we study MV-TSGs from the perspective of sensitivity-based optimization. The performance difference formula and performance derivative formula for joint policies are derived, which provide optimization information for MV-TSGs. We prove the existence of a deterministic Nash policy for this problem. Subsequently, we propose a Mean-Variance Multi-Agent Policy Iteration (MV-MAPI) algorithm with a sequential update scheme, where individual agent policies are updated one by one in a given order. This sequential update scheme effectively addresses the environmental non-stationarity issue. We prove that the MV-MAPI algorithm converges to a first-order stationary point of the objective function. By analyzing the local geometry of stationary points, we derive specific conditions for stationary points to be (local) Nash equilibria, and further, strict local optima. To solve large-scale MV-TSGs in scenarios with unknown environmental parameters, we extend the idea of trust region methods to MV-MAPI and develop a multi-agent reinforcement learning algorithm named Mean-Variance Multi-Agent Trust Region Policy Optimization (MV-MATRPO). We derive a performance lower bound for each update of joint policies, which guarantees the monotone improvement of the MV-MATRPO algorithm. Finally, numerical experiments on energy management in multiple microgrid systems are conducted, which demonstrate the effectiveness of our algorithms. To our knowledge, this paper is the first to propose theoretically provable algorithms for solving MV-TSGs.",
  "summary": "This paper proposes novel algorithms for optimizing long-run mean-variance in team stochastic games (TSGs), a cooperative multi-agent setting where agents share a common reward. Traditional dynamic programming approaches fail due to the non-additive and non-Markovian nature of variance. The key innovation is applying sensitivity-based optimization to derive performance difference and derivative formulas. This enables the creation of a policy iteration algorithm (MV-MAPI) with a sequential update scheme, guaranteeing convergence to a first-order stationary point. Further, a multi-agent reinforcement learning algorithm (MV-MATRPO) using trust-region optimization is proposed for model-free scenarios, allowing for application in complex environments.  Relevant to LLM-based systems, these algorithms address challenges in cooperative multi-agent learning involving long-term performance and volatility, sequential decision-making, and scalability to larger state spaces through function approximation with neural networks. The sequential update within a centralized training framework offers a practical implementation approach for complex multi-agent scenarios where direct policy iteration is intractable.",
  "takeaways": "This paper's core contribution for a JavaScript developer working with LLM-based multi-agent systems is the introduction of the MV-MAPI and MV-MATRPO algorithms, offering a structured way to optimize the long-term behavior of interacting agents while considering both average reward and its variance (risk). Here's how you can apply these insights practically:\n\n**Scenario 1: Collaborative Content Creation**\n\nImagine building a web app where multiple LLMs collaborate to write a story. Each LLM is an agent, and their actions involve contributing sentences or paragraphs. The reward could be based on story coherence, creativity, and engagement (measured by user feedback or pre-trained metrics).  Variance represents the risk of the story deviating from a desired narrative arc or tone.\n\n* **MV-MAPI in JavaScript:** You could implement a simplified version of MV-MAPI using Node.js and a message queue (e.g., Redis, RabbitMQ) for agent communication.  Each LLM agent would maintain its own policy (represented by a JavaScript object or function mapping states to actions). The sequential update loop could be orchestrated by a central Node.js process. Each LLM would generate text, receive feedback (reward), and update its policy using the performance difference formula (Lemma 2). LangChain could be used to manage the LLMs and chain their outputs.\n\n* **Frontend Implementation:** The frontend, built with React, Vue, or similar, could display the evolving story and collect user feedback. The feedback would be sent to the backend to influence the reward calculation.\n\n**Scenario 2: Multi-Agent Chatbot System**\n\nConsider building a customer service system with multiple specialized chatbot agents. Each agent has its own area of expertise (e.g., order status, technical support, returns). The goal is to route customer queries to the most appropriate agent while minimizing wait time and maximizing customer satisfaction. Variance represents the risk of assigning a query to the wrong agent, leading to delays or inaccurate information.\n\n* **MV-MATRPO with TensorFlow.js:** Here, MV-MATRPO's data-driven approach shines. You could use TensorFlow.js to implement the policy networks for each agent. The training data could come from logged chat interactions. The agents' policies could be updated in a centralized training loop (in Node.js or even on the client-side with TensorFlow.js) using the trust region optimization approach.\n\n* **Agent Communication:**  Socket.IO could facilitate real-time communication between agents to exchange information about the current state (customer query, agent availability) and coordinate actions.\n\n**Scenario 3: Decentralized Game AI**\n\nImagine developing a browser-based multi-player game where each player is controlled by an LLM agent. MV-MATRPO can train agents to compete or cooperate effectively.\n\n* **MV-MATRPO with Web Workers:** The training process could be offloaded to Web Workers to avoid blocking the main thread. Each worker could train a separate agent, and the policies could be synchronized periodically using SharedArrayBuffer or a similar mechanism.\n\n\n**JavaScript Libraries and Frameworks:**\n\n* **TensorFlow.js:** For implementing neural network-based policies.\n* **LangChain:** For managing LLM interactions and prompting strategies.\n* **Socket.IO or WebRTC:** For real-time communication between agents.\n* **React, Vue, or Svelte:** For building interactive frontends.\n* **Node.js with message queues (Redis, RabbitMQ):** For backend coordination.\n\n**Key Considerations:**\n\n* **State Representation:** Carefully design the state representation to capture relevant information for decision-making.\n* **Reward Function:** Define a clear reward function that reflects the desired behavior of the agents.\n* **Variance Parameter (β):** Tune β to balance the trade-off between reward and risk.\n* **Sequential Update Scheme:** Implement the sequential update scheme (or a randomized version) for training.\n* **Trust Region Hyperparameters:**  Carefully choose the trust region hyperparameters to ensure stable and efficient learning.\n\n\nBy combining the theoretical insights of MV-MAPI/MV-MATRPO with the practical tools available in the JavaScript ecosystem, developers can build sophisticated and robust LLM-based multi-agent applications for the web.  The paper's emphasis on long-term optimization and risk consideration is particularly relevant for web development, where user engagement and system stability are paramount.",
  "pseudocode": "```javascript\n// Algorithm 1: Mean-variance multi-agent policy iteration\n\nfunction mvMAPI(environment) {\n  let numAgents = environment.getNumAgents();\n  let initialPolicies = [];\n  for (let i = 0; i < numAgents; i++) {\n    initialPolicies.push(environment.createRandomDeterministicPolicy()); // Initialize random deterministic policy\n  }\n  let jointPolicy = initialPolicies;\n\n  for (let k = 0; ; k++) { // Outer loop, potentially infinite, will break upon convergence\n    let previousJointPolicy = jointPolicy;\n    let permutation = shuffleArray([...Array(numAgents).keys()]); // Create a random permutation of agent indices\n\n    for (let h = 0; h < numAgents; h++) {\n      let agentIndex = permutation[h];\n\n      // Policy evaluation \n      let { meanReward, variance, meanVarianceValue, valueFunction, actionValueFunction, advantageFunction } = \n        environment.evaluatePolicy(jointPolicy);\n\n      // Policy improvement for current agent\n      let newPolicy = environment.createDeterministicPolicy();\n\n      for (let state of environment.getStates()) {\n        let bestAction = null;\n        let bestAdvantage = -Infinity;\n\n        for (let action of environment.getActions(agentIndex)) {\n          let otherAgentsActions = [];\n          for (let i = 0; i < numAgents; i++) {\n            if (i !== agentIndex) {\n              otherAgentsActions.push(jointPolicy[i].getAction(state)); \n            }\n          }\n          let currentAdvantage = advantageFunction(state, action, otherAgentsActions);\n          if (currentAdvantage > bestAdvantage) {\n            bestAdvantage = currentAdvantage;\n            bestAction = action;\n          }\n        }\n        newPolicy.setAction(state, bestAction);\n      }\n\n      jointPolicy[agentIndex] = newPolicy; // Update the joint policy\n    }\n\n\n    if (policiesEqual(previousJointPolicy, jointPolicy)) { // Check if policies converge\n      break;\n    }\n  }\n\n  return jointPolicy;\n}\n\n\n\nfunction policiesEqual(policy1, policy2) {\n    if(policy1.length !== policy2.length) return false;\n    for(let i=0; i<policy1.length; i++) {\n        if(!policy1[i].equals(policy2[i])) { // Assuming 'equals' method is defined for individual policies\n            return false;\n        }\n    }\n    return true;\n}\n\nfunction shuffleArray(array) {\n    for (let i = array.length - 1; i > 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [array[i], array[j]] = [array[j], array[i]];\n    }\n    return array;\n}\n\n\n// Algorithm 2: Modified mean-variance multi-agent policy iteration\n\nfunction mvMAPIModified(environment) {\n    let jointPolicy = mvMAPI(environment); // get an initial policy from Algorithm 1\n\n    for(let agentIndex=0; agentIndex < environment.getNumAgents(); agentIndex++) {\n        let alternatePolicies = [];\n\n        for (let state of environment.getStates()) {\n            let bestAlternateAction = null;\n            let bestAlternateAdvantage = -Infinity;\n\n            for (let action of environment.getActions(agentIndex)) {\n                if(action !== jointPolicy[agentIndex].getAction(state)) { // Explore actions different from the current policy\n                    let otherAgentsActions = [];\n                    for (let i = 0; i < environment.getNumAgents(); i++) {\n                        if (i !== agentIndex) {\n                            otherAgentsActions.push(jointPolicy[i].getAction(state)); \n                        }\n                    }\n                    let advantage = environment.advantageFunction(state, action, otherAgentsActions);\n                    if(advantage > bestAlternateAdvantage) {\n                        bestAlternateAdvantage = advantage;\n                        bestAlternateAction = action;\n                    }\n                }\n            }\n\n            if(bestAlternateAction !== null) { // Store the best alternate actions\n                alternatePolicies.push({state, action: bestAlternateAction});\n            }\n        }\n\n\n\n        for(let altPolicy of alternatePolicies) {\n\n            let tempPolicy = [...jointPolicy];\n            tempPolicy[agentIndex].setAction(altPolicy.state, altPolicy.action) \n\n            let newMeanReward = environment.evaluatePolicy(tempPolicy).meanReward;\n            let oldMeanReward = environment.evaluatePolicy(jointPolicy).meanReward;\n\n\n             if(newMeanReward !== oldMeanReward) { // Apply Corollary 1\n                 jointPolicy = mvMAPI(environment, tempPolicy); // Restart MV-MAPI with improved policy\n                 break; // Move to next agent after improvement\n             }\n\n        }\n    }\n    return jointPolicy;\n}\n\n\n// Algorithm 3: Mean-Variance Multi-Agent Trust Region Policy Optimization (MV-MATRPO)\n\n// ... (This algorithm is too complex for a concise JavaScript translation given current limitations\n//      and requires a deep learning library and substantial infrastructure. See explanation below.)\n\n```\n\n**Algorithm 1 (MV-MAPI):** This algorithm aims to find a near-optimal joint policy for a multi-agent mean-variance team stochastic game (MV-TSG). It uses policy iteration with a sequential update scheme.  Agents iteratively update their policies one by one in a random order, aiming to improve their individual advantage functions while keeping other agents' policies fixed. The algorithm converges to a first-order stationary point of the objective function.\n\n**Algorithm 2 (Modified MV-MAPI):** This algorithm builds upon Algorithm 1 to prevent getting stuck in certain types of first-order stationary points (which might not be local optima). After Algorithm 1 converges, Algorithm 2 explores alternative actions for each agent and checks if these lead to a change in the mean reward (using Corollary 1 from the paper). If a change occurs, it restarts Algorithm 1 with the improved joint policy. This helps the algorithm escape saddle points and converge to a strict local optimum.\n\n**Algorithm 3 (MV-MATRPO):**  This algorithm is a model-free reinforcement learning algorithm designed for large-scale MV-TSGs where the environment's transition probabilities and reward functions are unknown.  It uses a trust region optimization approach within a centralized training, decentralized execution framework. The algorithm parametrizes the policies and value functions using neural networks. Agents collect trajectories of experience by interacting with the environment, and these are used to estimate advantage functions and update policies within a trust region, ensuring monotonic performance improvement.  Due to the complexity of deep learning implementation, a direct, functional JavaScript translation is beyond the scope of this response.  It would require using a library like TensorFlow.js or Brain.js and significantly more code to define the neural network architectures, training loops, experience buffers, and other necessary components.  It's important to understand that the core idea revolves around using trust regions to limit policy updates, estimated advantages to guide updates, and centralized training to improve sample efficiency.",
  "simpleQuestion": "How to optimize multi-agent RL for mean-variance objectives?",
  "timestamp": "2025-04-01T05:11:57.077Z"
}