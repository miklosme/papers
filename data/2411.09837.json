{
  "arxivId": "2411.09837",
  "title": "Real-time Adapting Routing (RAR): Improving Efficiency Through Continuous Learning in Software Powered by Layered Foundation Models",
  "abstract": "Abstract-To balance the quality and inference cost of a Foundation Model (FM, such as large language models (LLMs)) powered software, people often opt to train a routing model that routes requests to FMs with different sizes and capabilities. Existing routing models rely on learning the optimal routing decision from carefully curated data, require complex computations to be updated, and do not consider the potential evolution of weaker FMs. In this paper, we propose Real-time Adaptive Routing (RAR), an approach to continuously adapt FM routing decisions while using guided in-context learning to enhance the capabilities of weaker FM. The goal is to reduce reliance on stronger, more expensive FMs. We evaluate our approach on different subsets of the popular MMLU benchmark. Over time, our approach routes 50.2% fewer requests to computationally expensive models while maintaining around 90.5% of the general response quality. In addition, the guides generated from stronger models have shown intra-domain generalization and led to a better quality of responses compared to an equivalent approach with a standalone weaker FM.",
  "summary": "This paper introduces Real-time Adapting Routing (RAR), a method for optimizing the use of multiple large language models (LLMs) of varying sizes and computational costs.  RAR aims to reduce reliance on larger, more expensive LLMs by continually training a smaller LLM with \"guides\" (reasoning steps) generated by the larger LLM.  This allows the smaller LLM to handle increasingly complex tasks over time, improving efficiency without significantly sacrificing performance.  Key points for LLM-based multi-agent systems include the use of a layered architecture, continuous learning through in-context learning and guides, dynamic routing decisions based on real-time performance, and the potential for intra- and inter-domain generalization of learned knowledge.",
  "takeaways": "This paper's Real-time Adapting Routing (RAR) offers valuable insights for JavaScript developers building LLM-based multi-agent web apps. Here are some practical examples illustrating how to apply these insights:\n\n**Scenario 1: Multi-Agent Customer Support Chatbot**\n\nImagine building a customer support chatbot with multiple LLMs: a smaller, faster on-device model (`weaker FM` - e.g., a quantized Llama 2 model running with WebGPU) handles simple queries, while a larger, more powerful cloud-based model (`stronger FM` - e.g., GPT-4) tackles complex issues.\n\n* **RAR Implementation:** Use a JavaScript library like `LangChain.js` or build your own system.  Initially, route all requests to the on-device model. If the confidence score (calculated using a similarity metric with previous successful answers or an LLM-as-a-judge approach) is below a threshold, trigger shadow inference.  Send the query to the cloud LLM. If it provides a significantly better answer (again judged via similarity metrics or LLM-as-a-judge), store the cloud LLM's response as a \"guide\" along with the original query's embedding in a client-side vector database (e.g., `localForage` combined with a JS embedding library). Future similar queries leverage this guide as in-context learning for the on-device model.\n\n* **Code Example (Conceptual):**\n\n```javascript\nasync function handleQuery(userQuery) {\n  const embedding = await generateEmbedding(userQuery);\n  const similarQueries = await vectorDB.query(embedding);\n\n  if (similarQueries.length > 0 && similarQueries[0].guide) {\n    // Use guide for on-device LLM\n    const onDeviceResponse = await onDeviceLLM.call(userQuery, similarQueries[0].guide);\n    if (isResponseAligned(onDeviceResponse, similarQueries[0].strongerFMResponse)) {\n       return onDeviceResponse;\n    }\n  } else {\n      // Try on-device LLM first\n      const onDeviceResponse = await onDeviceLLM.call(userQuery);\n\n      // Shadow inference\n      const strongerFMResponse = await strongerLLM.call(userQuery);\n\n       if (isResponseAligned(onDeviceResponse, strongerFMResponse)) {\n         // save to skill memory\n          await vectorDB.add({ embedding, strongerFMResponse: strongerFMResponse });\n         return onDeviceResponse;\n       } else { // guide generation\n            const guide = await strongerLLM.generateGuide(userQuery);\n\n            // weaker LLM inference with guide\n            const guidedResponse = await weakerLLM.call(userQuery, guide);\n\n            if(isResponseAligned(guidedResponse, strongerFMResponse)) {\n                 await vectorDB.add({ embedding, strongerFMResponse, guide });\n                 return guidedResponse;\n            }\n\n           return strongerFMResponse; // fallback\n       }\n  }\n}\n\n\n```\n\n**Scenario 2: Collaborative Multi-Agent Web Design**\n\nImagine multiple agents designing a website. One agent (`weaker FM`) specializes in basic HTML structure, while another (`stronger FM`) handles complex CSS styling and JavaScript interactions.\n\n* **RAR Implementation:**  The weaker agent generates initial HTML.  If user feedback suggests improvements to style or interactivity (e.g., \"Make the button animated\"), this triggers the stronger agent. The stronger agent's CSS/JS code becomes the \"guide\" for the weaker agent, enabling it to incorporate advanced features in future similar scenarios. Store these guides linked to design element embeddings.\n\n**Scenario 3: Personalized News Summarization App**\n\nA weaker on-device LLM provides basic summaries. User feedback (likes, dislikes, detailed reads) helps train the system. If a user consistently dislikes short summaries, the system engages a stronger cloud LLM to generate a more comprehensive summary, which then becomes the \"guide\" for future articles from the same source or topic.\n\n\n**Key Considerations for JavaScript Developers:**\n\n* **Client-Side Vector Databases:** Use IndexedDB wrappers like `localForage`, `Dexie.js`, or explore `JS embeddings` for client-side storage and efficient similarity search.\n* **LLM Integration:**  Use `LangChain.js`, or other JavaScript LLM libraries for easy integration with cloud-based and locally running LLMs.\n* **Performance:**  Quantization, WebGPU, and on-device LLMs become important for client-side performance.\n* **User Privacy:** On-device processing enhances privacy, but client-side storage requires careful management.\n\nBy adapting the principles of RAR, JavaScript developers can build sophisticated, cost-effective, and privacy-preserving multi-agent web applications that leverage the strengths of different LLMs. Remember that effective prompting for both LLMs and guide generation/usage is crucial for successful application.",
  "pseudocode": "No pseudocode block found. However, Figure 2 provides a flowchart visually depicting the RAR algorithm. This flowchart can be translated into JavaScript code. Here’s a simplified example of how the core logic of RAR could be implemented in JavaScript:\n\n```javascript\nasync function rar(userRequest, staticRouter, weakerFM, strongerFM, skillMemory, guideMemory) {\n  const routingDecision = await staticRouter.predict(userRequest);\n\n  if (routingDecision === 'weaker') {\n    return weakerFM.generate(userRequest); // Use weaker FM directly\n  } else { // routingDecision === 'stronger'\n    const strongerFMResponse = await strongerFM.generate(userRequest);\n    const weakerFMResponse = await weakerFM.generate(userRequest);\n\n    if (areResponsesSimilar(weakerFMResponse, strongerFMResponse)) {\n        skillMemory.add(userRequest); // Add to skill memory (Case 1)\n        return strongerFMResponse; // But return pre-calculated better response for user\n    } else {\n      let guide = await guideMemory.findSimilar(userRequest);\n\n      if (!guide) {\n        guide = await strongerFM.generateGuide(userRequest);\n      }\n\n      const guidedWeakerFMResponse = await weakerFM.generate(userRequest, guide);\n\n      if (areResponsesSimilar(guidedWeakerFMResponse, strongerFMResponse)) {\n        guideMemory.add(userRequest, guide); // Add to guide memory (Case 2)\n        return strongerFMResponse; // But return pre-calculated better response for user\n      } else {\n        skillMemory.add(userRequest, { requiresStronger: true }); // Add to skill memory with flag (Case 3)\n        return strongerFMResponse;\n      }\n    }\n  }\n}\n\n\n// Helper functions (placeholders – these would need proper implementations)\nasync function areResponsesSimilar(response1, response2) {\n    // Implement logic using a similarity measure, returning true if similar, false otherwise.\n    // ...\n}\n\n// Example usage\n// (Assuming staticRouter, weakerFM, strongerFM, skillMemory, guideMemory are initialized)\nconst response = await rar(\"User's request\", staticRouter, weakerFM, strongerFM, skillMemory, guideMemory);\nconsole.log(response); \n```\n\n**Explanation of the code and its purpose:**\n\nThis code represents a simplified version of the RAR algorithm in JavaScript. Its purpose is to dynamically route requests between a weaker and stronger LLM, optimizing for cost while maintaining quality.  Here's a breakdown:\n\n1. **`rar` function:** This is the main function implementing the RAR logic. It takes the user request, the static router, both LLMs, and the skill/guide memories as input.\n\n2. **Static Routing:** It first uses the `staticRouter` to get an initial routing decision.\n\n3. **Weaker FM Path:** If the decision is 'weaker', the `weakerFM` handles the request directly.\n\n4. **Stronger FM and Shadow Inference:** If 'stronger', it gets responses from both the `strongerFM` (for immediate return to the user) and `weakerFM` to compare.\n\n5. **Case 1: Standalone Weaker FM:** If the responses are similar, the request is added to `skillMemory`, and the `strongerFMResponse` is returned since it is the higher-quality answer.\n\n6. **Case 2: Weaker FM with Guide:** If responses are dissimilar, it tries to find a suitable guide in `guideMemory`. If not found, it generates a new guide using `strongerFM`. Then, it calls the `weakerFM` again with the guide. If *this* guided response is similar to the stronger one, the request and guide are stored in `guideMemory`, and `strongerFMResponse` is returned.\n\n7. **Case 3: Weaker FM Fails with Guide:** If the guided response is still not similar, the request is added to `skillMemory` with a flag to prioritize the `strongerFM` in the future, and `strongerFMResponse` is returned.\n\n\n**Key Concepts Illustrated:**\n\n* **Dynamic Routing:** The core of RAR is the dynamic routing between two LLMs.\n* **Shadow Inference:**  Comparing the weaker FM's output with the stronger FM's in the background.\n* **Guided Generation:**  Using the stronger FM's output as a \"guide\" to improve the weaker FM.\n* **Memory Management:**  The `skillMemory` and `guideMemory` track successful requests and guides for future reuse.\n\n\nThis implementation is simplified and lacks details like embedding generation, similarity thresholding, and robust memory management. However, it provides a starting point for JavaScript developers to experiment with the core concepts of RAR. More sophisticated components (vector databases, embedding models, and similarity measures) would need to be integrated into a real-world application.",
  "simpleQuestion": "How can I route LLM requests efficiently with continuous learning?",
  "timestamp": "2024-11-18T06:03:16.967Z"
}