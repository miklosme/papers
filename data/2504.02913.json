{
  "arxivId": "2504.02913",
  "title": "On Word-of-Mouth and Private-Prior Sequential Social Learning",
  "abstract": "Abstract-Social learning provides a fundamental framework in economics and social sciences for studying interactions among rational agents who observe each other's actions but lack direct access to individual beliefs. This paper investigates a specific social learning paradigm known as Word-of-Mouth (WoM), where a series of agents seeks to estimate the state of a dynamical system. The first agent receives noisy measurements of the state, while each subsequent agent relies solely on a degraded version of her predecessor's estimate. A defining feature of WoM is that the final agent's belief is publicly broadcast and adopted by all agents, in place of their own. We analyze this setting both theoretically and through numerical simulations, showing that some agents benefit from using the public belief broadcast by the last agent, while others suffer from performance deterioration.",
  "summary": "This paper analyzes two models of social learning, \"Private-Prior\" (PP) and \"Word-of-Mouth\" (WoM), where networked agents estimate a dynamic value. In PP, agents combine private beliefs with noisy observations of predecessors' actions. In WoM, the last agent's belief becomes the shared prior for all.  The study finds that WoM benefits later agents but harms earlier ones regarding estimation accuracy, a point relevant to hierarchical LLM agent systems where information flow resembles WoM. It highlights the risk of \"data incest\" or over-reliance on shared data in such systems, potentially degrading overall knowledge despite benefits for some agents. The analytical framework presented, although focusing on Kalman filter agents, offers a starting point for understanding information propagation and belief update dynamics within LLM-based multi-agent applications.",
  "takeaways": "This paper explores Word-of-Mouth (WoM) social learning, where agents in a sequence learn from each other's actions, with the last agent's belief becoming the \"public prior\" for the next sequence. While the paper uses Kalman filters for a Gaussian state estimation problem, the core concepts can be applied to LLM-based multi-agent systems in web development using JavaScript.\n\nHere's how a JavaScript developer can apply these insights:\n\n**1. Hierarchical Learning in Chatbots:**\n\n* **Scenario:** Imagine a multi-agent chatbot system for customer support.  A junior bot interacts with the customer, gathering initial information. This bot relays its understanding (its \"estimate\") to a more senior bot, which then interacts with the customer.  Finally, a \"manager\" bot (the last agent) oversees the interaction. Its refined understanding becomes the training data/prior for the next sequence of junior bots.\n* **Implementation:**\n    * **Agents:** Each bot could be an LLM-powered agent implemented using a JavaScript library like `LangChain` or a cloud-based LLM API.\n    * **WoM:** The \"manager\" bot's final output (e.g., a summary of the interaction, identified customer issue, solution provided) becomes the new context/prompt for the junior bots in the next sequence. This simulates WoM by propagating the final agent's understanding down the hierarchy.\n    * **Benefit:**  This allows junior bots to benefit from the accumulated knowledge of senior bots and the \"manager\" bot, leading to more accurate and efficient customer support.\n\n**2. Collaborative Content Creation:**\n\n* **Scenario:** Multiple LLM agents collaborate to write articles or generate creative content (e.g., stories, poems).  Each agent contributes a section, building on the previous agent's work. The final agent refines the entire piece and its output becomes input for the next round of collaboration.\n* **Implementation:**\n    * **Agents:** Each agent can be a specialized LLM, potentially fine-tuned for different writing styles or aspects of the content (e.g., generating ideas, writing introductions, refining grammar).\n    * **WoM:**  The final agent's refined output becomes the initial input for the next group of agents.\n    * **Benefit:** This iterative WoM process can lead to more creative and coherent content by leveraging the strengths of multiple specialized LLMs.\n\n**3. Multi-Agent Game Development:**\n\n* **Scenario:** Develop a browser-based multi-agent game where each agent is controlled by an LLM. Agents observe each other's moves and learn from their actions, refining their strategies over time.\n* **Implementation:**\n    * **Agents:** LLMs can decide each agent's actions in the game, using the game state and other agents' actions as input.\n    * **WoM:**  At the end of a game or a round, the most successful agent's strategy or learned parameters (e.g., weights in a neural network) can be shared with other agents, simulating WoM.  This allows faster learning and adaptation.\n    * **Frontend:**  Use JavaScript game development libraries like `Phaser` or `Babylon.js` for the game interface. Socket.io can facilitate real-time communication between agents.\n\n**4. Addressing Data Incest (Model Collapse):**\n\n* **Scenario:**  Mitigate the risks of model collapse when using LLM-generated content for training.  \n* **Implementation:**\n    * **Diversity:** Introduce diversity into the agent pool, ensuring different LLMs with different initializations and training data participate. This prevents the system from converging to a single, suboptimal solution.\n    * **Controlled WoM:**  Instead of directly using the final agent's output as the new prior, combine it with original training data or introduce random variations. This helps avoid overfitting to the LLM-generated data.\n    * **Monitoring:** Implement metrics to monitor the quality and diversity of the generated content to detect signs of model collapse.\n\n**Key JavaScript Tools:**\n\n* **LLM APIs/Libraries:** OpenAI API, `LangChain`, Hugging Face `transformers.js`.\n* **Game Development Libraries:** `Phaser`, `Babylon.js`.\n* **Real-time Communication:** Socket.io.\n\nBy understanding the principles of WoM social learning and adapting them to LLM-driven scenarios, JavaScript developers can build more sophisticated and efficient multi-agent applications for the web.  Experimenting with these concepts can lead to breakthroughs in collaborative content creation, improved chatbot performance, and innovative game development.",
  "pseudocode": "The paper does not contain explicit pseudocode blocks describing algorithms. However, the core algorithm discussed, Kalman filtering within a social learning context, is presented through mathematical formulas (equations 3, 4, 5, 10, 11, 12, and 13). These can be translated into JavaScript.  The paper presents two variants of this Kalman filter integration within social learning: Private-Prior (PP) and Word-of-Mouth (WoM).\n\nHere's a JavaScript implementation of the core Kalman filter and its integration into both the PP and WoM social learning models:\n\n```javascript\nclass KalmanFilter {\n  constructor(a, q, r, initialState, initialCovariance) {\n    this.a = a; // State transition model\n    this.q = q; // Process noise covariance\n    this.r = r; // Measurement noise covariance\n    this.x = initialState; // Initial state estimate\n    this.P = initialCovariance; // Initial state covariance\n  }\n\n  predict() {\n    this.P = this.a * this.P * this.a + this.q;\n    this.x = this.a * this.x;\n  }\n\n  update(measurement) {\n    const K = this.P / (this.P + this.r); // Kalman Gain\n    this.x = this.x + K * (measurement - this.x);\n    this.P = this.P * (1 - K);\n  }\n}\n\n\nfunction privatePrior(agents, measurements) {\n  for (let i = 0; i < agents.length; i++) {\n    agents[i].predict();\n    let effectiveMeasurement = measurements[i];\n    if (i > 0) {\n         // Adjust measurement based on previous agent's estimate (Eq. 7 & 9 logic)\n        effectiveMeasurement = (agents[i-1].x + effectiveMeasurement)/ agents[i-1].a\n    }\n\n    agents[i].update(effectiveMeasurement);\n\n        //Post processing for next agent (if not last) -- Eq. 6 logic, simplified for single value\n    if(i < agents.length-1){\n        agents[i].x = agents[i].x - (1- agents[i].a)*agents[i].x/agents[i].a\n    }\n  }\n}\n\n\nfunction wordOfMouth(agents, measurements) {\n\n  agents[0].predict();\n  agents[0].update(measurements[0]);\n\n  for (let i = 1; i < agents.length; i++) {\n    agents[i].P = agents[0].P; // Prior covariance from last agent (Eq. 11)\n    agents[i].x = agents[0].x; // Prior estimate from last agent (Eq. 11)\n    agents[i].update(measurements[i]);\n  }\n\n\n}\n\n\n\n// Example usage (adapt parameters as needed based on the paper's experimental settings):\n\nconst numAgents = 3;\nconst a = 0.95;\nconst q = 1;\nconst r = 1; //Initial measurement noise.  Recalculated in algorithm based on interconnections and paper formulas\nconst initialState = 25;\nconst initialCovariance = 3;\n\n\n//Example measurements (Replace with actual data)\nconst measurementsPP = [24,25,26]; //Example measurements\nconst measurementsWoM = [24,25,26]; //Example measurements\n\n\nlet agentsPP = [];\nlet agentsWoM = [];\n\nfor (let i = 0; i < numAgents; i++) {\n  agentsPP.push(new KalmanFilter(a, q, r, initialState, initialCovariance));\n  agentsWoM.push(new KalmanFilter(a, q, r, initialState, initialCovariance));\n}\n\n\n\nprivatePrior(agentsPP, measurementsPP);\nwordOfMouth(agentsWoM, measurementsWoM);\n\nconsole.log(\"Private Prior:\", agentsPP.map(agent => agent.x));  // Estimated states for each agent in PP model.\nconsole.log(\"Word of Mouth:\", agentsWoM.map(agent => agent.x)); // Estimated states for each agent in WoM model.\n\n```\n\n**Explanation and Purpose:**\n\n* **`KalmanFilter` Class:** This class implements the standard Kalman filter predict and update steps (Equation 3). The constructor initializes the model parameters and the initial state estimate and covariance.  The predict step projects the state and covariance forward in time. and the update step incorporates a new measurement to refine the state estimate.\n\n* **`privatePrior` Function:** This function simulates the Private-Prior social learning scenario. Each agent performs Kalman filtering sequentially, using their own prior and the potentially modified observations of the previous agent's output (Equation 7, and variance calculated via Equation 9 logic in the code).\n\n* **`wordOfMouth` Function:** This function simulates the Word-of-Mouth social learning scenario. The last agent's posterior becomes the common prior for all agents in the next iteration (Equations 11 and 12).\n\n\nThis JavaScript code provides a foundation for experimenting with the multi-agent Kalman filtering models discussed in the paper. Remember to replace the example measurements and adapt the parameters (a, q, r, initial state, initial covariance) based on the paper's specific experimental setup and your own data.  This implementation covers the core aspects of the algorithms presented in the paper.  Certain details, such as dynamic noise updates and edge case handling, may need refinement based on the specific application and experimental design.",
  "simpleQuestion": "How does WoM impact sequential social learning accuracy?",
  "timestamp": "2025-04-07T05:05:13.625Z"
}