{
  "arxivId": "2502.01711",
  "title": "EXPECTED RETURN SYMMETRIES",
  "abstract": "Symmetry is an important inductive bias that can improve model robustness and generalization across many deep learning domains. In multi-agent settings, a priori known symmetries have been shown to address a fundamental coordination failure mode known as mutually incompatible symmetry breaking; e.g. in a game where two independent agents can choose to move \"left\" or \"right\", and where a reward of +1 or -1 is received when the agents choose the same action or different actions, respectively. However, the efficient and automatic discovery of environment symmetries, in particular for decentralized partially observable Markov decision processes, remains an open problem. Furthermore, environmental symmetry breaking constitutes only one type of coordination failure, which motivates the search for a more accessible and broader symmetry class. In this paper, we introduce such a broader group of previously unexplored symmetries, which we call expected return symmetries, which contains environment symmetries as a subgroup. We show that agents trained to be compatible under the group of expected return symmetries achieve better zero-shot coordination results than those using environment symmetries. As an additional benefit, our method makes minimal a priori assumptions about the structure of their environment and does not require access to ground truth symmetries.",
  "summary": "This paper introduces \"expected return symmetries,\" a broader class of symmetries in multi-agent systems than previously considered. These symmetries transform policies into other policies that achieve the same expected return in self-play. By training agents to be compatible under these symmetries, the researchers achieve better zero-shot coordination, particularly in scenarios where traditional methods struggle.  They also propose algorithms to learn these symmetries directly from agent-environment interactions without prior knowledge of the environment's structure.\n\nFor LLM-based multi-agent systems, this research suggests a new way to improve coordination by focusing on expected return symmetries. This is particularly relevant when deploying LLMs in complex environments with unknown or difficult-to-define symmetries, as the symmetries can be learned rather than pre-defined. The focus on expected returns could also translate into more robust and adaptable multi-agent LLM systems.",
  "takeaways": "This paper introduces the concept of Expected Return (ER) Symmetries for improved zero-shot coordination in multi-agent systems, particularly relevant for LLM-based agents interacting in web environments. Here's how JavaScript developers can apply these insights:\n\n**1. Building Collaborative LLM Agents for Web Applications:**\n\n* **Scenario:** Imagine building a web application for collaborative writing using multiple LLMs.  Each LLM acts as an agent, contributing to a shared document. Without coordination, they might produce incoherent or conflicting text.\n* **Applying ER Symmetries:**  Train the LLMs using a variant of Other-Play (OP) with learned ER symmetries.  This could involve training the LLMs to generate text conditioned on the output of other LLMs, while periodically applying learned transformations to their input or output. This encourages them to develop compatible communication strategies and avoid \"over-coordination\" where they rely on overly specific prompts or stylistic conventions.\n* **JavaScript Implementation:**  Use a JavaScript machine learning library like TensorFlow.js or WebDNN to implement the LLMs and the training process within the browser. A Node.js backend could handle more computationally intensive training.  For communication between agents, consider using WebSockets or a message queue.\n\n**2. Creating Dynamic Web Interfaces with Multi-Agent LLMs:**\n\n* **Scenario:**  Develop a dynamic web interface where multiple LLM agents manage different aspects of the user experience. For instance, one agent handles content generation, another manages layout, and a third personalizes the interface based on user interactions.\n* **Applying ER Symmetries:** Train the LLMs to be compatible with each other using OP and ER symmetries. This could involve defining a reward function that encourages coherent and aesthetically pleasing layouts, along with personalized content. The ER symmetries ensure that the agents don't develop incompatible strategies for achieving this shared goal.\n* **JavaScript Implementation:** Use a frontend framework like React or Vue.js to manage the dynamic interface. Each LLM agent could be integrated as a component, communicating with other components through the framework's state management system.  The training process could be offloaded to a Node.js backend or a cloud-based machine learning service.\n\n**3. Developing Interactive Storytelling Experiences with LLM Agents:**\n\n* **Scenario:** Create an interactive storytelling experience where users interact with multiple LLM-powered characters in a web-based virtual world. Each character is an agent with its own personality and motivations.\n* **Applying ER Symmetries:**  Train the LLMs using OP and ER symmetries to ensure that the characters' interactions are consistent and engaging. This could involve a reward function that encourages diverse and unpredictable storylines, while also maintaining narrative coherence.  ER symmetries prevent the characters from becoming too predictable or repetitive in their interactions.\n* **JavaScript Implementation:** Use a JavaScript game engine like Phaser or Babylon.js to create the virtual world. Each character could be implemented as an LLM agent integrated with the game engine, responding to user input and interacting with other characters through the game engine's event system.\n\n**4. Enhancing Web Accessibility with Multi-Agent LLMs:**\n\n* **Scenario:** Develop a web application that uses multiple LLMs to enhance accessibility for users with disabilities.  One agent might generate alternative text descriptions for images, another might simplify complex text, and a third might provide audio descriptions of the interface.\n* **Applying ER Symmetries:** Train the LLMs using OP and ER symmetries to ensure that they provide consistent and comprehensive accessibility features. This could involve a reward function that measures the clarity, accuracy, and completeness of the accessibility information provided. ER symmetries prevent the agents from developing conflicting or redundant accessibility features.\n* **JavaScript Implementation:** Use standard web accessibility APIs and libraries to integrate the LLM-generated accessibility features into the web application. The training process could be handled by a backend service or a cloud-based machine learning platform.\n\n**Key JavaScript Libraries and Tools:**\n\n* **TensorFlow.js/WebDNN:**  For implementing and training LLMs in the browser.\n* **React/Vue.js/Angular:**  For building dynamic web interfaces and managing communication between LLM agents.\n* **Node.js:** For backend processing and integration with cloud-based machine learning services.\n* **WebSockets/Message Queues:** For real-time communication between LLM agents.\n* **Phaser/Babylon.js:**  For creating interactive web-based virtual worlds.\n* **Web Accessibility APIs/Libraries:** For integrating LLM-generated accessibility features.\n\nBy leveraging the concept of ER symmetries, JavaScript developers can create more robust and adaptable multi-agent LLM systems for a variety of web applications, pushing the boundaries of intelligent web interactions.  The key is to carefully consider the reward function and the training process to ensure that the agents learn to cooperate effectively and avoid over-coordination.",
  "pseudocode": "The paper contains pseudocode blocks describing three algorithms. Here are their JavaScript translations and explanations:\n\n**Algorithm 1: Learning Expected Return Symmetries with Policy Gradients (without enforcing compositionality nor invertibility)**\n\n```javascript\nasync function learnERSymmetries(decPOMDP, policySet, phi_o_theta, theta, learningRate, topK) {\n  let Jtop = Array(topK).fill(-Infinity);\n  let phi_top = Array(topK).fill(null);\n\n  for (const phi_a of decPOMDP.getActionTranspositions()) {  // Iterate over action transpositions\n    theta = decPOMDP.initializeObservationTransformation(theta); // Initialize observation transformation parameters\n\n    let converged = false;\n    while (!converged) {\n      for (const pi of policySet) {\n        const [aohs, returns] = await decPOMDP.sampleBatch(phi_theta.apply(pi, theta, phi_a));\n        const advantages = decPOMDP.computeAdvantage(aohs, returns);\n\n        // Compute policy gradient using advantages\n        const policyGradient = decPOMDP.computePolicyGradient(pi, aohs, advantages, theta, phi_a);\n\n        theta = math.add(theta, math.multiply(learningRate, policyGradient));\n      }\n      // Convergence check (not implemented here for brevity, would involve checking J change)\n\n    }\n\n\n    const J_phi = await decPOMDP.averageExpectedReturn(policySet, phi_theta, theta, phi_a);\n\n    const minIndex = Jtop.indexOf(Math.min(...Jtop));\n\n    if (J_phi > Jtop[minIndex]) {\n      Jtop[minIndex] = J_phi;\n      phi_top[minIndex] = [phi_o_theta, phi_a];\n    }\n\n\n  }\n  return phi_top;\n}\n```\n\n* **Purpose:** This algorithm aims to discover transformations of the observation space (φ_o) that preserve the expected return of optimal policies, given a fixed transposition of the action space (φ_a). This leverages the concept of expected return symmetries, which are crucial for zero-shot coordination.\n* **Explanation:** The algorithm iterates through all possible action transpositions. For each action transposition, it learns an observation transformation using policy gradients.  It maintains a list of the `topK` best-performing transformations based on their average expected return over a set of optimal policies. The optimization process uses a reinforcement learning inner loop.\n\n**Algorithm 2: Learning Expected Return Symmetries (enforcing compositionality and invertibility)**\n\n```javascript\n// ... (Similar structure as Algorithm 1, with added regularization terms)\nasync function learnERSymmetriesRegularized(decPOMDP, policySet, phi_o_theta, theta, learningRate, transformations, topK, lambda1, lambda2) {\n\n// ... (Initialization similar to Algorithm 1)\n\n for (const phi_a of decPOMDP.getActionTranspositions()) {\n\n // ... (Inner loop initialization similar to Algorithm 1)\n\n\n      while (!converged) {\n        for (const pi of policySet) {\n          let phi_theta_current = phi_theta;\n          if (Math.random() < lambda1) {\n            const [phi_i, phi_j] = decPOMDP.sampleTransformations(transformations, 2);\n            phi_theta_current = decPOMDP.composeTransformations(phi_i, phi_theta, phi_j);\n          }\n          // ... (Sample batch, compute advantage and gradient as in Algorithm 1, but with phi_theta_current)\n\n\n         const invertibilityLossGradient = decPOMDP.computeInvertibilityLossGradient(theta, phi_o_theta);\n\n           theta = math.add(theta, math.subtract(math.multiply(learningRate, policyGradient), math.multiply(lambda2, invertibilityLossGradient)));\n\n        }\n\n// ... (Rest of the code is analogous to Algorithm 1, with updated J_phi computation)\n}\n\n```\n\n* **Purpose:**  This algorithm extends Algorithm 1 by adding regularization to enforce that the learned transformations form a group. Specifically, it encourages closure under composition and invertibility.\n* **Explanation:** It takes as input a set of transformations learned from Algorithm 1. In the inner loop, it randomly composes these previously learned transformations with the current transformation being learned with probability  `lambda1`. This promotes closure under composition. The regularization term `lambda2` penalizes transformations that are far from their inverse, promoting invertibility.\n\n\n**Algorithm 3: Learning Expected Return Symmetries through cross-play maximization between pairs of Policies**\n\n```javascript\n// ... (Similar structure as Algorithm 1, focusing on cross-play)\nasync function learnERSymmetriesCrossplay(decPOMDP, policySet, phi_o_theta, theta, learningRate, topK) {\n // ...(Initialization is analogous to Algorithm 1)\n\n\n for (let i = 0; i < policySet.length; i++) {\n    for (let j = 0; j < policySet.length; j++) {\n      if (i !== j) { // Ensure different policies for cross-play\n\n        // ...(Inner loop initialization is analogous to Algorithm 1)\n\n         while (!converged) {\n          // Sample batch and compute advantages using cross-play between pi and the transformed pj\n          const [aohs, returns] = await decPOMDP.sampleBatchCrossplay(policySet[i], phi_theta.apply(policySet[j], theta, phi_a));\n          const advantages = decPOMDP.computeAdvantage(aohs, returns);\n\n\n\n// ... (Rest of the code is analogous to Algorithm 1, with J now referring to cross-play between pi and the transformed pj)\n\n\n      }\n    }\n  }\n  return phi_top;\n}\n```\n\n* **Purpose:** This algorithm offers an alternative approach to learning ER symmetries by directly maximizing the cross-play performance between pairs of policies.\n* **Explanation:** It iterates through pairs of policies from a set of optimal policies and learns a transformation that maximizes their cross-play score.  Similar to Algorithm 1, it keeps track of the  `topK` best-performing transformations. This approach directly optimizes for zero-shot coordination performance.\n\n\nThese JavaScript implementations provide a starting point for software engineers to experiment with ER symmetries in JavaScript for multi-agent system development.  Note that these are simplified versions and would require additional implementation details for a full working system, such as specific definitions of the `decPOMDP` methods, matrix operations, and a convergence criterion in the training loops.  Further, helper functions would be needed to compute the transpositions, compose them, and apply them to policies and action-observation histories.  These would likely involve manipulating arrays and objects representing actions, observations, and policies.",
  "simpleQuestion": "Can expected return symmetries improve multi-agent coordination?",
  "timestamp": "2025-02-05T06:06:22.091Z"
}