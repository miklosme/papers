{
  "arxivId": "2502.17618",
  "title": "Hierarchical Imitation Learning of Team Behavior from Heterogeneous Demonstrations",
  "abstract": "Successful collaboration requires team members to stay aligned, especially in complex sequential tasks. Team members must dynamically coordinate which subtasks to perform and in what order. However, real-world constraints like partial observability and limited communication bandwidth often lead to suboptimal collaboration. Even among expert teams, the same task can be executed in multiple ways. To develop multi-agent systems and human-AI teams for such tasks, we are interested in data-driven learning of multimodal team behaviors. Multi-Agent Imitation Learning (MAIL) provides a promising framework for data-driven learning of team behavior from demonstrations, but existing methods struggle with heterogeneous demonstrations, as they assume that all demonstrations originate from a single team policy. Hence, in this work, we introduce DTIL: a hierarchical MAIL algorithm designed to learn multimodal team behaviors in complex sequential tasks. DTIL represents each team member with a hierarchical policy and learns these policies from heterogeneous team demonstrations in a factored manner. By employing a distribution-matching approach, DTIL mitigates compounding errors and scales effectively to long horizons and continuous state representations. Experimental results show that DTIL outperforms MAIL baselines and accurately models team behavior across a variety of collaborative scenarios.",
  "summary": "This paper introduces DTIL, a new algorithm for training AI agents to work together in teams, learning from examples of how humans collaborate.  It focuses on scenarios where teams might perform the same task in different ways and where agents have limited information about their teammates.  DTIL uses a hierarchical structure, similar to how humans break down complex tasks into smaller subtasks, and improves upon existing methods by handling diverse team strategies and partial observability.\n\nFor LLM-based multi-agent systems, DTIL offers a way to train agents that can exhibit flexible and diverse collaboration strategies learned from human demonstrations, even when agents have only partial views of the overall task.  This is particularly relevant for complex, real-world applications where explicit communication may be limited or undesirable, making flexible coordination through learned subtask structures a valuable approach.  DTIL's use of non-adversarial imitation learning could also potentially address some of the training instability often encountered in generative adversarial imitation learning methods.",
  "takeaways": "This paper presents DTIL, a hierarchical multi-agent imitation learning algorithm designed to learn from diverse and potentially suboptimal demonstrations, a common scenario when dealing with human input.  Let's explore how a JavaScript developer can apply these insights to LLM-based multi-agent app development:\n\n**1. Modeling Complex Multi-Agent Interactions in Web Apps:**\n\nImagine building a collaborative writing app where multiple LLM agents assist users. DTIL's hierarchical structure can be mirrored in JavaScript using a combination of LLMs for high-level planning and lower-level execution.\n\n* **High-level LLM (Planner):** This LLM could use a library like LangChain to chain together different prompts and tools. Its role would be to break down the overall writing task into subtasks (e.g., brainstorming, outlining, drafting, editing). The output of the planner could be a structured JSON object representing the plan.\n* **Low-level LLMs (Executors):** Multiple specialized executor LLMs could handle specific subtasks.  For instance, one LLM could focus on generating creative content, another on grammar and style checking, and a third on fact verification.  These executors could be implemented using a JavaScript library like `transformers.js` or by calling a cloud-based LLM API.\n* **Coordination:**  The planner LLM can dynamically assign subtasks to executors based on the current state of the document. The executors can communicate their progress back to the planner, enabling dynamic adjustments to the plan. This coordination can be implemented using Node.js and a message queue like Redis.\n\n**2. Learning from User Demonstrations:**\n\nDTIL focuses on learning from heterogeneous demonstrations, which is highly relevant for web apps where user behavior can be very diverse.\n\n* **Recording User Interactions:**  In the collaborative writing app, user actions (e.g., text edits, comments, suggestions) could be recorded as demonstrations. Libraries like `rrweb` can be used to capture and replay user sessions.\n* **Training the Planner LLM:** These demonstrations could be used to fine-tune the planner LLM.  The demonstrations can be converted into a dataset of (observation, subtask, action) tuples, where observations are the state of the document, subtasks are the inferred user intentions, and actions are the user's edits.\n* **Inferring User Intentions (Subtasks):** DTIL's EM approach can be implemented in JavaScript to infer user intentions from recorded interactions.  This requires modeling the probabilities of user actions given different subtasks, which can be achieved through supervised learning techniques.\n\n**3. Partial Observability and Decentralized Execution:**\n\nIn web apps, each agent might only have access to a partial view of the system. DTIL's handling of partial observability is crucial here.\n\n* **Local State for Each Agent:** Each executor LLM could be provided with a limited view of the document relevant to its subtask, rather than the entire document.\n* **Message Passing for Communication:** Agents could communicate with each other using a message passing system implemented using WebSockets or Server-Sent Events, mirroring the decentralized nature of DTIL.\n\n**4. Practical JavaScript Frameworks and Libraries:**\n\n* **LangChain:** For orchestrating LLM workflows and prompting.\n* **`transformers.js`:** For running LLMs client-side.\n* **Node.js and Redis:** For backend processing and message queuing.\n* **TensorFlow.js:** For implementing machine learning models related to subtask inference.\n* **`rrweb`:** For recording and replaying user sessions.\n\n**5. Experimentation:**\n\nStart with simpler scenarios, like a multi-agent chatbot for customer service.  Gradually increase the complexity by adding more agents and more complex tasks.\n\n\nBy applying DTIL's principles and utilizing these JavaScript tools, developers can create sophisticated LLM-based multi-agent web applications that can learn from diverse user behavior and adapt to dynamic environments. This allows for the development of truly interactive and personalized web experiences.",
  "pseudocode": "```javascript\n// Algorithm 1: DTIL: Deep Team Imitation Learner (JavaScript adaptation)\n\nasync function dtil(D, optionalSubtaskLabels) {\n  // 1. Input: Demonstrations D = [trajectory1, trajectory2, ...],\n  //    Optional: partial subtask labels (e.g., [[x1_0, x1_1, ...], [x2_0, x2_1,...], ...])\n\n  const numAgents = D[0][0].observations.length; // Assuming observation structure\n\n  // 2. Initialize: Agent models for each agent\n  let agentModels = [];\n  for (let i = 0; i < numAgents; i++) {\n    agentModels.push({\n      lowLevelPolicy: initializeLowLevelPolicy(i), // Initialize policy network (θ_i)\n      highLevelPolicy: initializeHighLevelPolicy(i)  // Initialize policy network (φ_i)\n    });\n  }\n\n  let converged = false;\n  while (!converged) {\n    // 3. E-step: Infer expert intents (subtasks)\n    let augmentedD = D.map((trajectory, index) => {\n      if (optionalSubtaskLabels && optionalSubtaskLabels[index]) {\n        return augmentTrajectory(trajectory, optionalSubtaskLabels[index]);\n      } else {\n        let inferredSubtasks = inferExpertIntents(trajectory, agentModels);\n        return augmentTrajectory(trajectory, inferredSubtasks);\n      }\n    });\n\n    // Collect online rollouts using current agent models\n    let R = await collectRollouts(agentModels);\n    augmentedD = augmentedD.concat(R);\n\n\n    // 4. M-step: Update agent model parameters\n    let newAgentModels = [];\n    for (let i = 0; i < numAgents; i++) {\n      let {lowLevelPolicy, highLevelPolicy} = agentModels[i];\n\n      // Prepare agent-specific data\n      let Di = extractAgentData(augmentedD, i); \n      let Ri = extractAgentData(R, i);\n\n      // Update low-level policy using IQ-Learn (Eq. 5)\n      let updatedLowLevelPolicy = await updateLowLevelPolicy(lowLevelPolicy, Di, Ri);\n\n      // Update high-level policy using IQ-Learn (Eq. 6)\n      let updatedHighLevelPolicy = await updateHighLevelPolicy(highLevelPolicy, Di, Ri);\n\n\n      newAgentModels.push({\n        lowLevelPolicy: updatedLowLevelPolicy,\n        highLevelPolicy: updatedHighLevelPolicy\n      });\n\n    }\n    agentModels = newAgentModels;\n\n\n    // 5. Convergence check (Implementation specific)\n    converged = checkConvergence(agentModels); // Example: Check if policy changes are small\n  }\n\n  return agentModels;\n}\n\n\n\n\n// Helper function examples (Placeholders; replace with your implementation)\n\nfunction initializeLowLevelPolicy(agentIndex) {/* ... */}\nfunction initializeHighLevelPolicy(agentIndex) { /* ... */ }\nfunction inferExpertIntents(trajectory, agentModels) {/* ... */}\nfunction augmentTrajectory(trajectory, subtasks) { /* ... */}\nasync function collectRollouts(agentModels) {/* ... */}\nfunction extractAgentData(augmentedD, agentIndex) {/* ... */}\nasync function updateLowLevelPolicy(policy, D, R) {/* ... */}\nasync function updateHighLevelPolicy(policy, D, R) {/* ... */}\nfunction checkConvergence(agentModels) { /* ... */ }\n\n\n```\n\n**Explanation of the DTIL algorithm and its JavaScript adaptation:**\n\nThe Deep Team Imitation Learner (DTIL) algorithm is designed to learn multimodal team behaviors from heterogeneous demonstrations, addressing the challenge of training multi-agent systems in complex scenarios where a single optimal policy might not exist. It leverages a hierarchical structure to represent individual agent policies, consisting of a low-level policy that dictates actions and a high-level policy that governs subtask selection.\n\n\n1. **Input:** The algorithm takes demonstrations `D` (sequences of observations and actions of all agents) and optionally some subtask labels.\n\n2. **Initialization:**  Initializes low-level (θ_i) and high-level (φ_i) policy networks for each agent. In the JavaScript code, these are represented as objects within the `agentModels` array.\n\n3. **E-step (Expectation):**  This step infers the hidden subtasks (intents) from the demonstrations. If labels are available, they are used; otherwise, the current agent models are used to estimate the most likely subtasks using a Maximum a Posteriori (MAP) estimation (similar to the Viterbi algorithm). Online rollouts are collected by allowing the current agents to interact with the environment. The rollout data is added to the augmented demonstrations.\n\n4. **M-step (Maximization):** Updates the parameters of the agent models (low-level and high-level policies) using a factored distribution matching approach. The objective is to minimize the difference between the occupancy measure of the learned policy and an estimated expert occupancy measure. The provided code leverages IQ-Learn for this optimization within the `updateLowLevelPolicy` and `updateHighLevelPolicy` functions.\n\n5. **Convergence Check:** The algorithm iterates between the E-step and M-step until a convergence criterion is met (e.g., small change in policy parameters).\n\n**Purpose:** The purpose of DTIL is to enable effective learning of complex, coordinated team behaviors in realistic scenarios where partial observability and suboptimal demonstrations are common. It addresses the limitations of traditional imitation learning approaches that assume a single optimal policy, allowing for a more nuanced representation of teamwork and facilitating human-AI collaboration in various domains.\n\n\nThe provided JavaScript code is a structural adaptation of the pseudocode, illustrating how the core concepts of DTIL can be translated into a JavaScript environment. Helper functions are outlined as placeholders, and their specific implementations would depend on the chosen machine learning library (e.g., TensorFlow.js, Brain.js) and the details of the task environment.  The asynchronous nature of certain operations (e.g., rollouts, policy updates) is highlighted using the `async/await` syntax in the JavaScript code.",
  "simpleQuestion": "How can I train agents for diverse teamwork?",
  "timestamp": "2025-02-26T06:03:12.259Z"
}