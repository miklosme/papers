{
  "arxivId": "2412.04859",
  "title": "Breaking Event Rumor Detection via Stance-Separated Multi-Agent Debate",
  "abstract": "The rapid spread of rumors on social media platforms during breaking events severely hinders the dissemination of the truth. Previous studies reveal that the lack of annotated resources hinders the direct detection of unforeseen breaking events not covered in yesterday's news. Leveraging large language models (LLMs) for rumor detection holds significant promise. However, it is challenging for LLMs to provide comprehensive responses to complex or controversial issues due to limited diversity. In this work, we propose the Stance Separated Multi-Agent Debate (S2MAD) to address this issue. Specifically, we firstly introduce Stance Separation, categorizing comments as either supporting or opposing the original claim. Subsequently, claims are classified as subjective or objective, enabling agents to generate reasonable initial viewpoints with different prompt strategies for each type of claim. Debaters then follow specific instructions through multiple rounds of debate to reach a consensus. If a consensus is not reached, a judge agent evaluates the opinions and delivers a final verdict on the claim's veracity. Extensive experiments conducted on two real-world datasets demonstrate that our proposed model outperforms state-of-the-art methods in terms of performance and effectively improves the performance of LLMs in breaking event rumor detection.",
  "summary": "This paper introduces S2MAD, a multi-agent debate system for detecting rumors on social media during breaking events.  S2MAD uses multiple LLMs as \"debaters\" with assigned stances (supporting or opposing a claim) determined by analyzing comments.  Claims are categorized as subjective or objective, guiding initial prompt design for each LLM.  Debaters engage in multiple rounds of argumentation, and if no consensus is reached, a \"judge\" LLM makes the final veracity determination. Key to LLM-based multi-agent systems is the stance separation, specialized prompting based on claim subjectivity, and the debate framework for refining and consolidating information from multiple LLMs, leading to more robust rumor detection.",
  "takeaways": "This paper presents exciting opportunities for JavaScript developers working with LLMs in multi-agent systems.  Let's explore practical applications inspired by S2MAD, tailored for web development:\n\n**1. Collaborative Content Moderation:**\n\n* **Scenario:** Imagine a social media platform or forum using LLMs to moderate user-generated content. Multiple LLM agents could debate the appropriateness of a post flagged as potentially harmful.\n* **Implementation:**\n    * **Frontend (React, Vue, etc.):**  Display the post and the ongoing debate between agents.  Use a visually appealing interface to show each agent's stance (support/oppose) and reasoning.  Allow users to provide feedback, which can be fed back into the system.\n    * **Backend (Node.js, LangChain):** Orchestrate the debate using LangChain.  Use separate LLM instances for each agent, initialized with different prompts (pro/con, subjective/objective as in the paper).  Implement the debate logic using asynchronous JavaScript, passing messages between agents via LangChain's callback mechanisms.  A final \"judge\" agent can resolve stalemates.\n    * **Libraries:** LangChain for LLM interaction and orchestration.  React, Vue, or similar for the frontend.\n\n**2. Enhanced Chatbots for Complex Decision-Making:**\n\n* **Scenario:**  A chatbot helping users choose a product, plan a trip, or make a financial decision.  Multiple LLM agents could debate the pros and cons of different options.\n* **Implementation:**\n    * **Frontend (React):**  Present options with interactive elements.  Display the chatbot's internal \"debate\" in a user-friendly way, showing the reasoning of each agent.\n    * **Backend (Node.js, LangChain):**  Use LangChain to manage the agent interactions.  Implement the stance separation and debate logic in JavaScript.  Train a smaller LLM as a \"scorer\" to categorize user preferences and feedback.\n    * **Libraries:** LangChain for LLM interaction, React for the frontend.\n\n**3. Real-Time Fact-Checking and Misinformation Detection:**\n\n* **Scenario:** A browser extension that analyzes news articles or social media posts in real-time, identifying potential misinformation by having LLM agents debate claims.\n* **Implementation:**\n    * **Frontend (Browser Extension API):**  Inject a small UI element into the webpage to display the fact-checking results.\n    * **Backend (Background Script, LangChain):**  Extract claims from the webpage.  Use LangChain to initialize agents with different perspectives and trigger the debate.  A \"judge\" agent can synthesize the results and provide a final verdict.  Use stance separation to identify supporting and opposing evidence.\n    * **Libraries:** LangChain, Browser Extension APIs.\n\n\n**Key JavaScript Considerations:**\n\n* **Asynchronous Programming:** Essential for managing multiple LLM agents concurrently.  Use `async/await` and Promises effectively.\n* **State Management:**  Keep track of the debate state, agent responses, and user interactions.  Use Redux, MobX, or similar libraries if needed.\n* **UI/UX:**  Present the multi-agent debate in a clear and engaging way, avoiding information overload.\n* **LLM Selection:** Experiment with different LLMs (OpenAI, Cohere, etc.) to find the best balance of performance and cost-effectiveness.\n\n**Starting Small:**\n\nBegin with a simplified version of S2MAD, perhaps with only two agents and a few rounds of debate.  Focus on one specific scenario, like content moderation.  As you gain experience, you can add more agents, refine the debate logic, and explore more complex scenarios.  The key is to start experimenting and iterating, leveraging the power of JavaScript and LLMs to build innovative multi-agent web applications.",
  "pseudocode": "The paper includes Algorithm 1, which describes the Stance-Separated Multi-Agent Debate (S2MAD) algorithm. Here is the JavaScript equivalent:\n\n```javascript\nasync function s2mad(claim, comments, maxRounds, agentP, agentN, judge) {\n  // Stance Separation\n  const scores = {};\n  for (const comment of comments) {\n    scores[comment] = await scoreComment(comment, claim); // Eq. 1: Call a helper function to get the score\n  }\n\n  const sortedComments = Object.entries(scores).sort(([, scoreA], [, scoreB]) => scoreB - scoreA);\n\n  const k = Math.floor(comments.length / 4);  // Example k value, adjust as needed\n\n  const supportingComments = sortedComments.filter(([, score]) => score > 0).slice(0,k).map(([comment]) => comment);\n  const opposingComments = sortedComments.filter(([, score]) => score < 0).slice(0,k).map(([comment]) => comment);\n\n\n  // Initial Opinion Generation\n  const isSubjective = await checkSubjectivity(claim); // Helper function to determine subjectivity\n\n  let opinionP = await getInitialOpinion(claim, supportingComments, isSubjective); // Eq. 3\n  let opinionN = await getInitialOpinion(claim, opposingComments, isSubjective);\n\n\n  // Multi-Agent Debate\n  for (let round = 1; round <= maxRounds; round++) {\n\n    const newOpinionP = await debate(claim, opinionN, opinionP); // Eq. 4\n    const newOpinionN = await debate(claim, opinionP, opinionN);\n    opinionP = newOpinionP;\n    opinionN = newOpinionN;\n\n  }\n\n\n  // Judge's Decision\n  if (opinionP === opinionN) {\n    return opinionP;\n  } else {\n    return await judgeDebate(claim, opinionP, opinionN); // Eq. 5\n  }\n}\n\n\n\n// Helper functions (these would interact with your LLM)\nasync function scoreComment(comment, claim) {\n  // Use your LLM with appropriate prompt to score the comment\n  // Return a score between -1 and 1 (inclusive)\n  // ... your LLM interaction logic ...\n  const llmOutput = await llm.generateText(`Instruction: ... ${claim} ... ${comment}`); //Example\n  return parseFloat(JSON.parse(llmOutput).Score); // Assuming your LLM returns a JSON with a Score field\n}\n\n\nasync function checkSubjectivity(claim){\n  const llmOutput = await llm.generateText(`Claim: ${claim}\\nDoes the claim only express the personal opinions of the user? Please answer Yes or No.`);\n\n  return llmOutput.trim().toLowerCase() === \"yes\";\n}\n\n\n\nasync function getInitialOpinion(claim, comments, isSubjective) {\n  // Use your LLM with appropriate prompt (based on isSubjective) to generate an initial opinion\n  // ... your LLM interaction logic ...\n    const prompt = isSubjective? `Instruction: You need to do:\\n(1) Evaluate whether the content of the source post is a reasonable subjective expression, considering context, humor, satire, and cultural references.\\n(2) Evaluate whether the content of the original post may damage public trust in government or public figures.\\nClaim: ${claim}\\nComment: ${comments.join('\\n')}\\nAt the end, please choose the answer from the following options: Fake, Real.` :  `Instruction: You need to do:\\n(1) Evaluate the consistency and reliability of the supporting comments. Look for specific facts, data, or credible sources.\\n(2) Assess the consistency and reliability of the rebuttal comments. Identify any valid points that raise doubts.\\n(3) Consider common sense and general knowledge related to the topic.\\nClaim: ${claim}\\nComment: ${comments.join('\\n')}\\nAt the end, please choose the answer from the following options: Fake, Real.` ;\n  const llmOutput = await llm.generateText(prompt); //Example\n  return llmOutput.trim().toLowerCase();\n\n}\n\nasync function debate(claim, otherOpinion, myOpinion) {\n    const llmOutput = await llm.generateText(`Instruction: These are the opinions from other debaters. Based on the opinion of yourself and other debaters, you need: use critical thinking to analyze the views of others.\\nOther debaters' opinions: [${otherOpinion}]\\nCan you give an updated response, at the end, please choose the answer from the following options: Fake, Real.\\nClaim: ${claim}`); //Example\n    return llmOutput.trim().toLowerCase();\n\n\n}\n\nasync function judgeDebate(claim, opinionP, opinionN) {\n\n  const llmOutput = await llm.generateText(`Instruction: Please judge whether the post text is fake or real based on the following debate between Agentp and Agentn:\\nClaim: ${claim}\\nAgentp arguing: [${opinionP}]\\nAgent arguing: [${opinionN}]\\nConsider the arguments presented by both agents and make your determination about the authenticity of the post. At the end, must choose the answer from the following options: Fake, Real.`); //Example\n  return llmOutput.trim().toLowerCase();\n\n\n}\n\n\n// Example usage (replace with your actual data and agents)\nconst claim = \"Breaking: A child under the age of 18 has died of coronavirus in Los Angeles, public health officials announce, in a rare and tragic case.\";\nconst comments = [\"So sad.\", \"Fake news!\", \"Sources?\", \"terrible!\", \"U hv no proof\"]; // Example comments\n\n\nconst maxRounds = 2; // Debate rounds\n\n\n// Placeholder for your LLM agent implementations (e.g., using Langchain or other libraries).\nconst llm =  /*Your LLM Agent*/; \n\ns2mad(claim, comments, maxRounds, llm, llm, llm)\n  .then(result => console.log(\"Final verdict:\", result))\n  .catch(error => console.error(\"Error:\", error));\n\n```\n\n**Explanation and Purpose of S2MAD Algorithm:**\n\nThe S2MAD algorithm aims to detect rumors in social media posts, particularly during breaking events, by simulating a debate between multiple Large Language Model (LLM) agents. Its core idea is to leverage the diverse perspectives and reasoning capabilities of LLMs to analyze a claim and its associated comments thoroughly.\n\nHere's a breakdown of the algorithm's steps:\n\n1. **Stance Separation:** Comments related to the claim are analyzed and categorized into \"supporting\" and \"opposing\" stances based on their sentiment. This separation provides distinct viewpoints for the debating agents.\n\n2. **Initial Opinion Generation:**  LLM agents are initialized with either supporting or opposing comments and prompted to generate an initial opinion on the claim's veracity (real or fake). The prompts are tailored based on whether the claim is subjective or objective.\n\n3. **Multi-Agent Debate:** The agents engage in a debate, exchanging their opinions and reasoning for a predefined number of rounds.  Each agent refines its stance based on the arguments presented by the other.\n\n4. **Judge's Decision:** If the agents fail to reach a consensus after the debate, a \"judge\" agent analyzes the final arguments of both debaters and makes a final determination on the claim's truthfulness.\n\n\nThis multi-agent debate framework improves rumor detection accuracy compared to single-agent approaches by promoting critical thinking, reducing individual agent biases, and ensuring a more comprehensive evaluation of available information.  The use of tailored prompts and the incorporation of a judge agent further enhance the algorithm's robustness and reliability. This approach is particularly beneficial during breaking events, where information is often limited and conflicting.",
  "simpleQuestion": "Can debating LLMs detect breaking event rumors?",
  "timestamp": "2024-12-09T06:03:23.575Z"
}