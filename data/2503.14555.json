{
  "arxivId": "2503.14555",
  "title": "A GENERALIST HANABI AGENT",
  "abstract": "Traditional multi-agent reinforcement learning (MARL) systems can develop cooperative strategies through repeated interactions. However, these systems are unable to perform well on any other setting than the one they have been trained on, and struggle to successfully cooperate with unfamiliar collaborators. This is particularly visible in the Hanabi benchmark, a popular 2-to-5 player cooperative card-game which requires complex reasoning and precise assistance to other agents. Current MARL agents for Hanabi can only learn one specific game-setting (e.g., 2-player games), and play with the same algorithmic agents. This is in stark contrast to humans, who can quickly adjust their strategies to work with unfamiliar partners or situations. In this paper, we introduce Recurrent Replay Relevance Distributed DQN (R3D2), a generalist agent for Hanabi, designed to overcome these limitations. We reformulate the task using text, as language has been shown to improve transfer. We then propose a distributed MARL algorithm that copes with the resulting dynamic observation- and action-space. In doing so, our agent is the first that can play all game settings concurrently, and extend strategies learned from one setting to other ones. As a consequence, our agent also demonstrates the ability to collaborate with different algorithmic agents â€“ agents that are themselves unable to do so. The implementation code is available at: R3D2-A-Generalist-Hanabi-Agent",
  "summary": "This paper introduces R3D2, a new AI agent for the cooperative card game Hanabi, designed to be more flexible and adaptable than previous approaches.  It tackles the challenge of agents overfitting to specific teammates and game configurations (number of players) during training, hindering their ability to cooperate with unfamiliar agents or in different settings.\n\nKey points for LLM-based multi-agent systems: R3D2 uses a text-based representation of the game state and actions, facilitating generalization across different game configurations and promoting knowledge transfer.  The dynamic action space, also text-based, enables collaboration with agents trained on different game settings. This text-based approach allows for a simpler self-play training regimen while achieving robust zero-shot coordination.  The paper also explores the use of various Language Models, highlighting the limitations of using LLMs directly for playing Hanabi and motivating the reinforcement learning approach of R3D2.  Furthermore, it introduces the concept of \"variable-player learning,\" a multi-agent variant of multi-task learning where the number of players can change during training, enabling generalization across diverse gameplay scenarios.",
  "takeaways": "This paper offers several key insights translatable into practical JavaScript examples for LLM-based multi-agent web applications:\n\n**1. Text-Based Interaction and State Representation:**\n\n* **Concept:** R3D2 uses text to represent game state and actions, facilitating transfer learning between different game configurations (e.g., 2-player vs. 5-player Hanabi). This can be applied to web agents by representing the application state and agent actions as natural language.\n* **JavaScript Example:** Imagine building a multi-agent system for collaborative document editing. Instead of complex JSON objects, the application state could be described as:  \"User A added 'Hello' at position 5. User B is currently editing paragraph 3.  The document title is 'Project X'.\" Agent actions could be: \"Insert 'World' at position 11\", \"Change document title to 'Project Y'\". This allows LLMs to understand and manipulate the state easily.\n* **Libraries:** LangChain can be used to structure prompts and interact with LLMs, translating the text-based state and actions into LLM prompts.\n\n**2. Dynamic Action Spaces:**\n\n* **Concept:** R3D2 handles dynamic action spaces by encoding actions as embeddings. This is crucial in web development where the number of possible actions can change (e.g., available menu options, interactive elements).\n* **JavaScript Example:**  In a multi-agent chat application, the set of available actions might change based on the conversation context. Actions could be: \"Suggest topic 'A'\", \"Summarize last 5 messages\", \"Provide link to resource 'X'\". Using embeddings allows the agent to handle a growing vocabulary of actions.\n* **Libraries:** TensorFlow.js or similar libraries can be used for creating and managing action embeddings.\n\n**3. Variable-Player/Multi-Task Learning:**\n\n* **Concept:**  R3D2 can train across multiple game settings (2-5 players) concurrently, transferring knowledge between them.  This can be applied to web applications by training agents on multiple related tasks.\n* **JavaScript Example:**  A customer service chatbot could be trained on multiple tasks like answering FAQs, booking appointments, and handling complaints. By sharing knowledge across these tasks, the agent can generalize better to unseen scenarios.\n* **Frameworks:**  Reinforcement learning libraries like rl.js or web-based platforms like Hugging Face can be used to implement multi-task learning.\n\n**4. Decentralized Learning with Self-Play:**\n\n* **Concept:**  R3D2 uses self-play to train decentralized agents. This is important for web applications where agents need to act independently based on their local observations.\n* **JavaScript Example:** In a multi-agent resource allocation system, each agent representing a user could bid on resources independently based on their needs and budget. Self-play allows these agents to learn effective bidding strategies without explicit communication.\n* **Frameworks:** PettingZoo can be leveraged for creating multi-agent environments in JavaScript, facilitating self-play training.\n\n**Example code snippet (conceptual):**\n\n```javascript\n// Text-based state representation\nlet state = \"User A is editing document 'MyDoc'. Cursor at position 10.\";\n\n// Dynamic action embedding (using a fictional embedding function)\nlet action = \"Insert text 'Hello'\";\nlet actionEmbedding = embedAction(action); \n\n// Send prompt to LLM (using a fictional LLM interface)\nlet llmResponse = llm.generateText(`Given the state: ${state}, and the action: ${action}, predict the next state.`);\nlet nextState = llmResponse.text;\n\n// Update application state based on LLM response\n// ...\n```\n\n**Key Considerations for JavaScript Developers:**\n\n* **LLM Integration:** Selecting the right LLM (e.g., GPT-3.5-turbo, open-source alternatives) and using efficient prompting techniques is crucial for performance and cost-effectiveness.\n* **Scalability:** Handling a large number of agents and complex interactions requires careful design of the system architecture and efficient use of web technologies (e.g., WebSockets, Web Workers).\n* **User Interface:** Creating user interfaces that effectively display and interact with the multi-agent system is essential for real-world web applications.\n\n\nBy adapting these principles and using the appropriate JavaScript frameworks and libraries, developers can create innovative and intelligent multi-agent web applications powered by LLMs.  The text-based approach makes it significantly easier to leverage the power of LLMs, opening new possibilities for creating interactive and collaborative web experiences.",
  "pseudocode": "No pseudocode block found.",
  "simpleQuestion": "Can LLMs build adaptable Hanabi-playing agents?",
  "timestamp": "2025-03-20T06:02:33.093Z"
}